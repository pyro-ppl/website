<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Dirichlet Process Mixture Models in Pyro &mdash; Pyro Tutorials 1.8.3 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Example: Toy Mixture Model With Discrete Enumeration" href="toy_mixture_model_discrete_enumeration.html" />
    <link rel="prev" title="Gaussian Mixture Model" href="gmm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introductory Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_long.html">Introduction to Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_rendering.html">Automatic rendering of Pyro models</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iv.html">SVI Part IV: Tips and Tricks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">Bayesian Regression - Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">Bayesian Regression - Inference Algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow.html">High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains</a></li>
<li class="toctree-l1"><a class="reference internal" href="prior_predictive.html">Interactive posterior predictives checks</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Using the PyTorch JIT Compiler with Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="cvae.html">Conditional Variational Auto-encoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="normalizing_flows_i.html">Normalizing Flows - Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="prodlda.html">Probabilistic Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="scanvi.html"><em>scANVI: Deep Generative Modeling for Single Cell Data with Pyro</em></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Dirichlet Process Mixture Models in Pyro</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#What-are-Bayesian-nonparametric-models?">What are Bayesian nonparametric models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-Dirichlet-Process-(Ferguson,-1973)">The Dirichlet Process (Ferguson, 1973)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-Chinese-Restaurant-Process-(Aldous,-1985)">The Chinese Restaurant Process (Aldous, 1985)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-Stick-Breaking-Method-(Sethuraman,-1994)">The Stick-Breaking Method (Sethuraman, 1994)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Inference"><strong>Inference</strong></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Synthetic-Mixture-of-Gaussians">Synthetic Mixture of Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Dirichlet-Mixture-Model-for-Long-Term-Solar-Observations">Dirichlet Mixture Model for Long Term Solar Observations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ELBO-Behavior">ELBO Behavior</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Criticism"><strong>Criticism</strong></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Long-Term-Sunspot-Model">Long-Term Sunspot Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mle_map.html">MLE and MAP Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle_map.html#Doing-the-same-thing-with-AutoGuides">Doing the same thing with AutoGuides</a></li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">Customizing SVI objectives and training loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="boosting_bbvi.html">Boosting Black Box Variational Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecasting_i.html">Forecasting I: univariate, heavy tailed</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_ii.html">Forecasting II: state space models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_iii.html">Forecasting III: hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_dlm.html">Forecasting with Dynamic Linear Model (DLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="stable.html">Levy Stable models of Stochastic Volatility</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_intro.html">Epidemiological models: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="logistic-growth.html">Logistic growth models of SARS-CoV-2 lineage proportions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Experimental Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="working_memory.html">Designing Adaptive Experiments to Study Working Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="elections.html">Predicting the outcome of a US presidential election using Bayesian optimal experimental design</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Object Tracking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
<li class="toctree-l1"><a class="reference internal" href="predictive_deterministic.html">Example: Utilizing Predictive and Deterministic with MCMC and SVI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: A Guide to Programming with Effect Handlers in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib_funsor_intro_i.html"><code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code>, a new backend for Pyro - New primitives (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib_funsor_intro_ii.html"><code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code>, a new backend for Pyro - Building inference algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deprecated</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">(DEPRECATED) An Introduction to Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">(DEPRECATED) An Introduction to Inference in Pyro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Dirichlet Process Mixture Models in Pyro</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/dirichlet_process_mixture.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Dirichlet-Process-Mixture-Models-in-Pyro">
<h1>Dirichlet Process Mixture Models in Pyro<a class="headerlink" href="#Dirichlet-Process-Mixture-Models-in-Pyro" title="Permalink to this headline">¶</a></h1>
<section id="What-are-Bayesian-nonparametric-models?">
<h2>What are Bayesian nonparametric models?<a class="headerlink" href="#What-are-Bayesian-nonparametric-models?" title="Permalink to this headline">¶</a></h2>
<p>Bayesian nonparametric models are models where the number of parameters grow freely with the amount of data provided; thus, instead of training several models that vary in complexity and comparing them, one is able to design a model whose complexity grows as more data are observed. The prototypical example of Bayesian nonparametrics in practice is the <em>Dirichlet Process Mixture Model</em> (DPMM). A DPMM allows for a practitioner to build a mixture model when the number of distinct clusters in the
geometric structure of their data is unknown – in other words, the number of clusters is allowed to grow as more data is observed. This feature makes the DPMM highly useful towards exploratory data analysis, where few facets of the data in question are known; this presentation aims to demonstrate this fact.</p>
</section>
<section id="The-Dirichlet-Process-(Ferguson,-1973)">
<h2>The Dirichlet Process (Ferguson, 1973)<a class="headerlink" href="#The-Dirichlet-Process-(Ferguson,-1973)" title="Permalink to this headline">¶</a></h2>
<p>Dirichlet processes are a family of probability distributions over discrete probability distributions. Formally, the Dirichlet process (DP) is specified by some base probability distribution <span class="math notranslate nohighlight">\(G_0: \Omega \to \mathbb{R}\)</span> and a positive, real, scaling parameter commonly denoted as <span class="math notranslate nohighlight">\(\alpha\)</span>. A sample <span class="math notranslate nohighlight">\(G\)</span> from a Dirichlet process with parameters <span class="math notranslate nohighlight">\(G_0: \Omega \to \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\alpha\)</span> is itself a distribution over <span class="math notranslate nohighlight">\(\Omega\)</span>. For any disjoint partition
<span class="math notranslate nohighlight">\(\Omega_1, ..., \Omega_k\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span>, and any sample <span class="math notranslate nohighlight">\(G \sim DP(G_0, \alpha)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[(G(\Omega_1), ..., G(\Omega_k)) \sim \text{Dir}(\alpha G_0(\Omega_1), ..., \alpha G_0(\Omega_k))\]</div>
<p>Essentially, this is taking a discrete partition of our sample space <span class="math notranslate nohighlight">\(\Omega\)</span> and subsequently constructing a discrete distribution over it using the base distribution <span class="math notranslate nohighlight">\(G_0\)</span>. While quite abstract in formulation, the Dirichlet process is very useful as a prior in various graphical models. This fact becomes easier to see in the following scheme.</p>
</section>
<section id="The-Chinese-Restaurant-Process-(Aldous,-1985)">
<h2>The Chinese Restaurant Process (Aldous, 1985)<a class="headerlink" href="#The-Chinese-Restaurant-Process-(Aldous,-1985)" title="Permalink to this headline">¶</a></h2>
<p>Imagine a restaurant with infinite tables (indexed by the positive integers) that accepts customers one at a time. The <span class="math notranslate nohighlight">\(n\)</span>th customer chooses their seat according to the following probabilities:</p>
<ul class="simple">
<li><p>With probability <span class="math notranslate nohighlight">\(\frac{n_t}{\alpha + n - 1}\)</span>, sit at table <span class="math notranslate nohighlight">\(t\)</span>, where <span class="math notranslate nohighlight">\(n_t\)</span> is the number of people at table <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p>With probability <span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha + n - 1}\)</span>, sit at an empty table</p></li>
</ul>
<p>If we associate to each table <span class="math notranslate nohighlight">\(t\)</span> a draw from a base distribution <span class="math notranslate nohighlight">\(G_0\)</span> over <span class="math notranslate nohighlight">\(\Omega\)</span>, and then associate unnormalized probability mass <span class="math notranslate nohighlight">\(n_t\)</span> to that draw, the resulting distribution over <span class="math notranslate nohighlight">\(\Omega\)</span> is equivalent to a draw from a Dirichlet process <span class="math notranslate nohighlight">\(DP(G_0, \alpha)\)</span>.</p>
<p>Furthermore, we can easily extend this to define the generative process of a nonparametric mixture model: every table <span class="math notranslate nohighlight">\(t\)</span> that has at least one customer seated is associated with a set of cluster parameters <span class="math notranslate nohighlight">\(\theta_t\)</span>, which were themselves drawn from some base distribution <span class="math notranslate nohighlight">\(G_0\)</span>. For each new observation, first assign that observation to a table according to the above probabilities; then, that observation is drawn from the distribution parameterized by the cluster parameters
for that table. If the observation was assigned to a new table, draw a new set of cluster parameters from <span class="math notranslate nohighlight">\(G_0\)</span>, and then draw the observation from the distribution parameterized by those cluster parameters.</p>
<p>While this formulation of a Dirichlet process mixture model is intuitive, it is also very difficult to perform inference on in a probabilistic programming framework. This motivates an alternative formulation of DPMMs, which has empirically been shown to be more conducive to inference (e.g. Blei and Jordan, 2004).</p>
</section>
<section id="The-Stick-Breaking-Method-(Sethuraman,-1994)">
<h2>The Stick-Breaking Method (Sethuraman, 1994)<a class="headerlink" href="#The-Stick-Breaking-Method-(Sethuraman,-1994)" title="Permalink to this headline">¶</a></h2>
<p>The generative process for the stick-breaking formulation of DPMMs proceeds as follows:</p>
<ul class="simple">
<li><p>Draw <span class="math notranslate nohighlight">\(\beta_i \sim \text{Beta}(1, \alpha)\)</span> for <span class="math notranslate nohighlight">\(i \in \mathbb{N}\)</span></p></li>
<li><p>Draw <span class="math notranslate nohighlight">\(\theta_i \sim G_0\)</span> for <span class="math notranslate nohighlight">\(i \in \mathbb{N}\)</span></p></li>
<li><p>Construct the mixture weights <span class="math notranslate nohighlight">\(\pi\)</span> by taking <span class="math notranslate nohighlight">\(\pi_i(\beta_{1:\infty}) = \beta_i \prod_{j&lt;i} (1-\beta_j)\)</span></p></li>
<li><p>For each observation <span class="math notranslate nohighlight">\(n \in \{1, ..., N\}\)</span>, draw <span class="math notranslate nohighlight">\(z_n \sim \pi(\beta_{1:\infty})\)</span>, and then draw <span class="math notranslate nohighlight">\(x_n \sim f(\theta_{z_n})\)</span></p></li>
</ul>
<p>Here, the infinite nature of the Dirichlet process mixture model can more easily be seen. Furthermore, all <span class="math notranslate nohighlight">\(\beta_i\)</span> are independent, so it is far easier to perform inference in a probabilistic programming framework.</p>
<p>First, we import all the modules we’re going to need:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">constraints</span>

<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">from</span> <span class="nn">pyro.distributions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyro.infer</span> <span class="kn">import</span> <span class="n">Predictive</span><span class="p">,</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">from</span> <span class="nn">pyro.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="k">assert</span> <span class="n">pyro</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;1.8.3&#39;</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">set_rng_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Inference">
<h2><strong>Inference</strong><a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h2>
<section id="Synthetic-Mixture-of-Gaussians">
<h3>Synthetic Mixture of Gaussians<a class="headerlink" href="#Synthetic-Mixture-of-Gaussians" title="Permalink to this headline">¶</a></h3>
<p>We begin by demonstrating the capabilities of Dirichlet process mixture models on a synthetic dataset generated by a mixture of four 2D Gaussians:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">50</span><span class="p">]),</span>
                  <span class="n">MultivariateNormal</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">50</span><span class="p">]),</span>
                  <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">50</span><span class="p">]),</span>
                  <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">50</span><span class="p">])))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data Samples from Mixture of 4 Gaussians&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_3_0.png" src="_images/dirichlet_process_mixture_3_0.png" />
</div>
</div>
<p>In this example, the cluster parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> are two dimensional vectors describing the means of a multivariate Gaussian with identity covariance. Therefore, the Dirichlet process base distribution <span class="math notranslate nohighlight">\(G_0\)</span> is also a multivariate Gaussian (i.e. the conjugate prior), although this choice is not as computationally useful, since we are not performing coordinate-ascent variational inference but rather black-box variational inference using Pyro.</p>
<p>First, let’s define the “stick-breaking” function that generates our weights, given our samples of <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mix_weights</span><span class="p">(</span><span class="n">beta</span><span class="p">):</span>
    <span class="n">beta1m_cumprod</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">beta1m_cumprod</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next, let’s define our model. It may be helpful to refer the definition of the stick-breaking model presented in the first part of this tutorial.</p>
<p>Note that all <span class="math notranslate nohighlight">\(\beta_i\)</span> samples are conditionally independent, so we model them using a <code class="docutils literal notranslate"><span class="pre">pyro.plate</span></code> of size <code class="docutils literal notranslate"><span class="pre">T-1</span></code>; we do the same for all samples of our cluster parameters <span class="math notranslate nohighlight">\(\mu_i\)</span>. We then construct a Categorical distribution whose parameters are the mixture weights using our sampled <span class="math notranslate nohighlight">\(\beta\)</span> values (line 9) below, and sample the cluster assignment <span class="math notranslate nohighlight">\(z_n\)</span> for each data point from that Categorical. Finally, we sample our observations from a multivariate Gaussian
distribution whose mean is exactly the cluster parameter corresponding to the assignment <span class="math notranslate nohighlight">\(z_n\)</span> we drew for the point <span class="math notranslate nohighlight">\(x_n\)</span>. This can be seen in the Pyro code below:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;beta_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;mu_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">mix_weights</span><span class="p">(</span><span class="n">beta</span><span class="p">)))</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)),</span> <span class="n">obs</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, it’s time to define our guide and perform inference.</p>
<p>The variational family <span class="math notranslate nohighlight">\(q(\beta, \theta, z)\)</span> that we are optimizing over during variational inference is given by:</p>
<div class="math notranslate nohighlight">
\[q(\beta, \theta, z) = \prod_{t=1}^{T-1} q_t(\beta_t) \prod_{t=1}^T q_t(\theta_t) \prod_{n=1}^N q_n(z_n)\]</div>
<p>Note that since we are unable to computationally model the infinite clusters posited by the model, we truncate our variational family at <span class="math notranslate nohighlight">\(T\)</span> clusters. This does not affect our model; rather, it is a simplification made in the <em>inference</em> stage to allow tractability.</p>
<p>The guide is constructed exactly according to the definition of our variational family <span class="math notranslate nohighlight">\(q(\beta, \theta, z)\)</span> above. We have <span class="math notranslate nohighlight">\(T-1\)</span> conditionally independent Beta distributions for each <span class="math notranslate nohighlight">\(\beta\)</span> sampled in our model, <span class="math notranslate nohighlight">\(T\)</span> conditionally independent multivariate Gaussians for each cluster parameter <span class="math notranslate nohighlight">\(\mu_i\)</span>, and <span class="math notranslate nohighlight">\(N\)</span> conditionally independent Categorical distributions for each cluster assignment <span class="math notranslate nohighlight">\(z_n\)</span>.</p>
<p>Our variational parameters (<code class="docutils literal notranslate"><span class="pre">pyro.param</span></code>) are therefore the <span class="math notranslate nohighlight">\(T-1\)</span> many positive scalars that parameterize the second parameter of our variational Beta distributions (the first shape parameter is fixed at <span class="math notranslate nohighlight">\(1\)</span>, as in the model definition), the <span class="math notranslate nohighlight">\(T\)</span> many two-dimensional vectors that parameterize our variational multivariate Gaussian distributions (we do not parameterize the covariance matrices of the Gaussians, though this should be done when analyzing a real-world dataset for
more flexibility), and the <span class="math notranslate nohighlight">\(N\)</span> many <span class="math notranslate nohighlight">\(T\)</span>-dimensional vectors that parameterize our variational Categorical distributions:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">guide</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;kappa&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;tau&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">T</span><span class="p">]))</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;phi&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">T</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">N</span><span class="p">]),</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">simplex</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;beta_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">q_beta</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">Beta</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">kappa</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;mu_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">q_mu</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">phi</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>When performing inference, we set our ‘guess’ for the maximum number of clusters in the dataset to <span class="math notranslate nohighlight">\(T = 6\)</span>. We define the optimization algorithm (<code class="docutils literal notranslate"><span class="pre">pyro.optim.Adam</span></code>) along with the Pyro SVI object and train the model for 1000 iterations.</p>
<p>After performing inference, we construct the Bayes estimators of the means (the expected values of each factor in our variational approximation) and plot them in red on top of the original dataset. Note that we also have we removed any clusters that have less than a certain weight assigned to them according to our learned variational distributions, and then re-normalize the weights so that they sum to one:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">})</span>
<span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">())</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">clear_param_store</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">svi</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">truncate</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">centers</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">**-</span><span class="mi">1</span> <span class="o">/</span> <span class="mf">100.</span>
    <span class="n">true_centers</span> <span class="o">=</span> <span class="n">centers</span><span class="p">[</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>
    <span class="n">true_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">true_centers</span><span class="p">,</span> <span class="n">true_weights</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># We make a point-estimate of our model parameters using the posterior means of tau and phi for the centers and weights</span>
<span class="n">Bayes_Centers_01</span><span class="p">,</span> <span class="n">Bayes_Weights_01</span> <span class="o">=</span> <span class="n">truncate</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;phi&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># We make a point-estimate of our model parameters using the posterior means of tau and phi for the centers and weights</span>
<span class="n">Bayes_Centers_15</span><span class="p">,</span> <span class="n">Bayes_Weights_15</span> <span class="o">=</span> <span class="n">truncate</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;phi&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Bayes_Centers_01</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Bayes_Centers_01</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Bayes_Centers_15</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Bayes_Centers_15</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 1000/1000 [00:15&lt;00:00, 64.86it/s]
100%|██████████| 1000/1000 [00:15&lt;00:00, 65.47it/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_11_1.png" src="_images/dirichlet_process_mixture_11_1.png" />
</div>
</div>
<p>The plots above demonstrate the effects of the scaling hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span>. A greater <span class="math notranslate nohighlight">\(\alpha\)</span> yields a more heavy-tailed distribution of the weights, whereas smaller <span class="math notranslate nohighlight">\(\alpha\)</span> will place more mass on fewer clusters. In particular, the middle cluster looks like it could be generated a single Gaussian (although in fact it was generated by two distinct Gaussians), and thus the setting of <span class="math notranslate nohighlight">\(\alpha\)</span> allows the practitioner to further encode their prior beliefs about how
many clusters the data contains.</p>
</section>
<section id="Dirichlet-Mixture-Model-for-Long-Term-Solar-Observations">
<h3>Dirichlet Mixture Model for Long Term Solar Observations<a class="headerlink" href="#Dirichlet-Mixture-Model-for-Long-Term-Solar-Observations" title="Permalink to this headline">¶</a></h3>
<p>As mentioned earlier, the Dirichlet process mixture model truly shines when exploring a dataset whose latent geometric structure is completely unknown. To demonstrate this, we fit a DPMM on sunspot count data taken over the past 300 years (provided by the Royal Observatory of Belgium):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;http://www.sidc.be/silso/DATA/SN_y_tot_V2.0.csv&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;sunspot.year&#39;</span><span class="p">],</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;sunspot.year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;sunspot.year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of Years vs. Sunspot Counts&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sunspot Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Years&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_14_0.png" src="_images/dirichlet_process_mixture_14_0.png" />
</div>
</div>
<p>For this example, the cluster parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> are rate parameters since we are constructing a scale-mixture of Poisson distributions. Again, <span class="math notranslate nohighlight">\(G_0\)</span> is chosen to be the conjugate prior, which in this case is a Gamma distribution, though this still does not strictly matter for doing inference through Pyro. Below is the implementation of the model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;beta_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;lambda_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">lmbda</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;lambda&quot;</span><span class="p">,</span> <span class="n">Gamma</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">mix_weights</span><span class="p">(</span><span class="n">beta</span><span class="p">)))</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">lmbda</span><span class="p">[</span><span class="n">z</span><span class="p">]),</span> <span class="n">obs</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">guide</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;kappa&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">tau_0</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;tau_0&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">T</span><span class="p">]),</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">tau_1</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;tau_1&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">LogNormal</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">T</span><span class="p">]),</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;phi&#39;</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">T</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="n">N</span><span class="p">]),</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">simplex</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;beta_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">q_beta</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">Beta</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">kappa</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;lambda_plate&quot;</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">q_lambda</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;lambda&quot;</span><span class="p">,</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">tau_0</span><span class="p">,</span> <span class="n">tau_1</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">phi</span><span class="p">))</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.1</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">})</span>
<span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">())</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">train</span><span class="p">(</span><span class="n">n_iter</span><span class="p">)</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

<span class="n">tau0_optimal</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;tau_0&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">tau1_optimal</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;tau_1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">kappa_optimal</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;kappa&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

<span class="c1"># We make a point-estimate of our latent variables using the posterior means of tau and kappa for the cluster params and weights</span>
<span class="n">Bayes_Rates</span> <span class="o">=</span> <span class="p">(</span><span class="n">tau0_optimal</span> <span class="o">/</span> <span class="n">tau1_optimal</span><span class="p">)</span>
<span class="n">Bayes_Weights</span> <span class="o">=</span> <span class="n">mix_weights</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">kappa_optimal</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">mixture_of_poisson</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">rates</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">likelihood</span> <span class="o">=</span> <span class="n">mixture_of_poisson</span><span class="p">(</span><span class="n">Bayes_Weights</span><span class="p">,</span> <span class="n">Bayes_Rates</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of Years vs. Sunspot Counts&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Estimated Mixture Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 1500/1500 [00:09&lt;00:00, 156.27it/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_16_1.png" src="_images/dirichlet_process_mixture_16_1.png" />
</div>
</div>
<p>The above plot is the mixture density of the Bayes estimators of the cluster parameters, weighted by their corresponding weights. As in the Gaussian example, we have taken the Bayes estimators of each cluster parameter and their corresponding weights by computing the posterior means of <code class="docutils literal notranslate"><span class="pre">lambda</span></code> and <code class="docutils literal notranslate"><span class="pre">beta</span></code> respectively.</p>
</section>
<section id="ELBO-Behavior">
<h3>ELBO Behavior<a class="headerlink" href="#ELBO-Behavior" title="Permalink to this headline">¶</a></h3>
<p>Below are plots of the behavior of the loss function (negative Trace_ELBO) over the SVI iterations during inference using Pyro, as well as a plot of the autocorrelations of the ELBO ‘time series’ versus iteration number. We can see that around 500 iterations, the loss stops decreasing significantly, so we can assume it takes around 500 iterations to achieve convergence. The autocorrelation plot reaches an autocorrelation very close to 0 around a lag of 500, further corroborating this hypothesis.
Note that these are heuristics and do not necessarily imply convergence.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">elbo_plot</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">elbo_ax</span> <span class="o">=</span> <span class="n">elbo_plot</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">elbo_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ELBO Value vs. Iteration Number for Pyro BBVI on Sunspot Data&quot;</span><span class="p">)</span>
<span class="n">elbo_ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>
<span class="n">elbo_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration Number&quot;</span><span class="p">)</span>
<span class="n">elbo_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_iter</span><span class="p">),</span> <span class="n">losses</span><span class="p">)</span>

<span class="n">autocorr_ax</span> <span class="o">=</span> <span class="n">elbo_plot</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">autocorr_ax</span><span class="o">.</span><span class="n">acorr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="n">detrend</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">maxlags</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span> <span class="n">usevlines</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
<span class="n">autocorr_ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">autocorr_ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">autocorr_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Autocorrelation of ELBO vs. Lag for Pyro BBVI on Sunspot Data&quot;</span><span class="p">)</span>
<span class="n">autocorr_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Lag&quot;</span><span class="p">)</span>
<span class="n">autocorr_ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Autocorrelation&quot;</span><span class="p">)</span>
<span class="n">elbo_plot</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_19_0.png" src="_images/dirichlet_process_mixture_19_0.png" />
</div>
</div>
</section>
</section>
<section id="Criticism">
<h2><strong>Criticism</strong><a class="headerlink" href="#Criticism" title="Permalink to this headline">¶</a></h2>
<section id="Long-Term-Sunspot-Model">
<h3>Long-Term Sunspot Model<a class="headerlink" href="#Long-Term-Sunspot-Model" title="Permalink to this headline">¶</a></h3>
<p>Since we computed the approximate posterior of the DPMM that was fit to the long-term sunspot data, we can utilize some intrinsic metrics, such as the log predictive, posterior dispersion indices, and posterior predictive checks.</p>
<p>Since the posterior predictive distribution for a Dirichlet process mixture model is itself a scale-mixture distribution that has an analytic approximation <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/BleiJordan2004.pdf">(Blei and Jordan, 2004)</a>, this makes it particularly amenable to the aforementioned metrics:</p>
<div class="math notranslate nohighlight">
\[p(x_{new} | X_{1:N}, \alpha, G_0) \approx \sum_{t=1}^T \mathbb{E}_q [\pi_t(\beta)] \ \mathbb{E}_q \left[p(x_{new} | \theta_t)\right].\]</div>
<p>In particular, to compute the log predictive, we first compute the posterior predictive distribution (defined above) after performing variational inference on our model using a training subsample of our data. The log predictive is then the log value of the predictive density evaluated at each point in the test subsample:</p>
<div class="math notranslate nohighlight">
\[\log p(x_{new} | X) = \log \mathbb{E}_{\beta, \theta | X} \left[ p(x_{new} | \beta, \theta) \right]\]</div>
<p>Since both the training samples and the testing samples were taken from the same dataset, we would expect the model to assign high probability to the test samples, despite not having seen them during inference. This gives a metric by which to select values of <span class="math notranslate nohighlight">\(T\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span>, and <span class="math notranslate nohighlight">\(G_0\)</span>, our hyperparameters: we would want to choose the values that maximize this value.</p>
<p>We perform this process below with varying values of <span class="math notranslate nohighlight">\(\alpha\)</span> to see what the optimal setting is.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hold out 10% of our original data to test upon</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">index</span><span class="p">)[</span><span class="s1">&#39;sunspot.year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
<span class="n">data_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;sunspot.year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">N_test</span> <span class="o">=</span> <span class="n">data_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">log_predictives</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">val</span>
    <span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">())</span>
    <span class="n">train</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">S</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># number of Monte Carlo samples to use in posterior predictive computations</span>

    <span class="c1"># Using pyro&#39;s built in posterior predictive class:</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">guide</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">S</span><span class="p">,</span> <span class="n">return_sites</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;lambda&quot;</span><span class="p">])(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">post_pred_weights</span> <span class="o">=</span> <span class="n">mix_weights</span><span class="p">(</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;beta&quot;</span><span class="p">])</span>
    <span class="n">post_pred_clusters</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;lambda&quot;</span><span class="p">]</span>

    <span class="c1"># log_prob shape = N_test x S</span>
    <span class="n">log_prob</span> <span class="o">=</span> <span class="p">(</span><span class="n">post_pred_weights</span><span class="o">.</span><span class="n">log</span><span class="p">()</span> <span class="o">+</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">post_pred_clusters</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mean_log_prob</span> <span class="o">=</span> <span class="n">log_prob</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
    <span class="n">log_posterior_predictive</span> <span class="o">=</span> <span class="n">mean_log_prob</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">log_predictives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_posterior_predictive</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">log_predictives</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Value of the Log Predictive at Varying Alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 500/500 [00:03&lt;00:00, 157.68it/s]
100%|██████████| 500/500 [00:03&lt;00:00, 165.35it/s]
100%|██████████| 500/500 [00:03&lt;00:00, 156.21it/s]
100%|██████████| 500/500 [00:03&lt;00:00, 165.50it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 172.95it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 169.13it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 169.17it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 169.48it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 173.85it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 171.00it/s]
100%|██████████| 500/500 [00:03&lt;00:00, 161.77it/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_22_1.png" src="_images/dirichlet_process_mixture_22_1.png" />
</div>
</div>
<p>From the above plot, we would surmise that we want to set <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span>, though the signal is not quite clear. A more comprehensive model criticism process would involve performing a grid search across all hyperparameters in order to find the one that maximizes the log predictive.</p>
</section>
</section>
<section id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Ferguson, Thomas. <em>A Bayesian Analysis of Some Nonparametric Problems</em>. The Annals of Statistics, Vol. 1, No. 2 (1973).</p></li>
<li><p>Aldous, D. <em>Exchangeability and Related Topics</em>. Ecole diete de Probabilities Saint Flour (1985).</p></li>
<li><p>Sethuraman, J. <em>A Constructive Definition of Dirichlet Priors</em>. Statistica, Sinica, 4:639-650 (1994).</p></li>
<li><p>Blei, David and Jordan, Michael. <em>Variational Inference for Dirichlet Process Mixtures</em>. Bayesian Analysis, Vol. 1, No. 1 (2004).</p></li>
<li><p>Pedregosa, et al. <em>Scikit-Learn: Machine Learning in Python</em>. JMLR 12, pp. 2825-2830 (2011).</p></li>
<li><p>Bishop, Christopher. <em>Pattern Recogition and Machine Learning</em>. Springer Ltd (2006).</p></li>
<li><p><em>Sunspot Index and Long-Term Solar Observations</em>. WDC-SILSO, Royal Observatory of Belgium, Brussels (2018).</p></li>
<li><p>Gelman, Andrew. <em>Understanding predictive information criteria for Bayesian models</em>. Statistics and Computing, Springer Link, 2014.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gmm.html" class="btn btn-neutral float-left" title="Gaussian Mixture Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="toy_mixture_model_discrete_enumeration.html" class="btn btn-neutral float-right" title="Example: Toy Mixture Model With Discrete Enumeration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>