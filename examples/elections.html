<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Pyro Tutorials 1.8.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="tracking_1d.html" />
    <link rel="prev" title="&lt;no title&gt;" href="working_memory.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/elections.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<dl>
<dt>{</dt><dd><dl>
<dt>“cells”: [</dt><dd><dl>
<dt>{</dt><dd><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“# Predicting the outcome of a US presidential election using Bayesian optimal experimental designn”,
“n”,
“In this tutorial, we explore the use of optimal experimental design techniques to create an optimal polling strategy to predict the outcome of a US presidential election. In a [previous tutorial](<a class="reference external" href="http://pyro.ai/examples/working_memory.html">http://pyro.ai/examples/working_memory.html</a>), we explored the use of Bayesian optimal experimental design to learn the working memory capacity of a single person. Here, we apply the same concepts to study a whole country.n”,
“n”,
“To begin, we need a Bayesian model of the winner of the election <cite>w</cite>, as well as the outcome <cite>y</cite> of any poll we may plan to conduct. The experimental design is the number of people $n_i$ to poll in each state. To set up our exploratory model, we are going to make a number of simplifying assumptions. We will use historical election data 1976-2012 to construct a plausible prior and the 2016 election as our test set: we imagine that we are conducting polling just before the 2016 election.n”,
“n”,
“## Choosing a priorn”,
“In our model, we include a 51 dimensional latent variabe <cite>alpha</cite>. For each of the 50 states plus DC we define n”,
“n”,
“$$ \alpha_i = \text{logit }\mathbb{P}(\text{a random voter in state } i \text{ votes Democrat in the 2016 election}) $$n”,
“n”,
“and we assume all other voters vote Republican. Right before the election, the value of $\alpha$ is unknown and we wish to estimate it by conducting a poll with $n_i$ people in state $i$ for $i=1, …, 51$ . The winner $w$ of the election is decided by the Electoral College system. The number of electoral college votes gained by the Democrats in state $i$ isn”,
“$$e_i =  \begin{cases}n”,
“k_i \text{ if } \alpha_i &gt; \frac{1}{2} \\n”,
“0 \text{ otherwise}n”,
“\end{cases}n”,
“$$(this is a rough approximation of the true system). All other electoral college votes go to the Republicans. Here $k_i$ is the number of electoral college votes alloted to state $i$, which are listed in the following data frame.”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 3,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“# Data pathn”,
“BASE_URL =  &quot;<a class="reference external" href="https://d2hg8soec8ck9v.cloudfront.net/datasets/us_elections/">https://d2hg8soec8ck9v.cloudfront.net/datasets/us_elections/</a>&quot;”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 4,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>”       Electoral college votesn”,
“State                         n”,
“AL                           9n”,
“AK                           3n”,
“AZ                          11n”,
“AR                           6n”,
“CA                          55n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“import pandas as pdn”,
“import torchn”,
“from urllib.request import urlopenn”,
“n”,
“electoral_college_votes = pd.read_pickle(urlopen(BASE_URL + &quot;electoral_college_votes.pickle&quot;))n”,
“print(electoral_college_votes.head())n”,
“ec_votes_tensor = torch.tensor(electoral_college_votes.values, dtype=torch.float).squeeze()”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“The winner $w$ of the election isn”,
“n”,
“$$ w = \begin{cases}n”,
“\text{Democrats if } \sum_i e_i &gt; \frac{1}{2}\sum_i k_i  \\n”,
“\text{Republicans otherwise}n”,
“\end{cases}n”,
“$$”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“In code, this is expressed as follows”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 5,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“def election_winner(alpha):n”,
”    dem_win_state = (alpha &gt; 0.).float()n”,
”    dem_electoral_college_votes = ec_votes_tensor * dem_win_staten”,
”    w = (dem_electoral_college_votes.sum(-1) / ec_votes_tensor.sum(-1) &gt; .5).float()n”,
”    return w”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We are interested in polling strategies that will help us predict $w$, rather than predicting the more complex state-by-state results $\alpha$.n”,
“n”,
“To set up a fully Bayesian model, we need a prior for $\alpha$. We will base the prior on the outcome of some historical presidential elections. Specifically, we’ll use the following dataset of state-by-state election results for the presidential elections 1976-2012 inclusive. Note that votes for parties other than Democrats and Republicans have been ignored.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 7,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>”          1976                1980                1984           n”,
”      Democrat Republican Democrat Republican Democrat Republicann”,
“State                                                            n”,
“AL      659170     504070   636730     654192   551899     872849n”,
“AK       44058      71555    41842      86112    62007     138377n”,
“AZ      295602     418642   246843     529688   333854     681416n”,
“AR      499614     268753   398041     403164   338646     534774n”,
“CA     3742284    3882244  3083661    4524858  3922519    5467009n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“frame = pd.read_pickle(urlopen(BASE_URL + &quot;us_presidential_election_data_historical.pickle&quot;))n”,
“print(frame[[1976, 1980, 1984]].head())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Based on this data alone, we will base our prior mean for $\alpha$ solely on the 2012 election. Our model will be based on logistic regression, so we will transform the probability of voting Democrat using the logit function. Specifically, we’ll choose a prior mean as follows:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 8,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“results_2012 = torch.tensor(frame[2012].values, dtype=torch.float)n”,
“prior_mean = torch.log(results_2012[…, 0] / results_2012[…, 1])”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Our prior distribution for $\alpha$ will be a multivariate Normal with mean <cite>prior_mean</cite>. The only thing left to decide upon is the covariance matrix. Since <cite>alpha</cite> values are logit-transformed, the covariance will be defined in logit space as well.n”,
“n”,
“<em>Aside</em>: The prior covariance is important in a number of ways. If we allow too much variance, the prior will be uncertain about the outcome in every state, and require polling everywhere. If we allow too little variance, we may be caught off-guard by an unexpected electoral outcome. If we assume states are independent, then we will not be able to pool information across states; but assume too much correlation and we could too faithfully base predictions about one state from poll results in another.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We select the prior covariance by taking the empirical covariance from the elections 1976 - 2012 and adding a small value <cite>0.01</cite> to the diagonal.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 9,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“idx = 2 * torch.arange(10)n”,
“as_tensor = torch.tensor(frame.values, dtype=torch.float)n”,
“logits = torch.log(as_tensor[…, idx] / as_tensor[…, idx + 1]).transpose(0, 1)n”,
“mean = logits.mean(0)n”,
“sample_covariance = (1/(logits.shape[0] - 1)) * (n”,
”    (logits.unsqueeze(-1) - mean) * (logits.unsqueeze(-2) - mean)n”,
“).sum(0)n”,
“prior_covariance = sample_covariance + 0.01 * torch.eye(sample_covariance.shape[0])”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Setting up the modeln”,
“We are now in a position to define our model. At a high-level the model works as follows:n”,
“n”,
“- $\alpha$ is multivariate Normaln”,
“- $w$ is a deterministic function of $\alpha$n”,
“- $y_i$ is Binomial($n_i$, sigmoid($\alpha_i$)) so we are assuming that people respond to the poll in exactly the same way that they will vote on election dayn”,
” n”,
“In Pyro, this model looks as follows”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 10,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“import pyron”,
“import pyro.distributions as distn”,
“n”,
“def model(polling_allocation):n”,
”    # This allows us to run many copies of the model in paralleln”,
”    with pyro.plate_stack(&quot;plate_stack&quot;, polling_allocation.shape[:-1]):n”,
”        # Begin by sampling alphan”,
”        alpha = pyro.sample(&quot;alpha&quot;, dist.MultivariateNormal(n”,
”            prior_mean, covariance_matrix=prior_covariance))n”,
”        n”,
”        # Sample y conditional on alphan”,
”        poll_results = pyro.sample(&quot;y&quot;, dist.Binomial(n”,
”            polling_allocation, logits=alpha).to_event(1))n”,
”        n”,
”        # Now compute w according to the (approximate) electoral college formulan”,
”        dem_win = election_winner(alpha)n”,
”        pyro.sample(&quot;w&quot;, dist.Delta(dem_win))n”,
”        n”,
”        return poll_results, dem_win, alpha”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Understanding the priorn”,
“n”,
“Before we go any further, we’re going to study the model to check it matches with our intuition about US presidential elections.n”,
“n”,
“First of all, let’s look at an upper and lower confidence limit for the proportion of voters who will vote Democrat in each state.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 11,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>”       Lower confidence limit  Upper confidence limitn”,
“State                                                n”,
“AL                   0.272258                0.517586n”,
“AK                   0.330472                0.529117n”,
“AZ                   0.321011                0.593634n”,
“AR                   0.214348                0.576079n”,
“CA                   0.458618                0.756616n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“std = prior_covariance.diag().sqrt()n”,
“ci = pd.DataFrame({&quot;State&quot;: frame.index,n”,
”                   &quot;Lower confidence limit&quot;: torch.sigmoid(prior_mean - 1.96 * std), n”,
”                   &quot;Upper confidence limit&quot;: torch.sigmoid(prior_mean + 1.96 * std)}n”,
”                 ).set_index(&quot;State&quot;)n”,
“print(ci.head())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“The prior on $\alpha$ implicitly defines our prior on <cite>w</cite>. We can investigate this prior by simulating many times from the prior.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 12,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Prior probability of Dem win 0.6799200177192688n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“_, dem_wins, alpha_samples = model(torch.ones(100000, 51))n”,
“prior_w_prob = dem_wins.float().mean()n”,
“print(&quot;Prior probability of Dem win&quot;, prior_w_prob.item())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Since our prior is based on 2012 and the Democrats won in 2012, it makes sense that we would favour a Democrat win in 2016 (this is before we have seen <em>any</em> polling data or incorporated any other information).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We can also investigate which states, a priori, are most marginal.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 13,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>”       Democrat win probabilityn”,
“State                          n”,
“FL                      0.52501n”,
“NC                      0.42730n”,
“NH                      0.61536n”,
“OH                      0.61997n”,
“VA                      0.63738n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“dem_prob = (alpha_samples &gt; 0.).float().mean(0)n”,
“marginal = torch.argsort((dem_prob - .5).abs()).numpy()n”,
“prior_prob_dem = pd.DataFrame({&quot;State&quot;: frame.index[marginal],n”,
”                               &quot;Democrat win probability&quot;: dem_prob.numpy()[marginal]}n”,
”                             ).set_index(‘State’)n”,
“print(prior_prob_dem.head())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“This is a sanity check, and seems to accord with our intuitions. Florida is frequently an important swing state and is top of our list of marginal states under the prior. We can also see states such as Pennsylvania and Wisconsin near the top of the list – we know that these were instrumental in the 2016 election.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Finally, we take a closer look at our prior covariance. Specifically, we examine states that we expect to be more or less correlated. Let’s begin by looking at states in New England”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 14,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“State        ME        VT        NH        MA        RI        CTn”,
“State                                                            n”,
“ME     1.000000  0.817323  0.857351  0.800276  0.822024  0.825383n”,
“VT     0.817323  1.000000  0.834723  0.716342  0.754026  0.844140n”,
“NH     0.857351  0.834723  1.000000  0.871370  0.803803  0.873496n”,
“MA     0.800276  0.716342  0.871370  1.000000  0.813665  0.835148n”,
“RI     0.822024  0.754026  0.803803  0.813665  1.000000  0.849644n”,
“CT     0.825383  0.844140  0.873496  0.835148  0.849644  1.000000n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“import numpy as npn”,
“n”,
“n”,
“def correlation(cov):n”,
”    return cov / np.sqrt(np.expand_dims(np.diag(cov.values), 0) * np.expand_dims(np.diag(cov.values), 1))n”,
”                n”,
“n”,
“new_england_states = [‘ME’, ‘VT’, ‘NH’, ‘MA’, ‘RI’, ‘CT’]n”,
“cov_as_frame = pd.DataFrame(prior_covariance.numpy(), columns=frame.index).set_index(frame.index)n”,
“ne_cov = cov_as_frame.loc[new_england_states, new_england_states]n”,
“ne_corr = correlation(ne_cov)n”,
“print(ne_corr)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Clearly, these states tend to vote similarly. We can also examine some states of the South which we also expect to be similar.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 15,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“State        LA        MS        AL        GA        SCn”,
“State                                                  n”,
“LA     1.000000  0.554020  0.651511  0.523784  0.517672n”,
“MS     0.554020  1.000000  0.699459  0.784371  0.769198n”,
“AL     0.651511  0.699459  1.000000  0.829908  0.723015n”,
“GA     0.523784  0.784371  0.829908  1.000000  0.852818n”,
“SC     0.517672  0.769199  0.723015  0.852818  1.000000n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“southern_states = [‘LA’, ‘MS’, ‘AL’, ‘GA’, ‘SC’]n”,
“southern_cov = cov_as_frame.loc[southern_states, southern_states]n”,
“southern_corr = correlation(southern_cov)n”,
“print(southern_corr)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“These correlation matrices show that, as expected, logical groupings of states tend to have similar voting trends. We now look at the correlations <em>between</em> the groups (e.g. between Maine and Louisiana).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 16,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“State        LA        MS        AL        GA        SCn”,
“State                                                  n”,
“ME     0.329438  0.309352 -0.000534  0.122375  0.333679n”,
“VT    -0.036079  0.009653 -0.366604 -0.202065  0.034438n”,
“NH     0.234105  0.146826 -0.105781  0.008411  0.233084n”,
“MA     0.338411  0.122257 -0.059107 -0.025730  0.182290n”,
“RI     0.314088  0.188819 -0.066307 -0.022142  0.186955n”,
“CT     0.139021  0.074646 -0.205797 -0.107684  0.125023n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“cross_cov = cov_as_frame.loc[new_england_states + southern_states, new_england_states + southern_states]n”,
“cross_corr = correlation(cross_cov)n”,
“print(cross_corr.loc[new_england_states, southern_states])”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Now, we see weaker correlation between New England states and Southern states than the correlation within those grouping. Again, this is as expected.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Measuring the expected information gain of a polling strategyn”,
“The prior we have set up appears to accord, at least approximately, with intuition. However, we now want to add a second source of information from polling. We aim to use our prior to select a polling strategy that will be most informative about our target $w$. A polling strategy, in this simplified set-up, is the number of people to poll in each state. (We ignore any other covariates such as regional variation inside states, demographics, etc.) We might imagine that polling 1000 people in Florida (the most marginal state), will be much more effective than polling 1000 people in DC (the least marginal state). That’s because the outcome in DC is already quite predictable, just based on our prior, whereas the outcome in Florida is really up for grabs.n”,
“n”,
“In fact, the information that our model will gain about $w$ based on conducting a poll with design $d$ and getting outcome $y$ can be described mathematically as follows:n”,
“n”,
“$$\text{IG}(d, y) = KL(p(w|y,d)||p(w)).$$n”,
“n”,
“Since the outcome of the poll is at present unknown, we consider the expected information gain [1]n”,
“n”,
“$$\text{EIG}(d) = \mathbb{E}_{p(y|d)}[KL(p(w|y,d)||p(w))].$$n”,
“n”,
“### Variational estimators of EIGn”,
“n”,
“In the [working memory tutorial](http://pyro.ai/examples/working_memory.html), we used the ‘marginal’ estimator to find the EIG. This involved estimating the marginal density $p(y|d)$. In this experiment, that would be relatively difficult: $y$ is 51-dimensional with some rather tricky constraints that make modelling its density difficult. Furthermore, the marginal estimator requires us to know $p(y|w)$ analytically, which we do not.n”,
“n”,
“Fortunately, other variational estimators of EIG exist: see [2] for more details. One such variational estimator is the ‘posterior’ estimator, based on the following representationn”,
“n”,
“$$\text{EIG}(d) = \max_q \mathbb{E}_{p(w, y|d)}\left[\log q(w|y) \right] + H(p(w)).$$n”,
“n”,
“Here, $H(p(w))$ is the prior entropy on $w$ (we can compute this quite easily). The important term involves the variational approximation $q(w|y)$. This $q$ can be used to perform amortized variational inference. Specifically, it takes as input $y$ and outputs a distribution over $w$. The bound is maximised when $q(w|y) = p(w|y)$ [2]. Since $w$ is a binary random variable, we can think of $q$ as a classifier that tries to decide, based on the poll outcome, who the eventual winner of the election will be. In this notebook, $q$ will be a neural classifier. Training a neural classifier is a fair bit easier than learning the marginal density of $y$, so we adopt this method to estimate the EIG in this tutorial.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 17,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“from torch import nnn”,
“n”,
“class OutcomePredictor(nn.Module):n”,
”    n”,
”    def __init__(self):n”,
”        super().__init__()n”,
”        self.h1 = nn.Linear(51, 64)n”,
”        self.h2 = nn.Linear(64, 64)n”,
”        self.h3 = nn.Linear(64, 1)n”,
”        n”,
”    def compute_dem_probability(self, y):n”,
”        z = nn.functional.relu(self.h1(y))n”,
”        z = nn.functional.relu(self.h2(z))n”,
”        return self.h3(z)n”,
”    n”,
”    def forward(self, y_dict, design, observation_labels, target_labels):n”,
”        n”,
”        pyro.module(&quot;posterior_guide&quot;, self)n”,
”        n”,
”        y = y_dict[&quot;y&quot;]n”,
”        dem_prob = self.compute_dem_probability(y).squeeze()n”,
”        pyro.sample(&quot;w&quot;, dist.Bernoulli(logits=dem_prob))”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We’ll now use this to compute the EIG for several possible polling strategies. First, we need to compute the $H(p(w))$ term in the above formula.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 18,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“prior_entropy = dist.Bernoulli(prior_w_prob).entropy()”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Let’s consider four simple polling strategies.n”,
” 1. Poll 1000 people in Florida onlyn”,
” 2. Poll 1000 people in DC onlyn”,
” 3. Poll 1000 people spread evenly over the USn”,
” 4. Using a polling allocation that focuses on swing states”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 19,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“from collections import OrderedDictn”,
“n”,
“poll_in_florida = torch.zeros(51)n”,
“poll_in_florida[9] = 1000n”,
“n”,
“poll_in_dc = torch.zeros(51)n”,
“poll_in_dc[8] = 1000n”,
“n”,
“uniform_poll = (1000 // 51) * torch.ones(51)n”,
“n”,
“# The swing score measures how close the state is to 50/50n”,
“swing_score = 1. / (.5 - torch.tensor(prior_prob_dem.sort_values(&quot;State&quot;).values).squeeze()).abs()n”,
“swing_poll = 1000 * swing_score / swing_score.sum()n”,
“swing_poll = swing_poll.round()n”,
“n”,
“poll_strategies = OrderedDict([(&quot;Florida&quot;, poll_in_florida),n”,
”                               (&quot;DC&quot;, poll_in_dc),n”,
”                               (&quot;Uniform&quot;, uniform_poll),n”,
”                               (&quot;Swing&quot;, swing_poll)])”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We’ll now compute the EIG for each option. Since this requires training the network (four times) it may take several minutes.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 20,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Florida 0.3088182508945465n”,
“DC 0.000973820686340332n”,
“Uniform 0.30316317081451416n”,
“Swing 0.3254041075706482n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“from pyro.contrib.oed.eig import posterior_eign”,
“from pyro.optim import Adamn”,
“n”,
“eigs = {}n”,
“best_strategy, best_eig = None, 0n”,
“n”,
“for strategy, allocation in poll_strategies.items():n”,
”    print(strategy, end=&quot; &quot;)n”,
”    guide = OutcomePredictor()n”,
”    pyro.clear_param_store()n”,
”    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))n”,
”    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each runn”,
”    # The return value of <cite>posterior_eig</cite> is then -E_p(w,y)[log q(w|y)]n”,
”    ape = posterior_eig(model, allocation, &quot;y&quot;, &quot;w&quot;, 10, 12500, guide, n”,
”                        Adam({&quot;lr&quot;: 0.001}), eig=False, final_num_samples=10000)n”,
”    eigs[strategy] = prior_entropy - apen”,
”    print(eigs[strategy].item())n”,
”    if eigs[strategy] &gt; best_eig:n”,
”        best_strategy, best_eig = strategy, eigs[strategy]”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Running the experimentn”,
“n”,
“We have now scored our four candidate designs and can choose the best one to use to actually gather new data. In this notebook, we will simulate the new data using the results from the 2016 election. Specifically, we will assume that the outcome of the poll comes from our model, where we condition the value of <cite>alpha</cite> to correspond to the actual results in 2016.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“First, we retrain $q$ with the chosen polling strategy.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 21,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><dl>
<dt>“data”: {</dt><dd><dl class="simple">
<dt>“text/plain”: [</dt><dd><p>“tensor(0.3653, grad_fn=&lt;DivBackward0&gt;)”</p>
</dd>
</dl>
<p>]</p>
</dd>
</dl>
<p>},
“execution_count”: 21,
“metadata”: {},
“output_type”: “execute_result”</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“best_allocation = poll_strategies[best_strategy]n”,
“pyro.clear_param_store()n”,
“guide = OutcomePredictor()n”,
“posterior_eig(model, best_allocation, &quot;y&quot;, &quot;w&quot;, 10, 12500, guide, n”,
”              Adam({&quot;lr&quot;: 0.001}), eig=False)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“The value of $\alpha$ implied by the 2016 results is computed in the same way we computed the prior.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 23,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“test_data = pd.read_pickle(urlopen(BASE_URL + &quot;us_presidential_election_data_test.pickle&quot;))n”,
“results_2016 = torch.tensor(test_data.values, dtype=torch.float)n”,
“true_alpha = torch.log(results_2016[…, 0] / results_2016[…, 1])”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 24,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“conditioned_model = pyro.condition(model, data={&quot;alpha&quot;: true_alpha})n”,
“y, _, _ = conditioned_model(best_allocation)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Let’s view the outcome of our poll.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 25,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>”       Number of people polled  Number who said they would vote Democratn”,
“State                                                                   n”,
“FL                       191.0                                      89.0n”,
“NE                        66.0                                      30.0n”,
“NJ                        41.0                                      22.0n”,
“OH                        40.0                                      19.0n”,
“VT                        35.0                                      22.0n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“outcome = pd.DataFrame({&quot;State&quot;: frame.index,n”,
”                        &quot;Number of people polled&quot;: best_allocation, n”,
”                        &quot;Number who said they would vote Democrat&quot;: y}n”,
”                      ).set_index(&quot;State&quot;)n”,
“print(outcome.sort_values([&quot;Number of people polled&quot;, &quot;State&quot;], ascending=[False, True]).head())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Analysing the datan”,
“Having collected our data, we can now perform inference under our model to obtain the posterior probability of a Democrat win. There are many ways to perform the inference: for instance we could use variational inference with Pyro’s <cite>SVI</cite> or MCMC such as Pyro’s <cite>NUTS</cite>. Using these methods, we would compute the posterior over <cite>alpha</cite>, and use this to obtain the posterior over <cite>w</cite>.n”,
“n”,
“However, a quick way to analyze the data is to use the neural network we have already trained. At convergence, we expect the network to give a good approximation to the true posterior, i.e. $q(w|y) \approx p(w|y)$.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 26,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“q_w = torch.sigmoid(guide.compute_dem_probability(y).squeeze().detach())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 27,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Prior probability of Democrat win 0.6799200177192688n”,
“Posterior probability of Democrat win 0.40527692437171936n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“print(&quot;Prior probability of Democrat win&quot;, prior_w_prob.item())n”,
“print(&quot;Posterior probability of Democrat win&quot;, q_w.item())”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“This completes the whole process of experimental design, obtaining data, and data analysis. By using our analysis model and prior to design the polling strategy, we were able to choose the design that led to the greatest expected gain in information.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Conclusionsn”,
“n”,
“In this tutorial, we showed how optimal experimental design principles can be applied to electoral polling. If we have the option of first designing the polling strategy using our prior, this can help lead to better predictions once we have collected our data.n”,
“n”,
“Our model could be enhanced in a number of ways: a useful guide to the kinds of models used in political science can be found in [3]. Our optimal design strategy will only ever be as good as our prior, so there is certainly a lot of use in investing time in choosing a good prior.n”,
“n”,
“It might also be possible to search over the possible polling strategies in a more sophisticated way. For instance, simulated annealing [4] is one technique that can be used for optimization in high-dimensional discrete spaces.n”,
“n”,
“Our polling strategies were chosen with one objective in mind: predict the final outcome $w$. If, on the other hand, we wanted to make more fine-grained predictions, we could use the same procedure but treat $\alpha$ as our target instead of $w$.n”,
“n”,
“Finally, it’s worth noting that there is a connection to Bayesian model selection: specifically, in a different model we might have $w$ a binary random variable to choose between different sub-models. Although conceptually different, our election set-up and the model selection set-up share many mathematical features which means that a similar experimental design approach could be used in both cases.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Referencesn”,
“n”,
“[1] Chaloner, K. and Verdinelli, I., 1995. <strong>Bayesian experimental design: A review.</strong> Statistical Science, pp.273-304.n”,
“n”,
“[2] Foster, A., Jankowiak, M., Bingham, E., Horsfall, P., Teh, Y.W., Rainforth, T. and Goodman, N., 2019. <strong>Variational Bayesian Optimal Experimental Design.</strong> Advances in Neural Information Processing Systems 2019 (to appear).n”,
“n”,
“[3] Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A. and Rubin, D.B., 2013. <strong>Bayesian data analysis.</strong> Chapman and Hall/CRC.n”,
“n”,
“[4] Kirkpatrick, S., Gelatt, C.D. and Vecchi, M.P., 1983. <strong>Optimization by simulated annealing.</strong> Science, 220(4598), pp.671-680.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>],
“metadata”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“kernelspec”: {</dt><dd><p>“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”</p>
</dd>
</dl>
<p>},
“language_info”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“codemirror_mode”: {</dt><dd><p>“name”: “ipython”,
“version”: 3</p>
</dd>
</dl>
<p>},
“file_extension”: “.py”,
“mimetype”: “text/x-python”,
“name”: “python”,
“nbconvert_exporter”: “python”,
“pygments_lexer”: “ipython3”,
“version”: “3.6.10”</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>},
“nbformat”: 4,
“nbformat_minor”: 2</p>
</dd>
</dl>
<p>}</p>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="working_memory.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tracking_1d.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>