<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains &mdash; Pyro Tutorials 1.8.4 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Interactive posterior predictives checks" href="prior_predictive.html" />
    <link rel="prev" title="Modules in Pyro" href="modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introductory Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_long.html">Introduction to Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_rendering.html">Automatic rendering of Pyro models</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iv.html">SVI Part IV: Tips and Tricks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">Bayesian Regression - Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">Bayesian Regression - Inference Algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules in Pyro</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Table-of-contents">Table of contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Running-example:-SARS-CoV-2-strain-prediction">Running example: SARS-CoV-2 strain prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Clean-the-data">Clean the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Create-a-generative-model">Create a generative model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Sanity-check-using-mean-field-inference">Sanity check using mean field inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Create-an-initialization-heuristic">Create an initialization heuristic</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Reparametrize-the-model">Reparametrize the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Customize-the-variational-family">Customize the variational family</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="prior_predictive.html">Interactive posterior predictives checks</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Using the PyTorch JIT Compiler with Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="cvae.html">Conditional Variational Auto-encoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="normalizing_flows_i.html">Normalizing Flows - Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="prodlda.html">Probabilistic Topic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="scanvi.html"><em>scANVI: Deep Generative Modeling for Single Cell Data with Pyro</em></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dirichlet_process_mixture.html">Dirichlet Process Mixture Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mle_map.html">MLE and MAP Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mle_map.html#Doing-the-same-thing-with-AutoGuides">Doing the same thing with AutoGuides</a></li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">Customizing SVI objectives and training loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="boosting_bbvi.html">Boosting Black Box Variational Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecasting_i.html">Forecasting I: univariate, heavy tailed</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_ii.html">Forecasting II: state space models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_iii.html">Forecasting III: hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_dlm.html">Forecasting with Dynamic Linear Model (DLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="stable.html">Levy Stable models of Stochastic Volatility</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_intro.html">Epidemiological models: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="logistic-growth.html">Logistic growth models of SARS-CoV-2 lineage proportions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Experimental Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="working_memory.html">Designing Adaptive Experiments to Study Working Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="elections.html">Predicting the outcome of a US presidential election using Bayesian optimal experimental design</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Object Tracking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
<li class="toctree-l1"><a class="reference internal" href="predictive_deterministic.html">Example: Utilizing Predictive and Deterministic with MCMC and SVI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: A Guide to Programming with Effect Handlers in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib_funsor_intro_i.html"><code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code>, a new backend for Pyro - New primitives (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib_funsor_intro_ii.html"><code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code>, a new backend for Pyro - Building inference algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deprecated</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">(DEPRECATED) An Introduction to Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">(DEPRECATED) An Introduction to Inference in Pyro</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/workflow.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="High-dimensional-Bayesian-workflow,-with-applications-to-SARS-CoV-2-strains">
<h1>High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains<a class="headerlink" href="#High-dimensional-Bayesian-workflow,-with-applications-to-SARS-CoV-2-strains" title="Permalink to this heading">¶</a></h1>
<p>This tutorial describes a workflow for incrementally building pipelines to analyze high-dimensional data in Pyro. This workflow has evolved over a few years of applying Pyro to models with <span class="math notranslate nohighlight">\(10^5\)</span> or more latent variables. We build on <a class="reference external" href="https://arxiv.org/abs/2011.01808">Gelman et al. (2020)</a>’s concept of <em>Bayesian workflow</em>, and focus on aspects particular to high-dimensional models: approximate inference and numerical stability. While the individual components of the pipeline deserve
their own tutorials, this tutorial focuses on incrementally combining those components.</p>
<p>The fastest way to find a good model of your data is to quickly discard many bad models, i.e. to iterate. In statistics we call this iterative workflow <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/Blei2014b.pdf">Box’s loop</a>. An efficient workflow allows us to discard bad models as quickly as possible. Workflow efficiency demands that code changes to upstream components don’t break previous coding effort on downstream components. Pyro’s approaches to this challenge include strategies for
variational approximations (<a class="reference external" href="https://docs.pyro.ai/en/stable/infer.autoguide.html">pyro.infer.autoguide</a>) and strategies for transforming model coordinate systems to improve geometry (<a class="reference external" href="https://docs.pyro.ai/en/stable/infer.reparam.html">pyro.infer.reparam</a>).</p>
<section id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Great models can only be achieved by iterative development.</p></li>
<li><p>Iterate quickly by building a pipeline that is robust to code changes.</p></li>
<li><p>Start with a simple model and <a class="reference external" href="https://docs.pyro.ai/en/dev/infer.autoguide.html#autonormal">mean-field inference</a>.</p></li>
<li><p>Avoid NANs by intelligently <a class="reference external" href="https://docs.pyro.ai/en/dev/infer.autoguide.html#module-pyro.infer.autoguide.initialization">initializing</a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.clamp.html">.clamp()</a>ing.</p></li>
<li><p><a class="reference external" href="https://docs.pyro.ai/en/dev/infer.reparam.html">Reparametrize</a> the model to improve geometry.</p></li>
<li><p>Create a custom variational family by combining <a class="reference external" href="https://docs.pyro.ai/en/dev/infer.autoguide.html">AutoGuides</a> or <a class="reference external" href="https://docs.pyro.ai/en/dev/contrib.easyguide.html">EasyGuides</a>.</p></li>
</ul>
</section>
<section id="Table-of-contents">
<h2>Table of contents<a class="headerlink" href="#Table-of-contents" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#Overview">Overview</a></p></li>
<li><p><a class="reference external" href="#Running-example">Running example: SARS-CoV-2 strain prediction</a></p></li>
</ul>
<ol class="arabic simple">
<li><p><a class="reference external" href="#Clean-the-data">Clean the data</a></p></li>
<li><p><a class="reference external" href="#Create-a-generative-model">Create a generative model</a></p></li>
<li><p><a class="reference external" href="#Sanity-check">Sanity check using mean-field inference</a></p></li>
<li><p><a class="reference external" href="#Create-an-initialization-heuristic">Create an initialization heuristic</a></p></li>
<li><p><a class="reference external" href="#Reparametrize">Reparametrize the model</a></p></li>
<li><p><a class="reference external" href="#Customize">Customize the variational family: autoguides, easyguides, custom guides</a></p></li>
</ol>
<section id="Overview">
<h3>Overview<a class="headerlink" href="#Overview" title="Permalink to this heading">¶</a></h3>
<p>Consider the problem of sampling from the posterior distribution of a probabilistic model with <span class="math notranslate nohighlight">\(10^5\)</span> or more continuous latent variables, but whose data fits entirely in memory. (For larger datasets, consider <a class="reference external" href="http://pyro.ai/examples/svi_part_ii.html">amortized variational inference</a>.) Inference in such high-dimensional models can be challenging even when posteriors are known to be <a class="reference external" href="https://en.wikipedia.org/wiki/Unimodality">unimodal</a> or even
<a class="reference external" href="https://arxiv.org/abs/1404.5886">log-concave</a>, due to correlations among latent variables.</p>
<p>To perform inference in such high-dimensional models in Pyro, we have evolved a <a class="reference external" href="https://arxiv.org/abs/2011.01808">workflow</a> to incrementally build data analysis pipelines combining variational inference, reparametrization effects, and ad-hoc initialization strategies. Our workflow is summarized as a sequence of steps, where validation after any step might suggest backtracking to change design decisions at a previous step.</p>
<ol class="arabic simple">
<li><p>Clean the data.</p></li>
<li><p>Create a generative model.</p></li>
<li><p>Sanity check using MAP or mean-field inference.</p></li>
<li><p>Create an initialization heuristic.</p></li>
<li><p>Reparameterize the model, evaluating results under mean field VI.</p></li>
<li><p>Customize the variational family (autoguides, easyguides, custom guides).</p></li>
</ol>
<p>The crux of efficient workflow is to ensure changes don’t break your pipeline. That is, after you build a number of pipeline stages, validate results, and decide to change one component in the pipeline, you’d like to minimize code changes needed in other components. The remainder of this tutorial describes these steps individually, then describes nuances of interactions among stages, then provides an example.</p>
</section>
<section id="Running-example:-SARS-CoV-2-strain-prediction">
<h3>Running example: SARS-CoV-2 strain prediction<a class="headerlink" href="#Running-example:-SARS-CoV-2-strain-prediction" title="Permalink to this heading">¶</a></h3>
<p>The running example in this tutorial will be a model <a class="reference external" href="https://www.medrxiv.org/content/10.1101/2021.09.07.21263228v2">(Obermeyer et al. 2022)</a> of the relative growth rates of different strains of the SARS-CoV-2 virus, based on <a class="reference external" href="https://docs.nextstrain.org/projects/ncov/en/latest/reference/remote_inputs.html">open data</a> counting different <a class="reference external" href="https://cov-lineages.org/">PANGO lineages</a> of viral genomic samples collected at different times around the world. There are about 2 million sequences
in total.</p>
<p>The model is a high-dimensional regression model with around 1000 coefficients, a multivariate logistic growth function (using a simple <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html">torch.softmax()</a>) and a <a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#multinomial">Multinomial</a> likelihood. While the number of coefficients is relatively small, there are about 500,000 local latent variables to estimate, and plate structure in the model should lead to an
approximately block diagonal posterior covariance matrix. For an introduction to simple logistic growth models using this same dataset, see the <a class="reference external" href="logistic-growth.html">logistic growth tutorial</a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from collections import defaultdict
from pprint import pprint
import functools
import math
import os
import torch
import pyro
import pyro.distributions as dist
import pyro.poutine as poutine
from pyro.distributions import constraints
from pyro.infer import SVI, Trace_ELBO
from pyro.infer.autoguide import (
    AutoDelta,
    AutoNormal,
    AutoMultivariateNormal,
    AutoLowRankMultivariateNormal,
    AutoGuideList,
    init_to_feasible,
)
from pyro.infer.reparam import AutoReparam, LocScaleReparam
from pyro.nn.module import PyroParam
from pyro.optim import ClippedAdam
from pyro.ops.special import sparse_multinomial_likelihood
import matplotlib.pyplot as plt

if torch.cuda.is_available():
    print(&quot;Using GPU&quot;)
    torch.set_default_tensor_type(&quot;torch.cuda.FloatTensor&quot;)
else:
    print(&quot;Using CPU&quot;)
smoke_test = (&#39;CI&#39; in os.environ)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using CPU
</pre></div></div>
</div>
</section>
<section id="Clean-the-data">
<h3>Clean the data<a class="headerlink" href="#Clean-the-data" title="Permalink to this heading">¶</a></h3>
<p>Our running example will use a pre-cleaned dataset. We started with Nextstrain’s <a class="reference external" href="https://docs.nextstrain.org/projects/ncov/en/latest/reference/remote_inputs.html">ncov</a> tool for preprocessing, followed by the Broad Institute’s <a class="reference external" href="https://github.com/broadinstitute/pyro-cov/blob/master/scripts/preprocess_nextstrain.py">pyro-cov</a> tool for aggregation, resulting in a dataset of SARS-CoV-2 lineages observed around the world through time.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pyro.contrib.examples.nextstrain import load_nextstrain_counts
dataset = load_nextstrain_counts()

def summarize(x, name=&quot;&quot;):
    if isinstance(x, dict):
        for k, v in sorted(x.items()):
            summarize(v, name + &quot;.&quot; + k if name else k)
    elif isinstance(x, torch.Tensor):
        print(f&quot;{name}: {type(x).__name__} of shape {tuple(x.shape)} on {x.device}&quot;)
    elif isinstance(x, list):
        print(f&quot;{name}: {type(x).__name__} of length {len(x)}&quot;)
    else:
        print(f&quot;{name}: {type(x).__name__}&quot;)
summarize(dataset)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
counts: Tensor of shape (27, 202, 1316) on cpu
features: Tensor of shape (1316, 2634) on cpu
lineages: list of length 1316
locations: list of length 202
mutations: list of length 2634
sparse_counts.index: Tensor of shape (3, 57129) on cpu
sparse_counts.total: Tensor of shape (27, 202) on cpu
sparse_counts.value: Tensor of shape (57129,) on cpu
start_date: datetime
time_step_days: int
</pre></div></div>
</div>
</section>
<section id="Create-a-generative-model">
<h3>Create a generative model<a class="headerlink" href="#Create-a-generative-model" title="Permalink to this heading">¶</a></h3>
<p>The first step to using Pyro is creating a generative model, either a python function or a <a class="reference external" href="https://docs.pyro.ai/en/dev/nn.html#pyro.nn.module.PyroModule">pyro.nn.Module</a>. Start simple. Start with a shallow hierarchy and later add latent variables to share statistical strength. Start with a slice of your data then add a <a class="reference external" href="https://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.plate">plate</a> over multiple slices. Start with simple distributions like
<a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.Normal">Normal</a>, <a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.LogNormal">LogNormal</a>, <a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.Poisson">Poisson</a> and <a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.Multinomial">Multinomial</a>, then consider overdispersed versions like
<a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.StudentT">StudentT</a>, <a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.Gamma">Gamma</a>, <a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.GammaPoisson">GammaPoisson</a>/<a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.NegativeBinomial">NegativeBinomial</a>, and
<a class="reference external" href="https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.DirichletMultinomial">DirichletMultinomial</a>. Keep your model simple and readable so you can share it and get feedback from domain experts. Use <a class="reference external" href="http://www.stat.columbia.edu/~gelman/presentations/weakpriorstalk.pdf">weakly informative priors</a>.</p>
<p>We’ll focus on a multivariate logistic growth model of competing SARS-CoV-2 strains, as described in <a class="reference external" href="https://www.medrxiv.org/content/10.1101/2021.09.07.21263228v2">Obermeyer et al. (2022)</a>. This model uses a numerically stable <code class="docutils literal notranslate"><span class="pre">logits</span></code> parameter in its multinomial likelihood, rather than a <code class="docutils literal notranslate"><span class="pre">probs</span></code> parameter. Similarly upstream variables <code class="docutils literal notranslate"><span class="pre">init</span></code>, <code class="docutils literal notranslate"><span class="pre">rate</span></code>, <code class="docutils literal notranslate"><span class="pre">rate_loc</span></code>, and <code class="docutils literal notranslate"><span class="pre">coef</span></code> are all in log-space. This will mean e.g. that a zero coefficient has multiplicative effect of 1.0, and a
positive coefficient has multiplicative effect greater than 1.</p>
<p>Note we scale <code class="docutils literal notranslate"><span class="pre">coef</span></code> by 1/100 because we want to model a very small number, but the automatic parts of Pyro and PyTorch work best for numbers on the order of 1.0 rather than very small numbers. When we later interpret <code class="docutils literal notranslate"><span class="pre">coef</span></code> in a volcano plot we’ll need to duplicate this scaling factor.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def model(dataset):
    features = dataset[&quot;features&quot;]
    counts = dataset[&quot;counts&quot;]
    assert features.shape[0] == counts.shape[-1]
    S, M = features.shape
    T, P, S = counts.shape
    time = torch.arange(float(T)) * dataset[&quot;time_step_days&quot;] / 5.5
    time -= time.mean()
    strain_plate = pyro.plate(&quot;strain&quot;, S, dim=-1)
    place_plate = pyro.plate(&quot;place&quot;, P, dim=-2)
    time_plate = pyro.plate(&quot;time&quot;, T, dim=-3)

    # Model each region as multivariate logistic growth.
    rate_scale = pyro.sample(&quot;rate_scale&quot;, dist.LogNormal(-4, 2))
    init_scale = pyro.sample(&quot;init_scale&quot;, dist.LogNormal(0, 2))
    with pyro.plate(&quot;mutation&quot;, M, dim=-1):
        coef = pyro.sample(&quot;coef&quot;, dist.Laplace(0, 0.5))
    with strain_plate:
        rate_loc = pyro.deterministic(&quot;rate_loc&quot;, 0.01 * coef @ features.T)
    with place_plate, strain_plate:
        rate = pyro.sample(&quot;rate&quot;, dist.Normal(rate_loc, rate_scale))
        init = pyro.sample(&quot;init&quot;, dist.Normal(0, init_scale))
    logits = init + rate * time[:, None, None]

    # Observe sequences via a multinomial likelihood.
    with time_plate, place_plate:
        pyro.sample(
            &quot;obs&quot;,
            dist.Multinomial(logits=logits.unsqueeze(-2), validate_args=False),
            obs=counts.unsqueeze(-2),
        )
</pre></div>
</div>
</div>
<p>The execution cost of this model is dominated by the multinomial likelihood over a large sparse count matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;counts has {:d} / {} nonzero elements&quot;.format(
    dataset[&#39;counts&#39;].count_nonzero(), dataset[&#39;counts&#39;].numel()
))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
counts has 57129 / 7177464 nonzero elements
</pre></div></div>
</div>
<p>To speed up inference (and model iteration!) we’ll replace the <code class="docutils literal notranslate"><span class="pre">pyro.sample(...,</span> <span class="pre">Multinomial)</span></code> likelihood with an equivalent but much cheaper <code class="docutils literal notranslate"><span class="pre">pyro.factor</span></code> statement using a helper <code class="docutils literal notranslate"><span class="pre">pyro.ops.sparse_multinomial_likelihood</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def model(dataset, predict=None):
    features = dataset[&quot;features&quot;]
    counts = dataset[&quot;counts&quot;]
    sparse_counts = dataset[&quot;sparse_counts&quot;]
    assert features.shape[0] == counts.shape[-1]
    S, M = features.shape
    T, P, S = counts.shape
    time = torch.arange(float(T)) * dataset[&quot;time_step_days&quot;] / 5.5
    time -= time.mean()

    # Model each region as multivariate logistic growth.
    rate_scale = pyro.sample(&quot;rate_scale&quot;, dist.LogNormal(-4, 2))
    init_scale = pyro.sample(&quot;init_scale&quot;, dist.LogNormal(0, 2))
    with pyro.plate(&quot;mutation&quot;, M, dim=-1):
        coef = pyro.sample(&quot;coef&quot;, dist.Laplace(0, 0.5))
    with pyro.plate(&quot;strain&quot;, S, dim=-1):
        rate_loc = pyro.deterministic(&quot;rate_loc&quot;, 0.01 * coef @ features.T)
        with pyro.plate(&quot;place&quot;, P, dim=-2):
            rate = pyro.sample(&quot;rate&quot;, dist.Normal(rate_loc, rate_scale))
            init = pyro.sample(&quot;init&quot;, dist.Normal(0, init_scale))
    if predict is not None:  # Exit early during evaluation.
        probs = (init + rate * time[predict]).softmax(-1)
        return probs
    logits = (init + rate * time[:, None, None]).log_softmax(-1)

    # Observe sequences via a cheap sparse multinomial likelihood.
    t, p, s = sparse_counts[&quot;index&quot;]
    pyro.factor(
        &quot;obs&quot;,
        sparse_multinomial_likelihood(
            sparse_counts[&quot;total&quot;], logits[t, p, s], sparse_counts[&quot;value&quot;]
        )
    )
</pre></div>
</div>
</div>
</section>
<section id="Sanity-check-using-mean-field-inference">
<h3>Sanity check using mean field inference<a class="headerlink" href="#Sanity-check-using-mean-field-inference" title="Permalink to this heading">¶</a></h3>
<p>Mean field Normal inference is cheap and robust, and is a good way to sanity check your posterior point estimate, even if the posterior uncertainty may be implausibly narrow. We recommend starting with an <a class="reference external" href="https://docs.pyro.ai/en/latest/infer.autoguide.html#autonormal">AutoNormal</a> guide, and possibly setting <code class="docutils literal notranslate"><span class="pre">init_scale</span></code> to a small value like <code class="docutils literal notranslate"><span class="pre">init_scale=0.01</span></code> or <code class="docutils literal notranslate"><span class="pre">init_scale=0.001</span></code>.</p>
<p>Note that while MAP estimating via <a class="reference external" href="https://docs.pyro.ai/en/latest/infer.autoguide.html#autodelta">AutoDelta</a> is even cheaper and more robust than mean-field <code class="docutils literal notranslate"><span class="pre">AutoNormal</span></code>, <code class="docutils literal notranslate"><span class="pre">AutoDelta</span></code> is coordinate-system dependent and is not invariant to reparametrization. Because in our experience most models benefit from some reparameterization, we recommend <code class="docutils literal notranslate"><span class="pre">AutoNormal</span></code> over <code class="docutils literal notranslate"><span class="pre">AutoDelta</span></code> because <code class="docutils literal notranslate"><span class="pre">AutoNormal</span></code> is less sensitive to reparametrization (<code class="docutils literal notranslate"><span class="pre">AutoDelta</span></code> can give incorrect results in some
reparametrized models).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def fit_svi(model, guide, lr=0.01, num_steps=1001, log_every=100, plot=True):
    pyro.clear_param_store()
    pyro.set_rng_seed(20211205)
    if smoke_test:
        num_steps = 2

    # Measure model and guide complexity.
    num_latents = sum(
        site[&quot;value&quot;].numel()
        for name, site in poutine.trace(guide).get_trace(dataset).iter_stochastic_nodes()
        if not site[&quot;infer&quot;].get(&quot;is_auxiliary&quot;)
    )
    num_params = sum(p.unconstrained().numel() for p in pyro.get_param_store().values())
    print(f&quot;Found {num_latents} latent variables and {num_params} learnable parameters&quot;)

    # Save gradient norms during inference.
    series = defaultdict(list)
    def hook(g, series):
        series.append(torch.linalg.norm(g.reshape(-1), math.inf).item())
    for name, value in pyro.get_param_store().named_parameters():
        value.register_hook(
            functools.partial(hook, series=series[name + &quot; grad&quot;])
        )

    # Train the guide.
    optim = ClippedAdam({&quot;lr&quot;: lr, &quot;lrd&quot;: 0.1 ** (1 / num_steps)})
    svi = SVI(model, guide, optim, Trace_ELBO())
    num_obs = int(dataset[&quot;counts&quot;].count_nonzero())
    for step in range(num_steps):
        loss = svi.step(dataset) / num_obs
        series[&quot;loss&quot;].append(loss)
        median = guide.median()  # cheap for autoguides
        for name, value in median.items():
            if value.numel() == 1:
                series[name + &quot; mean&quot;].append(float(value))
        if step % log_every == 0:
            print(f&quot;step {step: &gt;4d} loss = {loss:0.6g}&quot;)

    # Plot series to assess convergence.
    if plot:
        plt.figure(figsize=(6, 6))
        for name, Y in series.items():
            if name == &quot;loss&quot;:
                plt.plot(Y, &quot;k--&quot;, label=name, zorder=0)
            elif name.endswith(&quot; mean&quot;):
                plt.plot(Y, label=name, zorder=-1)
            else:
                plt.plot(Y, label=name, alpha=0.5, lw=1, zorder=-2)
        plt.xlabel(&quot;SVI step&quot;)
        plt.title(&quot;loss, scalar parameters, and gradient norms&quot;)
        plt.yscale(&quot;log&quot;)
        plt.xscale(&quot;symlog&quot;)
        plt.xlim(0, None)
        plt.legend(loc=&quot;best&quot;, fontsize=8)
        plt.tight_layout()
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
guide = AutoNormal(model, init_scale=0.01)
fit_svi(model, guide)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found 538452 latent variables and 1068600 learnable parameters
step    0 loss = 273.123
step  100 loss = 63.2423
step  200 loss = 44.9539
step  300 loss = 34.8813
step  400 loss = 30.4243
step  500 loss = 27.5258
step  600 loss = 25.4543
step  700 loss = 23.9134
step  800 loss = 22.7201
step  900 loss = 21.8574
step 1000 loss = 21.2031
CPU times: user 3min 4s, sys: 2min 48s, total: 5min 52s
Wall time: 1min 47s
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_15_1.png" src="_images/workflow_15_1.png" />
</div>
</div>
<p>After each change to the model or inference, you’ll validate model outputs, closing <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/Blei2014b.pdf">Box’s loop</a>. In our running example we’ll quantitiatively evaluate using the mean average error (MAE) over the last fully-observed time step.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def mae(true_counts, pred_probs):
    &quot;&quot;&quot;Computes mean average error between counts and predicted probabilities.&quot;&quot;&quot;
    pred_counts = pred_probs * true_counts.sum(-1, True)
    error = (true_counts - pred_counts).abs().sum(-1)
    total = true_counts.sum(-1).clamp(min=1)
    return (error / total).mean().item()

def evaluate(
    model, guide, num_particles=100, location=&quot;USA / Massachusetts&quot;, time=-2
):
    if smoke_test:
        num_particles = 4
    &quot;&quot;&quot;Evaluate posterior predictive accuracy at the last fully observed time step.&quot;&quot;&quot;
    with torch.no_grad(), poutine.mask(mask=False):  # makes computations cheaper
        with pyro.plate(&quot;particle&quot;, num_particles, dim=-3):  # vectorizes
            guide_trace = poutine.trace(guide).get_trace(dataset)
            probs = poutine.replay(model, guide_trace)(dataset, predict=time)
        probs = probs.squeeze().mean(0)  # average over Monte Carlo samples
        true_counts = dataset[&quot;counts&quot;][time]
        # Compute global and local KL divergence.
        global_mae = mae(true_counts, probs)
        i = dataset[&quot;locations&quot;].index(location)
        local_mae = mae(true_counts[i], probs[i])
    return {&quot;MAE (global)&quot;: global_mae, f&quot;MAE ({location})&quot;: local_mae}
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pprint(evaluate(model, guide))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;MAE (USA / Massachusetts)&#39;: 0.26023179292678833,
 &#39;MAE (global)&#39;: 0.22586050629615784}
</pre></div></div>
</div>
<p>We’ll also qualitatively evaluate using a volcano plot showing the effect size and statistical significance of each mutation’s coefficient, and labeling the mutation with the most significant positive effect. We expect: - most mutations have very little effect (they are near zero in log space, so their multiplicative effect is near 1x) - more mutations have positive effect than netagive effect - effect sizes are on the order of 1.1 or 0.9.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_volcano(guide, num_particles=100):
    if smoke_test:
        num_particles = 4
    with torch.no_grad(), poutine.mask(mask=False):  # makes computations cheaper
        with pyro.plate(&quot;particle&quot;, num_particles, dim=-3):  # vectorizes
            trace = poutine.trace(guide).get_trace(dataset)
            trace = poutine.trace(poutine.replay(model, trace)).get_trace(dataset, -1)
            coef = trace.nodes[&quot;coef&quot;][&quot;value&quot;].cpu()
    coef = coef.squeeze() * 0.01  # Scale factor as in the model.
    mean = coef.mean(0)
    std = coef.std(0)
    z_score = mean.abs() / std
    effect_size = mean.exp().numpy()
    plt.figure(figsize=(6, 3))
    plt.scatter(effect_size, z_score.numpy(), lw=0, s=5, alpha=0.5, color=&quot;darkred&quot;)
    plt.yscale(&quot;symlog&quot;)
    plt.ylim(0, None)
    plt.xlabel(&quot;$R_m/R_{wt}$&quot;)
    plt.ylabel(&quot;z-score&quot;)
    i = int((mean / std).max(0).indices)
    plt.text(effect_size[i], z_score[i] * 1.1, dataset[&quot;mutations&quot;][i], ha=&quot;center&quot;, fontsize=8)
    plt.title(f&quot;Volcano plot of {len(mean)} mutations&quot;)

plot_volcano(guide)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_20_0.png" src="_images/workflow_20_0.png" />
</div>
</div>
</section>
<section id="Create-an-initialization-heuristic">
<h3>Create an initialization heuristic<a class="headerlink" href="#Create-an-initialization-heuristic" title="Permalink to this heading">¶</a></h3>
<p>In high-dimensional models, convergence can be slow and NANs arise easily, even when sampling from <a class="reference external" href="http://www.stat.columbia.edu/~gelman/presentations/weakpriorstalk.pdf">weakly informative priors</a>. We recommend heuristically initializing a point estimate for each latent variable, aiming to initialize at something that is the right order of magnitude. Often you can initialize to a simple statistic of the data, e.g. a mean or standard deviation.</p>
<p>Pyro’s autoguides provide a number of <a href="#id1"><span class="problematic" id="id2">`initialization strategies &lt;&gt;`__</span></a> for initializing the location parameter of many variational families, specified as <code class="docutils literal notranslate"><span class="pre">init_loc_fn</span></code>. You can create a custom initializer by accepting a pyro sample site dict and generating a sample from <code class="docutils literal notranslate"><span class="pre">site[&quot;name&quot;]</span></code> and <code class="docutils literal notranslate"><span class="pre">site[&quot;fn&quot;]</span></code> using e.g. <code class="docutils literal notranslate"><span class="pre">site[&quot;fn&quot;].shape()</span></code>, <code class="docutils literal notranslate"><span class="pre">site[&quot;fn&quot;].support</span></code>, <code class="docutils literal notranslate"><span class="pre">site[&quot;fn&quot;].mean</span></code>, or sampling via <code class="docutils literal notranslate"><span class="pre">site[&quot;fn&quot;].sample()</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def init_loc_fn(site):
    shape = site[&quot;fn&quot;].shape()
    if site[&quot;name&quot;] == &quot;coef&quot;:
        return torch.randn(shape).sub_(0.5).mul(0.01)
    if site[&quot;name&quot;] == &quot;init&quot;:
        # Heuristically initialize based on data.
        return dataset[&quot;counts&quot;].mean(0).add(0.01).log()
    return init_to_feasible(site)  # fallback
</pre></div>
</div>
</div>
<p>As you evolve a model, you’ll add and remove and rename latent variables. We find it useful to require inits for all latent variables, add a message to remind yourself to udpate the <code class="docutils literal notranslate"><span class="pre">init_loc_fn</span></code> whenever the model changes.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def init_loc_fn(site):
    shape = site[&quot;fn&quot;].shape()
    if site[&quot;name&quot;].endswith(&quot;_scale&quot;):
        return torch.ones(shape)
    if site[&quot;name&quot;] == &quot;coef&quot;:
        return torch.randn(shape).sub_(0.5).mul(0.01)
    if site[&quot;name&quot;] == &quot;rate&quot;:
        return torch.zeros(shape)
    if site[&quot;name&quot;] == &quot;init&quot;:
        return dataset[&quot;counts&quot;].mean(0).add(0.01).log()
    raise NotImplementedError(f&quot;TODO initialize latent variable {site[&#39;name&#39;]}&quot;)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
guide = AutoNormal(model, init_loc_fn=init_loc_fn, init_scale=0.01)
fit_svi(model, guide, lr=0.02)
pprint(evaluate(model, guide))
plot_volcano(guide)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found 538452 latent variables and 1068600 learnable parameters
step    0 loss = 127.475
step  100 loss = 44.9544
step  200 loss = 31.4236
step  300 loss = 24.4205
step  400 loss = 20.6802
step  500 loss = 18.6063
step  600 loss = 17.2365
step  700 loss = 16.5067
step  800 loss = 16.001
step  900 loss = 15.5123
step 1000 loss = 18.8275
{&#39;MAE (USA / Massachusetts)&#39;: 0.29367634654045105,
 &#39;MAE (global)&#39;: 0.2283070981502533}
CPU times: user 3min 17s, sys: 2min 51s, total: 6min 9s
Wall time: 1min 58s
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_25_1.png" src="_images/workflow_25_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_25_2.png" src="_images/workflow_25_2.png" />
</div>
</div>
</section>
<section id="Reparametrize-the-model">
<h3>Reparametrize the model<a class="headerlink" href="#Reparametrize-the-model" title="Permalink to this heading">¶</a></h3>
<p>Reparametrizing a model preserves its distribution while changing its geometry. Reparametrizing is simply a change of coordinates. When reparametrizing we aim to warp a model’s geometry to remove correlations and to lift inconvenient topological manifolds into simpler higher dimensional flat Euclidean space.</p>
<p>Whereas many probabilistic programming languages require users to rewrite models to change coordinates, Pyro implements a library of about 15 different reparametrization effects including decentering (Gorinova et al. 2020), Haar wavelet transforms, and neural transport (Hoffman et al. 2019), as well as strategies to automatically apply effects and machinery to create custom reparametrization effects. Using these reparametrizers you can separate modeling from inference: first specify a model in a
form that is natural to domain experts, then in inference code, reparametrize the model to have geometry that is more amenable to variational inference.</p>
<p>In our SARS-CoV-2 model, the geometry might improve if we change</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gd">- rate = pyro.sample(&quot;rate&quot;, dist.Normal(rate_loc, rate_scale))</span><span class="w"></span>
<span class="gi">+ rate = pyro.sample(&quot;rate&quot;, dist.Normal(0, 1)) * rate_scale + rate_loc</span><span class="w"></span>
</pre></div>
</div>
<p>but that would make the model less interpretable. Instead we can reparametrize the model</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>reparam_model = poutine.reparam(model, config={&quot;rate&quot;: LocScaleReparam()})
</pre></div>
</div>
</div>
<p>or even automatically apply a set of recommended reparameterizers</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>reparam_model = AutoReparam()(model)
</pre></div>
</div>
</div>
<p>Let’s try reparametrizing both sites “rate” and “init”. Note we’ll create a fresh <code class="docutils literal notranslate"><span class="pre">reparam_model</span></code> each time we train a guide, since the parameters are stored in that <code class="docutils literal notranslate"><span class="pre">reparam_model</span></code> instance. Take care to use the <code class="docutils literal notranslate"><span class="pre">reparam_model</span></code> in downstream prediction tasks like running <code class="docutils literal notranslate"><span class="pre">evaluate(reparam_model,</span> <span class="pre">guide)</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
reparam_model = poutine.reparam(
    model, {&quot;rate&quot;: LocScaleReparam(), &quot;init&quot;: LocScaleReparam()}
)
guide = AutoNormal(reparam_model, init_loc_fn=init_loc_fn, init_scale=0.01)
fit_svi(reparam_model, guide, lr=0.05)
pprint(evaluate(reparam_model, guide))
plot_volcano(guide)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found 538452 latent variables and 1068602 learnable parameters
step    0 loss = 127.368
step  100 loss = 20.2831
step  200 loss = 11.0703
step  300 loss = 9.64594
step  400 loss = 9.52988
step  500 loss = 9.09012
step  600 loss = 9.25454
step  700 loss = 8.60661
step  800 loss = 8.9332
step  900 loss = 8.64206
step 1000 loss = 8.56663
{&#39;MAE (USA / Massachusetts)&#39;: 0.1336274892091751,
 &#39;MAE (global)&#39;: 0.1719919890165329}
CPU times: user 4min 21s, sys: 3min 9s, total: 7min 31s
Wall time: 2min 17s
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_32_1.png" src="_images/workflow_32_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_32_2.png" src="_images/workflow_32_2.png" />
</div>
</div>
</section>
<section id="Customize-the-variational-family">
<h3>Customize the variational family<a class="headerlink" href="#Customize-the-variational-family" title="Permalink to this heading">¶</a></h3>
<p>When creating a new model, we recommend starting with mean field variational inference using an <a href="#id1"><span class="problematic" id="id3">`AutoNormal &lt;&gt;`__</span></a> guide. This mean field guide is good at finding the neighborhood of your model’s mode, but naively it ignores correlations between latent variables. A first step in capturing correlations is to reparametrize the model as above: using a <code class="docutils literal notranslate"><span class="pre">LocScaleReparam</span></code> or <code class="docutils literal notranslate"><span class="pre">HaarReparam</span></code> (where appropriate) already allows the guide to capture some correlations among latent variables.</p>
<p>The next step towards modeling uncertainty is to customize the variational family by trying other autoguides, building on <a href="#id1"><span class="problematic" id="id4">`EasyGuide &lt;&gt;`__</span></a>, or creating a custom guide using Pyro primitives. We recommend increasing guide complexity gradually via these steps: 1. Start with an <a href="#id1"><span class="problematic" id="id5">`AutoNormal &lt;&gt;`__</span></a> guide. 2. Try <a href="#id1"><span class="problematic" id="id6">`AutoLowRankMultivariateNormal &lt;&gt;`__</span></a>, which can model the principle components of correlated uncertainty. (For models with only ~100 latent variables you might also try
<a href="#id1"><span class="problematic" id="id7">`AutoMultivariateNormal &lt;&gt;`__</span></a> or <a href="#id1"><span class="problematic" id="id8">`AutoGaussian &lt;&gt;`__</span></a>). 3. Try combining multiple guides using <a href="#id1"><span class="problematic" id="id9">`AutoGuideList &lt;&gt;`__</span></a>. For example if <a href="#id1"><span class="problematic" id="id10">`AutoLowRankMultivariateNormal &lt;&gt;`__</span></a> is too expensive for all the latent variables, you can use <a href="#id1"><span class="problematic" id="id11">`AutoGuideList &lt;&gt;`__</span></a> to combine an <a href="#id1"><span class="problematic" id="id12">`AutoLowRankMultivariateNormal &lt;&gt;`__</span></a> guide over a few top-level global latent variables, together with a cheaper <a href="#id1"><span class="problematic" id="id13">`AutoNormal &lt;&gt;`__</span></a> guide over more numerous local latent variables. 4. Try using <a href="#id1"><span class="problematic" id="id14">`AutoGuideList &lt;&gt;`__</span></a> to combine a autoguide
together with a custom guide function built using <code class="docutils literal notranslate"><span class="pre">pyro.sample</span></code>, <code class="docutils literal notranslate"><span class="pre">pyro.param</span></code>, and <code class="docutils literal notranslate"><span class="pre">pyro.plate</span></code>. Given a <code class="docutils literal notranslate"><span class="pre">partial_guide()</span></code> function that covers just a few latent variables, you can <code class="docutils literal notranslate"><span class="pre">AutoGuideList.append(partial_guide)</span></code> just as you append autoguides. 5. Consider customizing one of Pyro’s autoguides that leverage model structure, e.g. <a class="reference external" href="https://docs.pyro.ai/en/latest/infer.autoguide.html#autostructured">AutoStructured</a>,
<a class="reference external" href="https://docs.pyro.ai/en/latest/infer.autoguide.html#autonormalmessenger">AutoNormalMessenger</a>, <a class="reference external" href="https://docs.pyro.ai/en/latest/infer.autoguide.html#autohierarchicalnormalmessenger">AutoHierarchicalNormalMessenger</a> <a class="reference external" href="https://docs.pyro.ai/en/latest/infer.autoguide.html#autoregressivemessenger">AutoRegressiveMessenger</a>. 6. For models with local correlations, consider building on <a class="reference external" href="https://docs.pyro.ai/en/latest/contrib.easyguide.html">EasyGuide</a>, a framework for building guides over
groups of variables.</p>
<p>While a fully-custom guides built from <code class="docutils literal notranslate"><span class="pre">pyro.sample</span></code> primitives offer the most flexible variational family, they are also the most brittle guides because each code change to the model or reparametrizer requires changes in the guide. The author recommends avoiding completely low-level guides and instead using <code class="docutils literal notranslate"><span class="pre">AutoGuide</span></code> or <code class="docutils literal notranslate"><span class="pre">EasyGuide</span></code> for at least some parts of the model, thereby speeding up model iteration.</p>
<p>Let’s first try a simple <code class="docutils literal notranslate"><span class="pre">AutoLowRankMultivariateNormal</span></code> guide.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
reparam_model = poutine.reparam(
    model, {&quot;rate&quot;: LocScaleReparam(), &quot;init&quot;: LocScaleReparam()}
)
guide = AutoLowRankMultivariateNormal(
    reparam_model, init_loc_fn=init_loc_fn, init_scale=0.01, rank=100
)
fit_svi(reparam_model, guide, num_steps=10, log_every=1, plot=False)
# don&#39;t even bother to evaluate, since this is too slow.
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found 538452 latent variables and 54498602 learnable parameters
step    0 loss = 128.329
step    1 loss = 126.172
step    2 loss = 124.691
step    3 loss = 123.609
step    4 loss = 123.317
step    5 loss = 121.567
step    6 loss = 120.513
step    7 loss = 121.759
step    8 loss = 120.844
step    9 loss = 121.641
CPU times: user 45.9 s, sys: 38.2 s, total: 1min 24s
Wall time: 29 s
</pre></div></div>
</div>
<p>Yikes! This is quite slow and sometimes runs out of memory on GPU.</p>
<p>Let’s make this cheaper by using <code class="docutils literal notranslate"><span class="pre">AutoGuideList</span></code> to combine an <code class="docutils literal notranslate"><span class="pre">AutoLowRankMultivariateNormal</span></code> guide over the most important variables <code class="docutils literal notranslate"><span class="pre">rate_scale</span></code>, <code class="docutils literal notranslate"><span class="pre">init_scale</span></code>, and <code class="docutils literal notranslate"><span class="pre">coef</span></code>, together with a simple cheap <code class="docutils literal notranslate"><span class="pre">AutoNormal</span></code> guide on the rest of the model (the expensive <code class="docutils literal notranslate"><span class="pre">rate</span></code> and <code class="docutils literal notranslate"><span class="pre">init</span></code> variables). The typical pattern is to create two views of the model with <a class="reference external" href="https://docs.pyro.ai/en/stable/poutine.html#pyro.poutine.handlers.block">poutine.block</a>, one exposing the target variables and
the other hiding them.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
reparam_model = poutine.reparam(
    model, {&quot;rate&quot;: LocScaleReparam(), &quot;init&quot;: LocScaleReparam()}
)
guide = AutoGuideList(reparam_model)
mvn_vars = [&quot;coef&quot;, &quot;rate_scale&quot;, &quot;coef_scale&quot;]
guide.add(
    AutoLowRankMultivariateNormal(
        poutine.block(reparam_model, expose=mvn_vars),
        init_loc_fn=init_loc_fn,
        init_scale=0.01,
    )
)
guide.add(
    AutoNormal(
        poutine.block(reparam_model, hide=mvn_vars),
        init_loc_fn=init_loc_fn,
        init_scale=0.01,
    )
)
fit_svi(reparam_model, guide, lr=0.1)
pprint(evaluate(reparam_model, guide))
plot_volcano(guide)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found 538452 latent variables and 1202987 learnable parameters
step    0 loss = 832.956
step  100 loss = 11.9687
step  200 loss = 11.1152
step  300 loss = 9.60629
step  400 loss = 10.1724
step  500 loss = 9.18063
step  600 loss = 9.1669
step  700 loss = 9.06247
step  800 loss = 9.38853
step  900 loss = 9.12489
step 1000 loss = 8.93582
{&#39;MAE (USA / Massachusetts)&#39;: 0.09685955196619034,
 &#39;MAE (global)&#39;: 0.16698431968688965}
CPU times: user 4min 22s, sys: 3min 5s, total: 7min 28s
Wall time: 2min 15s
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_37_1.png" src="_images/workflow_37_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_37_2.png" src="_images/workflow_37_2.png" />
</div>
</div>
<p>Next let’s create a custom guide for part of the model, just the <code class="docutils literal notranslate"><span class="pre">rate</span></code> and <code class="docutils literal notranslate"><span class="pre">init</span></code> parts. Since we’ll want to use this with reparametrizers, we’ll make the guide use the auxiliary latent variables created by <code class="docutils literal notranslate"><span class="pre">poutine.reparam</span></code>, rather than the original <code class="docutils literal notranslate"><span class="pre">rate</span></code> and <code class="docutils literal notranslate"><span class="pre">init</span></code> variables. Let’s see what these variables are named:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for name, site in poutine.trace(reparam_model).get_trace(
    dataset
).iter_stochastic_nodes():
    print(name)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
rate_scale
init_scale
mutation
coef
strain
place
rate_decentered
init_decentered
</pre></div></div>
</div>
<p>It looks like these new auxiliary variables are called <code class="docutils literal notranslate"><span class="pre">rate_decentered</span></code> and <code class="docutils literal notranslate"><span class="pre">init_decentered</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def local_guide(dataset):
    # Create learnable parameters.
    T, P, S = dataset[&quot;counts&quot;].shape
    r_loc = pyro.param(&quot;rate_decentered_loc&quot;, lambda: torch.zeros(P, S))
    i_loc = pyro.param(&quot;init_decentered_loc&quot;, lambda: torch.zeros(P, S))
    skew = pyro.param(&quot;skew&quot;, lambda: torch.zeros(P, S))  # allows correlation
    r_scale = pyro.param(&quot;rate_decentered_scale&quot;, lambda: torch.ones(P, S),
                          constraint=constraints.softplus_positive)
    i_scale = pyro.param(&quot;init_decentered_scale&quot;, lambda: torch.ones(P, S),
                          constraint=constraints.softplus_positive)

    # Sample local variables inside plates.
    # Note plates are already created by the main guide, so we&#39;ll
    # use the existing plates rather than calling pyro.plate(...).
    with guide.plates[&quot;place&quot;], guide.plates[&quot;strain&quot;]:
        samples = {}
        samples[&quot;rate_decentered&quot;] = pyro.sample(
            &quot;rate_decentered&quot;, dist.Normal(r_loc, r_scale)
        )
        i_loc = i_loc + skew * samples[&quot;rate_decentered&quot;]
        samples[&quot;init_decentered&quot;] = pyro.sample(
            &quot;init_decentered&quot;, dist.Normal(i_loc, i_scale)
        )
    return samples
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
reparam_model = poutine.reparam(
    model, {&quot;rate&quot;: LocScaleReparam(), &quot;init&quot;: LocScaleReparam()}
)
guide = AutoGuideList(reparam_model)
local_vars = [&quot;rate_decentered&quot;, &quot;init_decentered&quot;]
guide.add(
    AutoLowRankMultivariateNormal(
        poutine.block(reparam_model, hide=local_vars),
        init_loc_fn=init_loc_fn,
        init_scale=0.01,
    )
)
guide.add(local_guide)
fit_svi(reparam_model, guide, lr=0.1)
pprint(evaluate(reparam_model, guide))
plot_volcano(guide)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Found 538452 latent variables and 1468870 learnable parameters
step    0 loss = 4804.42
step  100 loss = 31.7409
step  200 loss = 19.8206
step  300 loss = 15.2961
step  400 loss = 13.2222
step  500 loss = 12.1435
step  600 loss = 11.4291
step  700 loss = 10.9722
step  800 loss = 10.6209
step  900 loss = 10.3649
step 1000 loss = 10.1804
{&#39;MAE (USA / Massachusetts)&#39;: 0.1159871369600296,
 &#39;MAE (global)&#39;: 0.1876191794872284}
CPU times: user 4min 26s, sys: 3min 7s, total: 7min 33s
Wall time: 2min 18s
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_42_1.png" src="_images/workflow_42_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/workflow_42_2.png" src="_images/workflow_42_2.png" />
</div>
</div>
</section>
<section id="Conclusion">
<h3>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this heading">¶</a></h3>
<p>We’ve seen how to use initialization, reparameterization, autoguides, and custom guides in a Bayesian workflow. For more examples of these pieces of machinery, we recommend exploring the Pyro codebase, e.g. <a class="reference external" href="https://github.com/pyro-ppl/pyro/search?q=poutine.reparam&amp;type=code">search for “poutine.reparam”</a> or <a class="reference external" href="https://github.com/pyro-ppl/pyro/search?q=init_loc_fn&amp;type=code">“init_loc_fn”</a> in the Pyro codebase.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modules.html" class="btn btn-neutral float-left" title="Modules in Pyro" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="prior_predictive.html" class="btn btn-neutral float-right" title="Interactive posterior predictives checks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>