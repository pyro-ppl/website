<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Pyro Tutorials 1.8.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="bayesian_regression.html" />
    <link rel="prev" title="&lt;no title&gt;" href="svi_part_iii.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/svi_part_iv.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<dl>
<dt>{</dt><dd><dl>
<dt>“cells”: [</dt><dd><dl>
<dt>{</dt><dd><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“# SVI Part IV: Tips and Tricksn”,
“n”,
“The three SVI tutorials leading up to this one ([Part I](<a class="reference external" href="http://pyro.ai/examples/svi_part_i.html">http://pyro.ai/examples/svi_part_i.html</a>), [Part II](<a class="reference external" href="http://pyro.ai/examples/svi_part_ii.html">http://pyro.ai/examples/svi_part_ii.html</a>), &amp; [Part III](<a class="reference external" href="http://pyro.ai/examples/svi_part_iii.html">http://pyro.ai/examples/svi_part_iii.html</a>)) go throughn”,
“the various steps involved in using Pyro to do variationaln”,
“inference.n”,
“Along the way we defined models and guides (i.e. variational distributions),n”,
“setup variational objectives (in particular [ELBOs](<a class="reference external" href="https://docs.pyro.ai/en/dev/inference_algos.html?highlight=elbo#module-pyro.infer.elbo">https://docs.pyro.ai/en/dev/inference_algos.html?highlight=elbo#module-pyro.infer.elbo</a>)), n”,
“and constructed optimizers ([pyro.optim](<a class="reference external" href="http://docs.pyro.ai/en/dev/optimization.html">http://docs.pyro.ai/en/dev/optimization.html</a>)). n”,
“The effect of all this machinery is to cast Bayesian inference as a <em>stochastic optimization problem</em>. n”,
“n”,
“This is all very useful, but in order to arrive at our ultimate goal—learning model parameters, inferring approximate posteriors, making predictions with the posterior predictive distribution, etc.—we need to successfully solve this optimization problem. n”,
“Depending on the details of the particular problem—for example the dimensionality of the latent space, whether we have discrete latent variables, and so on—this can be easy or hard. n”,
“In this tutorial we cover a few tips and tricks we expect to be generally useful for users doing variational inference in Pyro. <em>ELBO not converging!? Running into NaNs!?</em> Look below for possible solutions!  n”,
“n”,
“#### Pyro Forumn”,
“n”,
“If you’re still having trouble with optimization after reading this tutorial, please don’t hesitate to ask a question on our [forum](<a class="reference external" href="https://forum.pyro.ai/">https://forum.pyro.ai/</a>)!”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 1. Start with a small learning raten”,
“n”,
“While large learning rates might be appropriate for some problems, it’s usually good practice to start with small learning rates like $10^{-3}$n”,
“or $10^{-4}$:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;optimizer</span> <span class="pre">=</span> <span class="pre">pyro.optim.Adam({\&quot;lr\&quot;:</span> <span class="pre">0.001})\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“This is because ELBO gradients are <em>stochastic</em>, and potentially high variance, so large learning rates can quickly lead to regions of model/guide parameter space that are numerically unstable or otherwise undesirable.n”,
“n”,
“You can try a larger learning rate once you have achieved stablen”,
“ELBO optimization using a smaller learning rate. n”,
“This is often a good idea because excessively small learning rates can lead to poor optimization. n”,
“In particular small learning rates can lead to getting stuck in poor local optima of the ELBO.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 2. Use Adam or ClippedAdam by defaultn”,
“n”,
“Use [Adam](<a class="reference external" href="http://docs.pyro.ai/en/stable/optimization.html?highlight=clippedadam#pyro.optim.pytorch_optimizers.Adam)n">http://docs.pyro.ai/en/stable/optimization.html?highlight=clippedadam#pyro.optim.pytorch_optimizers.Adam)n</a>”,
“or [ClippedAdam](<a class="reference external" href="http://docs.pyro.ai/en/stable/optimization.html?highlight=clippedadam#pyro.optim.optim.ClippedAdam">http://docs.pyro.ai/en/stable/optimization.html?highlight=clippedadam#pyro.optim.optim.ClippedAdam</a>) by default when doing Stochastic Variational Inference. Note that <cite>ClippedAdam</cite> is just a convenient extension of <cite>Adam</cite> that provides built-in support for learning rate decay and gradient clipping.n”,
“n”,
“The basic reason these optimization algorithms often do well in the context of variational inference is that the smoothing they provide via per-parameter momentum is often essential when the optimization problem is very stochastic. Note that in SVI stochasticity can come from sampling latent variables, from subsampling data, or from both. n”,
“n”,
“In addition to tuning the learning rate in some cases it may be necessary to also tune the pair of <cite>betas</cite> hyperparameters that controls the momentum used by <cite>Adam</cite>. In particular for very stochastic models it may make sense to use higher values of $\beta_1$:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;betas</span> <span class="pre">=</span> <span class="pre">(0.95,</span> <span class="pre">0.999)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“instead of n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;betas</span> <span class="pre">=</span> <span class="pre">(0.90,</span> <span class="pre">0.999)\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 3. Consider using a decaying learning raten”,
“n”,
“While a moderately large learning rate can be useful at the beginning of optimization when you’re far from the optimum and want to take large gradient steps, it’s often useful to have a smaller learning rate later on so that you don’t bounce around the optimum excessively without converging. n”,
“One way to do this is to use the learning rate schedulers [provided](<a class="reference external" href="http://docs.pyro.ai/en/stable/optimization.html?highlight=scheduler#pyro.optim.lr_scheduler.PyroLRScheduler">http://docs.pyro.ai/en/stable/optimization.html?highlight=scheduler#pyro.optim.lr_scheduler.PyroLRScheduler</a>) by Pyro. For example usage see the code snippet [here](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/a106882e8ffbfe6ac96f19aef9a218026482ed51/examples/scanvi/scanvi.py#L265">https://github.com/pyro-ppl/pyro/blob/a106882e8ffbfe6ac96f19aef9a218026482ed51/examples/scanvi/scanvi.py#L265</a>). n”,
“Another convenient way to do this is to use the [ClippedAdam](<a class="reference external" href="http://docs.pyro.ai/en/stable/optimization.html?highlight=clippedadam#pyro.optim.optim.ClippedAdam">http://docs.pyro.ai/en/stable/optimization.html?highlight=clippedadam#pyro.optim.optim.ClippedAdam</a>) optimizer that has built-in support for learning rate decay via the <cite>lrd</cite> argument:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;num_steps</span> <span class="pre">=</span> <span class="pre">1000\n&quot;,</span>
<span class="pre">&quot;initial_lr</span> <span class="pre">=</span> <span class="pre">0.001\n&quot;,</span>
<span class="pre">&quot;gamma</span> <span class="pre">=</span> <span class="pre">0.1</span>&#160; <span class="pre">#</span> <span class="pre">final</span> <span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">gamma</span> <span class="pre">*</span> <span class="pre">initial_lr\n&quot;,</span>
<span class="pre">&quot;lrd</span> <span class="pre">=</span> <span class="pre">gamma</span> <span class="pre">**</span> <span class="pre">(1</span> <span class="pre">/</span> <span class="pre">num_steps)\n&quot;,</span>
<span class="pre">&quot;optim</span> <span class="pre">=</span> <span class="pre">pyro.optim.ClippedAdam({'lr':</span> <span class="pre">initial_lr,</span> <span class="pre">'lrd':</span> <span class="pre">lrd})\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 4. Make sure your model and guide distributions have the same supportn”,
“n”,
“Suppose you have a distribution in your <cite>model</cite> with constrained support, e.g. a LogNormal distribution, which has support on the positive real axis:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">model():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.LogNormal(0.0,</span> <span class="pre">1.0))\n&quot;,</span>
<span class="pre">&quot;`</span></code> n”,
“Then you need to ensure that the accompanying <cite>sample</cite> site in the <cite>guide</cite> has the same support:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">good_guide():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">loc</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;loc\&quot;,</span> <span class="pre">torch.tensor(0.0))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.LogNormal(loc,</span> <span class="pre">1.0))\n&quot;,</span>
<span class="pre">&quot;`</span></code> n”,
“If you fail to do this and use for example the following inadmissable guide:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">bad_guide():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">loc</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;loc\&quot;,</span> <span class="pre">torch.tensor(0.0))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">Normal</span> <span class="pre">may</span> <span class="pre">sample</span> <span class="pre">x</span> <span class="pre">&lt;</span> <span class="pre">0\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Normal(loc,</span> <span class="pre">1.0))</span>&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“you will likely run into NaNs very quickly. n”,
“This is because the <cite>log_prob</cite> of a LogNormal distribution evaluated at a sample <cite>x</cite> that satisfies <cite>x&lt;0</cite> is undefined, and the <cite>bad_guide</cite> is likely to produce such samples.n”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 5. Constrain parameters that need to be constrainedn”,
“In a similar vein, you need to make sure that the parameters used to instantiate distributions are valid; otherwise you will quickly run into NaNs. n”,
“For example the <cite>scale</cite> parameter of a Normal distribution needs to be positive. Thus the following <cite>bad_guide</cite> is problematic:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">bad_guide():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">scale</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;scale\&quot;,</span> <span class="pre">torch.tensor(1.0))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Normal(0.0,</span> <span class="pre">scale))\n&quot;,</span>
<span class="pre">&quot;`</span></code> n”,
“while the following <cite>good_guide</cite> correctly uses a constraint to ensure positivity:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;from</span> <span class="pre">pyro.distributions</span> <span class="pre">import</span> <span class="pre">constraints\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">good_guide():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">scale</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;scale\&quot;,</span> <span class="pre">torch.tensor(0.05),</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">constraint=constraints.positive)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Normal(0.0,</span> <span class="pre">scale))\n&quot;,</span>
<span class="pre">&quot;`</span></code> “</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 6. If you are having trouble constructing a custom guide, use an AutoGuiden”,
“n”,
“In order for a model/guide pair to lead to stable optimization a number of conditions need to be satisfied, some of which we have covered above. n”,
“Sometimes it can be difficult to diagnose the reason for numerical instability or poor convergence. n”,
“Among other reasons this is because the fundamental issue could arise in a number of different places: in the model, in the guide, or in the choice of optimization algorithm or hyperparameters. n”,
“n”,
“Sometimes the problem is actually in your model even though you think it’s in the guide. n”,
“Conversely, sometimes the problem is in your guide even though you think it’s in the model or somewhere else. n”,
“For these reasons it can be helpful to reduce the number of moving parts while you try to identify the underyling issue.n”,
“One convenient way to do this is to replace your custom guide with a [pyro.infer.AutoGuide](<a class="reference external" href="http://docs.pyro.ai/en/stable/infer.autoguide.html#module-pyro.infer.autoguide">http://docs.pyro.ai/en/stable/infer.autoguide.html#module-pyro.infer.autoguide</a>). n”,
“n”,
“For example, if all the latent variables in your model are continuous, you can try a [pyro.infer.AutoNormal](<a class="reference external" href="http://docs.pyro.ai/en/stable/infer.autoguide.html#autonormal">http://docs.pyro.ai/en/stable/infer.autoguide.html#autonormal</a>) guide.n”,
“Alternatively, you can use MAP inference instead of full-blown variational inference. See the [MLE/MAP](<a class="reference external" href="http://pyro.ai/examples/mle_map.html">http://pyro.ai/examples/mle_map.html</a>) tutorial for further details. Once you have MAP inference working, there’s good reason to believe that your model is setup correctly (at least as far as basic numerical stability is concerned). n”,
“If you’re interested in obtaining approximate posterior distributions, you can now follow-up with full-blown SVI. Indeed a natural order of operations might use the following sequence of increasingly flexible autoguides:n”,
“n”,
“[AutoDelta](http://docs.pyro.ai/en/stable/infer.autoguide.html#autodelta)   →  [AutoNormal](http://docs.pyro.ai/en/stable/infer.autoguide.html#autonormal)  →  [AutoLowRankMultivariateNormal](http://docs.pyro.ai/en/stable/infer.autoguide.html#autolowrankmultivariatenormal)n”,
“n”,
“If you find that you want a more flexible guide or that you want to take more control over how exactly the guide is defined, at this juncture you can proceed to build a custom guide. n”,
“One way to go about doing this is to leverage [easy guides](http://pyro.ai/examples/easyguide.html), which strike a balance between the control of a fully custom guide and the automation of an autoguide.n”,
“n”,
“Also note that autoguides offer several initialization strategies and it may be necessary in some cases to experiment with these in order to get good optimization performance. n”,
“One way to control initialization behavior is using the <cite>init_loc_fn</cite>.n”,
“For example usage of <cite>init_loc_fn</cite>, including example usage for the easy guide API, see [here](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/a106882e8ffbfe6ac96f19aef9a218026482ed51/examples/sparse_gamma_def.py#L202">https://github.com/pyro-ppl/pyro/blob/a106882e8ffbfe6ac96f19aef9a218026482ed51/examples/sparse_gamma_def.py#L202</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 7. Parameter initialization matters: initialize guide distributions to have low variancen”,
“n”,
“Initialization in optimization problems can make all the difference between finding a good solution and failing catastrophically.n”,
“It is difficult to come up with a comprehensive set of good practices for initialization, as good initialization schemes are often very problem dependent. n”,
“In the context of Stochastic Variational Inference it is generally a good idea to initialize your guide distributions so that they have <strong>low variance</strong>. n”,
“This is because the ELBO gradients you use to optimize the ELBO are stochastic. n”,
“If the ELBO gradients you get at the beginning of ELBO optimization exhibit high variance, you may be led into numerically unstable or otherwise undesirable regions of parameter space. n”,
“One way to guard against this potential hazard is to pay close attention to parameters in your guide that control variance. n”,
“For example we would generally expect this to be a reasonably initialized guide:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;from</span> <span class="pre">pyro.distributions</span> <span class="pre">import</span> <span class="pre">constraints\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">good_guide():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">scale</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;scale\&quot;,</span> <span class="pre">torch.tensor(0.05),</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">constraint=constraints.positive)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Normal(0.0,</span> <span class="pre">scale))\n&quot;,</span>
<span class="pre">&quot;`</span></code> n”,
“while the following high-variance guide is very likely to lead to problems:n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">bad_guide():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">scale</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;scale\&quot;,</span> <span class="pre">torch.tensor(12345.6),</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">constraint=constraints.positive)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Normal(0.0,</span> <span class="pre">scale))\n&quot;,</span>
<span class="pre">&quot;`</span></code> n”,
“n”,
“Note that the initial variance of autoguides can be controlled with the <cite>init_scale</cite> argument, see e.g. [here](<a class="reference external" href="http://docs.pyro.ai/en/stable/infer.autoguide.html?highlight=init_scale#autonormal">http://docs.pyro.ai/en/stable/infer.autoguide.html?highlight=init_scale#autonormal</a>) for <cite>AutoNormal</cite>.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 8. Explore trade-offs controlled by <cite>num_particles</cite>, mini-batch size, etc.n”,
“n”,
“Optimization can be difficult if your ELBO exhibits large variance. n”,
“One way you can try to mitigate this issue is to increase the number of particles used to compute each stochastic ELBO estimate:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;elbo</span> <span class="pre">=</span> <span class="pre">pyro.infer.Trace_ELBO(num_particles=10,</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">vectorize_particles=True)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“(Note that to use <cite>vectorized_particles=True</cite> you need to ensure your model and guide are properly vectorized; see the [tensor shapes tutorial](<a class="reference external" href="http://pyro.ai/examples/tensor_shapes.html">http://pyro.ai/examples/tensor_shapes.html</a>) for best practices.)n”,
“This results in lower variance gradients at the cost of more compute. n”,
“If you are doing data subsampling, the mini-batch size offers a similar trade-off: larger mini-batch sizes reduce the variance at the cost of more compute. n”,
“Although what’s best is problem dependent, it’s usually worth taking more gradient steps with fewer particles than fewer gradient steps with more particles. n”,
“An important caveat to this is when you’re running on a GPU, in which case (at least for some models) the cost of increasing <cite>num_particles</cite> or your mini-batch size may be sublinear, in which case increasing <cite>num_particles</cite> is likely more attractive.n”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 9. Use <cite>TraceMeanField_ELBO</cite> if applicablen”,
“n”,
“The basic <cite>ELBO</cite> implementation in Pyro, [Trace_ELBO](<a class="reference external" href="http://docs.pyro.ai/en/stable/inference_algos.html?highlight=tracemeanfield#pyro.infer.trace_elbo.Trace_ELBO">http://docs.pyro.ai/en/stable/inference_algos.html?highlight=tracemeanfield#pyro.infer.trace_elbo.Trace_ELBO</a>), uses stochastic samples to estimate the KL divergence term. n”,
“When analytic KL diverences are available, you may be able to lower ELBO variance by using analytic KL divergences instead. This functionality is provided by [TraceMeanField_ELBO](<a class="reference external" href="http://docs.pyro.ai/en/stable/inference_algos.html?highlight=tracemeanfield#pyro.infer.trace_elbo.Trace_ELBO">http://docs.pyro.ai/en/stable/inference_algos.html?highlight=tracemeanfield#pyro.infer.trace_elbo.Trace_ELBO</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 10. Consider normalizing your ELBOn”,
“n”,
“By default Pyro computes a un-normalized ELBO, i.e. it computes the quantity that is a lower bound to the log evidence computed on the full set of data that is being conditioned on. n”,
“For large datasets this can be a number of large magnitude. n”,
“Since computers use finite precision (e.g. 32-bit floats) to do arithmetic, large numbers can be problematic for numerical stability, since they can lead to loss of precision, under/overflow, etc.n”,
“For this reason it can be helpful in many cases to normalize your ELBO so that it is roughly order one. n”,
“This can also be helpful for getting a rough feeling for how good your ELBO numbers are. n”,
“For example if we have $N$ datapoints of dimension $D$ (e.g. $N$ real-valued vectors of dimension $D$) then we generally expect a reasonably well optimized ELBO to be order $N \times D$. n”,
“Thus if we renormalize our ELBO by a factor of $N \times D$ we expect an ELBO of order one. n”,
“While this is just a rough rule-of-thumb, if we use this kind of normalization and obtain ELBO values like $-123.4$ or $1234.5$ then something is probably wrong: perhaps our model is terribly mis-specified; perhaps our initialization is catastrophically bad, etc. n”,
“For details on how you can scale your ELBO by a normalization constant see [this tutorial](<a class="reference external" href="http://pyro.ai/examples/custom_objectives.html#Example:-Scaling-the-Loss">http://pyro.ai/examples/custom_objectives.html#Example:-Scaling-the-Loss</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 11. Pay attention to scalesn”,
“n”,
“Scales of numbers matter. n”,
“They matter for at least two important reasons: n”,
“i) scales can make or break a particular initialization scheme; n”,
“ii) as discussed in the previous section, scales can have an impact on numerical precision and stability.n”,
“n”,
“To make this concrete suppose you are doing linear regression, i.e.n”,
“you’re learning a linear map of the form $Y = W &#64; X$. Often the data comes with particular units. n”,
“For example some of the components of the covariate $X$ may be in units of dollars (e.g. house prices), while others may be in units of density (e.g. residents per square mile). n”,
“Perhaps the the first covariate has typical values like $10^5$, while the second covariate has typical values like $10^2$. n”,
“You should always pay attention when you encounter numbers that range across many orders of magnitude. n”,
“In many cases it makes sense to normalize things so that they are order unity. n”,
“For example you might measure house prices in units of $100,000.n”,
“n”,
“These sorts of data transformations can have a number of benefits for downstream modeling and inference. n”,
“For example if you’ve normalized all of your covariates appropriately, it may be reasonable to set a simple n”,
“isotropic prior on your weightsn”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;pyro.sample(\&quot;W\&quot;,</span> <span class="pre">dist.Normal(torch.zeros(2),</span> <span class="pre">torch.ones(2)))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“instead of having to specify different prior covariances for different covariatesn”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;prior_scale</span> <span class="pre">=</span> <span class="pre">torch.tensor([1.0e-5,</span> <span class="pre">1.0e-2])\n&quot;,</span>
<span class="pre">&quot;pyro.sample(\&quot;W\&quot;,</span> <span class="pre">dist.Normal(torch.zeros(2),</span> <span class="pre">prior_scale))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“There are other benefits too. n”,
“It now becomes easier to initialize appropriate parameters for your guide. n”,
“It is also now much more likely that the default initializations used by a [pyro.infer.AutoGuide](<a class="reference external" href="http://docs.pyro.ai/en/stable/infer.autoguide.html#module-pyro.infer.autoguide">http://docs.pyro.ai/en/stable/infer.autoguide.html#module-pyro.infer.autoguide</a>) will work for your problem.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 12. Keep validation enabledn”,
“n”,
“By default Pyro enables validation logic that can be helpful in debugging models and guides. n”,
“For example, validation logic will inform you when distribution parameters become invalid.n”,
“Unless you have good reason to do otherwise, keep the validation logic enabled. n”,
“Once you’re satisfied with a model and inference procedure, you may wish to disable validation using [pyro.enable_validation](<a class="reference external" href="http://docs.pyro.ai/en/stable/primitives.html?highlight=enable_validation#pyro.primitives.enable_validation).n">http://docs.pyro.ai/en/stable/primitives.html?highlight=enable_validation#pyro.primitives.enable_validation).n</a>”,
“n”,
“Similarly in the context of <cite>ELBOs</cite> it is a good idea to set n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;strict_enumeration_warning=True\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“when you are enumerating discrete latent variables.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 13. Tensor shape errorsn”,
“n”,
“If you’re running into tensor shape errors please make sure you have carefully read the [corresponding tutorial](<a class="reference external" href="http://pyro.ai/examples/tensor_shapes.html">http://pyro.ai/examples/tensor_shapes.html</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 14. Enumerate discrete latent variables if possiblen”,
“n”,
“If your model contains discrete latent variables it may make sense to enumerate them out exactly, since this can significantly reduce ELBO variance. n”,
“For more discussion see the [corresponding tutorial](<a class="reference external" href="http://pyro.ai/examples/enumeration.html">http://pyro.ai/examples/enumeration.html</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 15. Some complex models can benefit from KL annealingn”,
“n”,
“The particular form of the ELBO encodes a trade-off between model fit via the expected log likelihood term and a prior regularization term via the KL divergence. n”,
“In some cases the KL divergence can act as a barrier that makes it difficult to find good optima. n”,
“In these cases it can help to anneal the relevant strength of the KL divergence term during optimization. For further discussion see the [deep markov model tutorial](<a class="reference external" href="http://pyro.ai/examples/dmm.html#The-Black-Magic-of-Optimization).n">http://pyro.ai/examples/dmm.html#The-Black-Magic-of-Optimization).n</a>”,
“n”,
“n”,
“n”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### 16. Consider clipping gradients or constraining parameters defensivelyn”,
“n”,
“Certain parameters in your model or guide may control distribution parameters that can be sensitive to numerical issues. n”,
“For example, the <cite>concentration</cite> and <cite>rate</cite> parameters that defines a [Gamma](<a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#gamma">http://docs.pyro.ai/en/stable/distributions.html#gamma</a>) distribution may exhibit such sensitivity. n”,
“In these cases it may make sense to clip gradients or constrain parameters defensively. n”,
“See [this code snippet](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/dev/examples/sparse_gamma_def.py#L135">https://github.com/pyro-ppl/pyro/blob/dev/examples/sparse_gamma_def.py#L135</a>) for an example of gradient clipping. n”,
“For a simple example of &quot;defensive&quot; parameter constraints consider the <cite>concentration</cite> parameter of a <cite>Gamma</cite> distribution. n”,
“This parameter must be positive: <cite>concentration</cite> &gt; 0.n”,
“If we want to ensure that <cite>concentration</cite> stays away from zero we can use a <cite>param</cite> statement with an appropriate constraint:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;from</span> <span class="pre">pyro.distributions</span> <span class="pre">import</span> <span class="pre">constraints\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;concentration</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;concentration\&quot;,</span> <span class="pre">torch.tensor(0.5),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">constraints.greater_than(0.001))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“These kinds of tricks can help ensure that your models and guides stay away from numerically dangerous parts of parameter space.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>],
“metadata”: {</p>
<blockquote>
<div><p>“celltoolbar”: “Raw Cell Format”,
“kernelspec”: {</p>
<blockquote>
<div><p>“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”</p>
</div></blockquote>
<p>},
“language_info”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“codemirror_mode”: {</dt><dd><p>“name”: “ipython”,
“version”: 3</p>
</dd>
</dl>
<p>},
“file_extension”: “.py”,
“mimetype”: “text/x-python”,
“name”: “python”,
“nbconvert_exporter”: “python”,
“pygments_lexer”: “ipython3”,
“version”: “3.8.2”</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>},
“nbformat”: 4,
“nbformat_minor”: 2</p>
</dd>
</dl>
<p>}</p>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="svi_part_iii.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bayesian_regression.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>