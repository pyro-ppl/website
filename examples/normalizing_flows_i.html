

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Normalizing Flows - Introduction (Part 1) &mdash; Pyro Tutorials 1.6.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Deep Markov Model" href="dmm.html" />
    <link rel="prev" title="Conditional Variational Auto-encoder" href="cvae.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.6.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introductory Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">An Introduction to Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">An Introduction to Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
</ul>
<p class="caption"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">Bayesian Regression - Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">Bayesian Regression - Inference Algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Using the PyTorch JIT Compiler with Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption"><span class="caption-text">Deep Generative Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="cvae.html">Conditional Variational Auto-encoder</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Normalizing Flows - Introduction (Part 1)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Univariate-Distributions">Univariate Distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Background">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Fixed-Univariate-Transforms-in-Pyro">Fixed Univariate Transforms in Pyro</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Learnable-Univariate-Distributions-in-Pyro">Learnable Univariate Distributions in Pyro</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Multivariate-Distributions">Multivariate Distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Multivariate-Transforms-in-Pyro">Multivariate Transforms in Pyro</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Conditional-versus-Joint-Distributions">Conditional versus Joint Distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Conditional-Transforms-in-Pyro">Conditional Transforms in Pyro</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Conclusions">Conclusions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="scanvi.html">Example: Single Cell RNA Sequencing Analysis with VAEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="prodlda.html">Probabilistic Topic Modeling</a></li>
</ul>
<p class="caption"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dirichlet_process_mixture.html">Dirichlet Process Mixture Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mle_map.html">MLE and MAP Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">Custom SVI Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="boosting_bbvi.html">Boosting Black Box Variational Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecasting_i.html">Forecasting I: univariate, heavy tailed</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_ii.html">Forecasting II: state space models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_iii.html">Forecasting III: hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_dlm.html">Forecasting with Dynamic Linear Model (DLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="stable.html">Levy Stable models of Stochastic Volatility</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_intro.html">Epidemiological models: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption"><span class="caption-text">Application: Experimental Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="working_memory.html">Designing Adaptive Experiments to Study Working Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="elections.html">Predicting the outcome of a US presidential election using Bayesian optimal experimental design</a></li>
</ul>
<p class="caption"><span class="caption-text">Application: Object Tracking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
</ul>
<p class="caption"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
</ul>
<p class="caption"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: A Guide to Programming with Effect Handlers in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib_funsor_intro_i.html"><code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code>, a new backend for Pyro - New primitives (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib_funsor_intro_ii.html"><code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code>, a new backend for Pyro - Building inference algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Normalizing Flows - Introduction (Part 1)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/normalizing_flows_i.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Normalizing-Flows---Introduction-(Part-1)">
<h1>Normalizing Flows - Introduction (Part 1)<a class="headerlink" href="#Normalizing-Flows---Introduction-(Part-1)" title="Permalink to this headline">¶</a></h1>
<p>This tutorial introduces Pyro’s normalizing flow library. It is independent of much of Pyro, but users may want to read about distribution shapes in the <a class="reference external" href="http://pyro.ai/examples/tensor_shapes.html">Tensor Shapes Tutorial</a>.</p>
<div class="section" id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p>In standard probabilistic modeling practice, we represent our beliefs over unknown continuous quantities with simple parametric distributions like the normal, exponential, and Laplacian distributions. However, using such simple forms, which are commonly symmetric and unimodal (or have a fixed number of modes when we take a mixture of them), restricts the performance and flexibility of our methods. For instance, standard variational inference in the Variational Autoencoder uses independent
univariate normal distributions to represent the variational family. The true posterior is neither independent nor normally distributed, which results in suboptimal inference and simplifies the model that is learnt. In other scenarios, we are likewise restricted by not being able to model multimodal distributions and heavy or light tails.</p>
<p>Normalizing Flows [1-4] are a family of methods for constructing flexible learnable probability distributions, often with neural networks, which allow us to surpass the limitations of simple parametric forms. Pyro contains state-of-the-art normalizing flow implementations, and this tutorial explains how you can use this library for learning complex models and performing flexible variational inference. We introduce the main idea of Normalizing Flows (NFs) and demonstrate learning simple
univariate distributions with element-wise, multivariate, and conditional flows.</p>
</div>
<div class="section" id="Univariate-Distributions">
<h2>Univariate Distributions<a class="headerlink" href="#Univariate-Distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Background">
<h3>Background<a class="headerlink" href="#Background" title="Permalink to this headline">¶</a></h3>
<p>Normalizing Flows are a family of methods for constructing flexible distributions. Let’s first restrict our attention to representing univariate distributions. The basic idea is that a simple source of noise, for example a variable with a standard normal distribution, <span class="math notranslate nohighlight">\(X\sim\mathcal{N}(0,1)\)</span>, is passed through a bijective (i.e. invertible) function, <span class="math notranslate nohighlight">\(g(\cdot)\)</span> to produce a more complex transformed variable <span class="math notranslate nohighlight">\(Y=g(X)\)</span>.</p>
<p>For a given random variable, we typically want to perform two operations: sampling and scoring. Sampling <span class="math notranslate nohighlight">\(Y\)</span> is trivial. First, we sample <span class="math notranslate nohighlight">\(X=x\)</span>, then calculate <span class="math notranslate nohighlight">\(y=g(x)\)</span>. Scoring <span class="math notranslate nohighlight">\(Y\)</span>, or rather, evaluating the log-density <span class="math notranslate nohighlight">\(\log(p_Y(y))\)</span>, is more involved. How does the density of <span class="math notranslate nohighlight">\(Y\)</span> relate to the density of <span class="math notranslate nohighlight">\(X\)</span>? We can use the substitution rule of integral calculus to answer this. Suppose we want to evaluate the expectation of some function of <span class="math notranslate nohighlight">\(X\)</span>.
Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \mathbb{E}_{p_X(\cdot)}\left[f(X)\right] &amp;= \int_{\text{supp}(X)}f(x)p_X(x)dx\\
     &amp;= \int_{\text{supp}(Y)}f(g^{-1}(y))p_X(g^{-1}(y))\left|\frac{dx}{dy}\right|dy\\
     &amp;= \mathbb{E}_{p_Y(\cdot)}\left[f(g^{-1}(Y))\right],
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{supp}(X)\)</span> denotes the support of <span class="math notranslate nohighlight">\(X\)</span>, which in this case is <span class="math notranslate nohighlight">\((-\infty,\infty)\)</span>. Crucially, we used the fact that <span class="math notranslate nohighlight">\(g\)</span> is bijective to apply the substitution rule in going from the first to the second line. Equating the last two lines we get,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \log(p_Y(y)) &amp;= \log(p_X(g^{-1}(y)))+\log\left(\left|\frac{dx}{dy}\right|\right)\\
     &amp;= \log(p_X(g^{-1}(y)))-\log\left(\left|\frac{dy}{dx}\right|\right).
\end{align}\end{split}\]</div>
<p>Inituitively, this equation says that the density of <span class="math notranslate nohighlight">\(Y\)</span> is equal to the density at the corresponding point in <span class="math notranslate nohighlight">\(X\)</span> plus a term that corrects for the warp in volume around an infinitesimally small length around <span class="math notranslate nohighlight">\(Y\)</span> caused by the transformation.</p>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is cleverly constructed (and we will see several examples shortly), we can produce distributions that are more complex than standard normal noise and yet have easy sampling and computationally tractable scoring. Moreover, we can compose such bijective transformations to produce even more complex distributions. By an inductive argument, if we have <span class="math notranslate nohighlight">\(L\)</span> transforms <span class="math notranslate nohighlight">\(g_{(0)}, g_{(1)},\ldots,g_{(L-1)}\)</span>, then the log-density of the transformed variable
<span class="math notranslate nohighlight">\(Y=(g_{(0)}\circ g_{(1)}\circ\cdots\circ g_{(L-1)})(X)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
     \log(p_Y(y)) &amp;= \log\left(p_X\left(\left(g_{(L-1)}^{-1}\circ\cdots\circ g_{(0)}^{-1}\right)\left(y\right)\right)\right)+\sum^{L-1}_{l=0}\log\left(\left|\frac{dg^{-1}_{(l)}(y_{(l)})}{dy'}\right|\right),
     %\left( g^{(l)}(y^{(l)})
     %\right).
\end{align}\]</div>
<p>where we’ve defined <span class="math notranslate nohighlight">\(y_{(0)}=x\)</span>, <span class="math notranslate nohighlight">\(y_{(L-1)}=y\)</span> for convenience of notation.</p>
<p>In a latter section, we will see how to generalize this method to multivariate <span class="math notranslate nohighlight">\(X\)</span>. The field of Normalizing Flows aims to construct such <span class="math notranslate nohighlight">\(g\)</span> for multivariate <span class="math notranslate nohighlight">\(X\)</span> to transform simple i.i.d. standard normal noise into complex, learnable, high-dimensional distributions. The methods have been applied to such diverse applications as image modeling, text-to-speech, unsupervised language induction, data compression, and modeling molecular structures. As probability distributions are
the most fundamental component of probabilistic modeling we will likely see many more exciting state-of-the-art applications in the near future.</p>
</div>
<div class="section" id="Fixed-Univariate-Transforms-in-Pyro">
<h3>Fixed Univariate Transforms in Pyro<a class="headerlink" href="#Fixed-Univariate-Transforms-in-Pyro" title="Permalink to this headline">¶</a></h3>
<p>PyTorch contains classes for representing <em>fixed</em> univariate bijective transformations, and sampling/scoring from transformed distributions derived from these. Pyro extends this with a comprehensive library of <em>learnable</em> univariate and multivariate transformations using the latest developments in the field. As Pyro imports all of PyTorch’s distributions and transformations, we will work solely with Pyro. We also note that the NF components in Pyro can be used independently of the probabilistic
programming functionality of Pyro, which is what we will be doing in the first two tutorials.</p>
<p>Let us begin by showing how to represent and manipulate a simple transformed distribution,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 X &amp;\sim \mathcal{N}(0,1)\\
 Y &amp;= \text{exp}(X).
\end{align}\end{split}\]</div>
<p>You may have recognized that this is by definition, <span class="math notranslate nohighlight">\(Y\sim\text{LogNormal}(0,1)\)</span>.</p>
<p>We begin by importing the relevant libraries:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">import</span> <span class="nn">pyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">pyro.distributions.transforms</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">smoke_test</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;CI&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A variety of bijective transformations live in the <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#transforms">pyro.distributions.transforms</a> module, and the classes to define transformed distributions live in <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html">pyro.distributions</a>. We first create the base distribution of <span class="math notranslate nohighlight">\(X\)</span> and the class encapsulating the transform <span class="math notranslate nohighlight">\(\text{exp}(\cdot)\)</span>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dist_x</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">exp_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ExpTransform</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The class <a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transforms.ExpTransform">ExpTransform</a> derives from <a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transforms.Transform">Transform</a> and defines the forward, inverse, and log-absolute-derivative operations for this transform,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     g(x) &amp;= \text{exp(x)}\\
     g^{-1}(y) &amp;= \log(y)\\
     \log\left(\left|\frac{dg}{dx}\right|\right) &amp;= y.
\end{align}\end{split}\]</div>
<p>In general, a transform class defines these three operations, from which it is sufficient to perform sampling and scoring.</p>
<p>The class <a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution">TransformedDistribution</a> takes a base distribution of simple noise and a list of transforms, and encapsulates the distribution formed by applying these transformations in sequence. We use it as:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dist_y</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">dist_x</span><span class="p">,</span> <span class="p">[</span><span class="n">exp_transform</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Now, plotting samples from both to verify that we that have produced the log-normal distribution:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">dist_x</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1000</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Standard Normal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">dist_y</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1000</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Standard Log-Normal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_9_0.png" src="_images/normalizing_flows_i_9_0.png" />
</div>
</div>
<p>Our example uses a single transform. However, we can compose transforms to produce more expressive distributions. For instance, if we apply an affine transformation we can produce the general log-normal distribution,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 X &amp;\sim \mathcal{N}(0,1)\\
 Y &amp;= \text{exp}(\mu+\sigma X).
\end{align}\end{split}\]</div>
<p>or rather, <span class="math notranslate nohighlight">\(Y\sim\text{LogNormal}(\mu,\sigma^2)\)</span>. In Pyro this is accomplished, e.g. for <span class="math notranslate nohighlight">\(\mu=3, \sigma=0.5\)</span>, as follows:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dist_x</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">affine_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">AffineTransform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">exp_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ExpTransform</span><span class="p">()</span>
<span class="n">dist_y</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">dist_x</span><span class="p">,</span> <span class="p">[</span><span class="n">affine_transform</span><span class="p">,</span> <span class="n">exp_transform</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">dist_x</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1000</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Standard Normal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">dist_y</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1000</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-Normal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_11_0.png" src="_images/normalizing_flows_i_11_0.png" />
</div>
</div>
<p>For the forward operation, transformations are applied in the order of the list that is the second argument to <a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transformed_distribution.TransformedDistribution">TransformedDistribution</a>. In this case, first <a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transforms.AffineTransform">AffineTransform</a> is applied to the base distribution and then
<a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transforms.ExpTransform">ExpTransform</a>.</p>
</div>
<div class="section" id="Learnable-Univariate-Distributions-in-Pyro">
<h3>Learnable Univariate Distributions in Pyro<a class="headerlink" href="#Learnable-Univariate-Distributions-in-Pyro" title="Permalink to this headline">¶</a></h3>
<p>Having introduced the interface for invertible transforms and transformed distributions, we now show how to represent <em>learnable</em> transforms and use them for density estimation. Our dataset in this section and the next will comprise samples along two concentric circles. Examining the joint and marginal distributions:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Samples from $p(x_1,x_2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_1)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_14_0.png" src="_images/normalizing_flows_i_14_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_14_1.png" src="_images/normalizing_flows_i_14_1.png" />
</div>
</div>
<p>Standard transforms derive from the <a class="reference external" href="https://pytorch.org/docs/master/distributions.html#torch.distributions.transforms.ExpTransform">Transform</a> class and are not designed to contain learnable parameters. Learnable transforms, on the other hand, derive from <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.TransformModule">TransformModule</a>, which is a <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module">torch.nn.Module</a> and registers
parameters with the object.</p>
<p>We will learn the marginals of the above distribution using such a transform, <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.Spline">Spline</a> [5,6], defined on a two-dimensional input:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">base_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">spline_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Spline</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">count_bins</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">flow_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">spline_transform</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>This transform passes each dimension of its input through a <em>separate</em> monotonically increasing function known as a spline. From a high-level, a spline is a complex parametrizable curve for which we can define specific points known as knots that it passes through and the derivatives at the knots. The knots and their derivatives are parameters that can be learnt, e.g., through stochastic gradient descent on a maximum likelihood objective, as we now demonstrate:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">smoke_test</span> <span class="k">else</span> <span class="mi">1001</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">spline_transform</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">flow_dist</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;step: </span><span class="si">{}</span><span class="s1">, loss: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
step: 0, loss: 2.682476758956909
step: 200, loss: 1.278384804725647
step: 400, loss: 1.2647961378097534
step: 600, loss: 1.2601449489593506
step: 800, loss: 1.2561875581741333
step: 1000, loss: 1.2545257806777954
CPU times: user 4.92 s, sys: 69.3 ms, total: 4.99 s
Wall time: 5.01 s
</pre></div></div>
</div>
<p>Note that we call <code class="docutils literal notranslate"><span class="pre">flow_dist.clear_cache()</span></code> after each optimization step to clear the transform’s forward-inverse cache. This is required because <code class="docutils literal notranslate"><span class="pre">flow_dist</span></code>’s <code class="docutils literal notranslate"><span class="pre">spline_transform</span></code> is a stateful <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.TransformModule">TransformModule</a> rather than a purely stateless <a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.transforms.Transform">Transform</a> object. Purely functional Pyro code typically creates
<code class="docutils literal notranslate"><span class="pre">Transform</span></code> objects each model execution, then discards them after <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, effectively clearing the transform caches. By contrast in this tutorial we create stateful module objects and need to manually clear their cache after update.</p>
<p>Plotting samples drawn from the transformed distribution after learning:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X_flow</span> <span class="o">=</span> <span class="n">flow_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1000</span><span class="p">,]))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Joint Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_flow</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_1)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_20_0.png" src="_images/normalizing_flows_i_20_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_20_1.png" src="_images/normalizing_flows_i_20_1.png" />
</div>
</div>
<p>As we can see, we have learnt close approximations to the marginal distributions, <span class="math notranslate nohighlight">\(p(x_1),p(x_2)\)</span>. It would have been challenging to fit the irregularly shaped marginals with standard methods, e.g., a mixture of normal distributions. As expected, since there is a dependency between the two dimensions, we do not learn a good representation of the joint, <span class="math notranslate nohighlight">\(p(x_1,x_2)\)</span>. In the next section, we explain how to learn multivariate distributions whose dimensions are not independent.</p>
</div>
</div>
<div class="section" id="Multivariate-Distributions">
<h2>Multivariate Distributions<a class="headerlink" href="#Multivariate-Distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Background<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The fundamental idea of normalizing flows also applies to multivariate random variables, and this is where its value is clearly seen - <em>representing complex high-dimensional distributions</em>. In this case, a simple multivariate source of noise, for example a standard i.i.d. normal distribution, <span class="math notranslate nohighlight">\(X\sim\mathcal{N}(\mathbf{0},I_{D\times D})\)</span>, is passed through a vector-valued bijection, <span class="math notranslate nohighlight">\(g:\mathbb{R}^D\rightarrow\mathbb{R}^D\)</span>, to produce the more complex transformed variable
<span class="math notranslate nohighlight">\(Y=g(X)\)</span>.</p>
<p>Sampling <span class="math notranslate nohighlight">\(Y\)</span> is again trivial and involves evaluation of the forward pass of <span class="math notranslate nohighlight">\(g\)</span>. We can score <span class="math notranslate nohighlight">\(Y\)</span> using the multivariate substitution rule of integral calculus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \mathbb{E}_{p_X(\cdot)}\left[f(X)\right] &amp;= \int_{\text{supp}(X)}f(\mathbf{x})p_X(\mathbf{x})d\mathbf{x}\\
     &amp;= \int_{\text{supp}(Y)}f(g^{-1}(\mathbf{y}))p_X(g^{-1}(\mathbf{y}))\det\left|\frac{d\mathbf{x}}{d\mathbf{y}}\right|d\mathbf{y}\\
     &amp;= \mathbb{E}_{p_Y(\cdot)}\left[f(g^{-1}(Y))\right],
 \end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(d\mathbf{x}/d\mathbf{y}\)</span> denotes the Jacobian matrix of <span class="math notranslate nohighlight">\(g^{-1}(\mathbf{y})\)</span>. Equating the last two lines we get,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     \log(p_Y(y)) &amp;= \log(p_X(g^{-1}(y)))+\log\left(\det\left|\frac{d\mathbf{x}}{d\mathbf{y}}\right|\right)\\
     &amp;= \log(p_X(g^{-1}(y)))-\log\left(\det\left|\frac{d\mathbf{y}}{d\mathbf{x}}\right|\right).
\end{align}\end{split}\]</div>
<p>Inituitively, this equation says that the density of <span class="math notranslate nohighlight">\(Y\)</span> is equal to the density at the corresponding point in <span class="math notranslate nohighlight">\(X\)</span> plus a term that corrects for the warp in volume around an infinitesimally small volume around <span class="math notranslate nohighlight">\(Y\)</span> caused by the transformation. For instance, in <span class="math notranslate nohighlight">\(2\)</span>-dimensions, the geometric interpretation of the absolute value of the determinant of a Jacobian is that it represents the area of a parallelogram with edges defined by the columns of the Jacobian. In
<span class="math notranslate nohighlight">\(n\)</span>-dimensions, the geometric interpretation of the absolute value of the determinant Jacobian is that is represents the hyper-volume of a parallelepiped with <span class="math notranslate nohighlight">\(n\)</span> edges defined by the columns of the Jacobian (see a calculus reference such as [7] for more details).</p>
<p>Similar to the univariate case, we can compose such bijective transformations to produce even more complex distributions. By an inductive argument, if we have <span class="math notranslate nohighlight">\(L\)</span> transforms <span class="math notranslate nohighlight">\(g_{(0)}, g_{(1)},\ldots,g_{(L-1)}\)</span>, then the log-density of the transformed variable <span class="math notranslate nohighlight">\(Y=(g_{(0)}\circ g_{(1)}\circ\cdots\circ g_{(L-1)})(X)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{align}
     \log(p_Y(y)) &amp;= \log\left(p_X\left(\left(g_{(L-1)}^{-1}\circ\cdots\circ g_{(0)}^{-1}\right)\left(y\right)\right)\right)+\sum^{L-1}_{l=0}\log\left(\left|\frac{dg^{-1}_{(l)}(y_{(l)})}{dy'}\right|\right),
     %\left( g^{(l)}(y^{(l)})
     %\right).
\end{align}\]</div>
<p>where we’ve defined <span class="math notranslate nohighlight">\(y_{(0)}=x\)</span>, <span class="math notranslate nohighlight">\(y_{(L-1)}=y\)</span> for convenience of notation.</p>
<p>The main challenge is in designing parametrizable multivariate bijections that have closed form expressions for both <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(g^{-1}\)</span>, a tractable Jacobian whose calculation scales with <span class="math notranslate nohighlight">\(O(D)\)</span> rather than <span class="math notranslate nohighlight">\(O(D^3)\)</span>, and can express a flexible class of functions.</p>
</div>
<div class="section" id="Multivariate-Transforms-in-Pyro">
<h3>Multivariate Transforms in Pyro<a class="headerlink" href="#Multivariate-Transforms-in-Pyro" title="Permalink to this headline">¶</a></h3>
<p>Up to this point we have used element-wise transforms in Pyro. These are indicated by having the property <code class="docutils literal notranslate"><span class="pre">transform.event_dim</span> <span class="pre">==</span> <span class="pre">0</span></code> set on the transform object. Such element-wise transforms can only be used to represent univariate distributions and multivariate distributions whose dimensions are independent (known in variational inference as the mean-field approximation).</p>
<p>The power of Normalizing Flow, however, is most apparent in their ability to model complex high-dimensional distributions with neural networks and Pyro contains several such flows for accomplishing this. Transforms that operate on vectors have the property <code class="docutils literal notranslate"><span class="pre">transform.event_dim</span> <span class="pre">==</span> <span class="pre">1</span></code>, transforms on matrices with <code class="docutils literal notranslate"><span class="pre">transform.event_dim</span> <span class="pre">==</span> <span class="pre">2</span></code>, and so on. In general, the <code class="docutils literal notranslate"><span class="pre">event_dim</span></code> property of a transform indicates how many dependent dimensions there are in the output of a transform.</p>
<p>In this section, we show how to use <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.SplineCoupling">SplineCoupling</a> to learn the bivariate toy distribution from our running example. A coupling transform [8, 9] divides the input variable into two parts and applies an element-wise bijection to the section half whose parameters are a function of the first. Optionally, an element-wise bijection is also applied to the first half. Dividing the inputs at <span class="math notranslate nohighlight">\(d\)</span>,
the transform is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \mathbf{y}_{1:d} &amp;= g_\theta(\mathbf{x}_{1:d})\\
 \mathbf{y}_{(d+1):D} &amp;= h_\phi(\mathbf{x}_{(d+1):D};\mathbf{x}_{1:d}),
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_{1:d}\)</span> represents the first <span class="math notranslate nohighlight">\(d\)</span> elements of the inputs, <span class="math notranslate nohighlight">\(g_\theta\)</span> is either the identity function or an elementwise bijection parameters <span class="math notranslate nohighlight">\(\theta\)</span>, and <span class="math notranslate nohighlight">\(h_\phi\)</span> is an element-wise bijection whose parameters are a function of <span class="math notranslate nohighlight">\(\mathbf{x}_{1:d}\)</span>.</p>
<p>This type of transform is easily invertible. We invert the first half, <span class="math notranslate nohighlight">\(\mathbf{y}_{1:d}\)</span>, then use the resulting <span class="math notranslate nohighlight">\(\mathbf{x}_{1:d}\)</span> to evaluate <span class="math notranslate nohighlight">\(\phi\)</span> and invert the second half,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 \mathbf{x}_{1:d} &amp;= g_\theta^{-1}(\mathbf{y}_{1:d})\\
 \mathbf{x}_{(d+1):D} &amp;= h_\phi^{-1}(\mathbf{y}_{(d+1):D};\mathbf{x}_{1:d}).
\end{align}\end{split}\]</div>
<p>Difference choices for <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span> form different types of coupling transforms. When both are monotonic rational splines, the transform is the spline coupling layer of Neural Spline Flow [5,6], which is represented in Pyro by the <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.SplineCoupling">SplineCoupling</a> class. As shown in the references, when we combine a sequence of coupling layers sandwiched between random permutations so we introduce
dependencies between all dimensions, we can model complex multivariate distributions.</p>
<p>Most of the learnable transforms in Pyro have a corresponding helper function that takes care of constructing a neural network for the transform with the correct output shape. This neural network outputs the parameters of the transform and is known as a <a class="reference external" href="https://arxiv.org/abs/1609.09106">hypernetwork</a> [10]. The helper functions are represented by lower-case versions of the corresponding class name, and usually input at the very least the input-dimension or shape of the distribution to model.
For instance, the helper function corresponding to <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.SplineCoupling">SplineCoupling</a> is <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#spline-coupling">spline_coupling</a>. We create a bivariate flow with a single spline coupling layer as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">base_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="n">spline_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">spline_coupling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">count_bins</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">flow_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">spline_transform</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Similarly to before, we train this distribution on the toy dataset and plot the results:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">smoke_test</span> <span class="k">else</span> <span class="mi">5001</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">spline_transform</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">flow_dist</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;step: </span><span class="si">{}</span><span class="s1">, loss: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
step: 0, loss: 8.446191787719727
step: 500, loss: 2.0197808742523193
step: 1000, loss: 1.794958472251892
step: 1500, loss: 1.73616361618042
step: 2000, loss: 1.7254879474639893
step: 2500, loss: 1.691617488861084
step: 3000, loss: 1.679549217224121
step: 3500, loss: 1.6967085599899292
step: 4000, loss: 1.6723777055740356
step: 4500, loss: 1.6505967378616333
step: 5000, loss: 1.8024061918258667
CPU times: user 10min 41s, sys: 14.7 s, total: 10min 56s
Wall time: 1min 39s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X_flow</span> <span class="o">=</span> <span class="n">flow_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1000</span><span class="p">,]))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Joint Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_flow</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_1)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_27_0.png" src="_images/normalizing_flows_i_27_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_27_1.png" src="_images/normalizing_flows_i_27_1.png" />
</div>
</div>
<p>We see from the output that this normalizing flow has successfully learnt both the univariate marginals <em>and</em> the bivariate distribution.</p>
</div>
</div>
<div class="section" id="Conditional-versus-Joint-Distributions">
<h2>Conditional versus Joint Distributions<a class="headerlink" href="#Conditional-versus-Joint-Distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>Background<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>In many cases, we wish to represent conditional rather than joint distributions. For instance, in performing variational inference, the variational family is a class of conditional distributions,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\{q_\psi(\mathbf{z}\mid\mathbf{x})\mid\theta\in\Theta\},
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is the latent variable and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> the observed one, that hopefully contains a member close to the true posterior of the model, <span class="math notranslate nohighlight">\(p(\mathbf{z}\mid\mathbf{x})\)</span>. In other cases, we may wish to learn to generate an object <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> conditioned on some context <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> using <span class="math notranslate nohighlight">\(p_\theta(\mathbf{x}\mid\mathbf{c})\)</span> and observations <span class="math notranslate nohighlight">\(\{(\mathbf{x}_n,\mathbf{c}_n)\}^N_{n=1}\)</span>. For instance, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> may be a spoken sentence
and <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> a number of speech features.</p>
<p>The theory of Normalizing Flows is easily generalized to conditional distributions. We denote the variable to condition on by <span class="math notranslate nohighlight">\(C=\mathbf{c}\in\mathbb{R}^M\)</span>. A simple multivariate source of noise, for example a standard i.i.d. normal distribution, <span class="math notranslate nohighlight">\(X\sim\mathcal{N}(\mathbf{0},I_{D\times D})\)</span>, is passed through a vector-valued bijection that also conditions on C, <span class="math notranslate nohighlight">\(g:\mathbb{R}^D\times\mathbb{R}^M\rightarrow\mathbb{R}^D\)</span>, to produce the more complex transformed variable
<span class="math notranslate nohighlight">\(Y=g(X;C=\mathbf{c})\)</span>. In practice, this is usually accomplished by making the parameters for a known normalizing flow bijection <span class="math notranslate nohighlight">\(g\)</span> the output of a hypernet neural network that inputs <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>.</p>
<p>Sampling of conditional transforms simply involves evaluating <span class="math notranslate nohighlight">\(Y=g(X; C=\mathbf{c})\)</span>. Conditioning the bijections on <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, the same formula holds for scoring as for the joint multivariate case.</p>
</div>
<div class="section" id="Conditional-Transforms-in-Pyro">
<h3>Conditional Transforms in Pyro<a class="headerlink" href="#Conditional-Transforms-in-Pyro" title="Permalink to this headline">¶</a></h3>
<p>In Pyro, most learnable transforms have a corresponding conditional version that derives from <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#conditionaltransformmodule">ConditionalTransformModule</a>. For instance, the conditional version of the spline transform is <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#conditionalspline">ConditionalSpline</a> with helper function <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#conditional-spline">conditional_spline</a>.</p>
<p>In this section, we will show how we can learn our toy dataset as the decomposition of the product of a conditional and a univariate distribution,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
p(x_1,x_2) &amp;= p(x_2\mid x_1)p(x_1).
\end{align}\]</div>
<p>First, we create the univariate distribution for <span class="math notranslate nohighlight">\(x_1\)</span> as shown previously,</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dist_base</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x1_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">spline</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dist_x1</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">dist_base</span><span class="p">,</span> <span class="p">[</span><span class="n">x1_transform</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>A conditional transformed distribution is created by passing the base distribution and list of conditional and non-conditional transforms to the <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#conditionaltransformeddistribution">ConditionalTransformedDistribution</a> class:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x2_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">conditional_spline</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">context_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dist_x2_given_x1</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">ConditionalTransformedDistribution</span><span class="p">(</span><span class="n">dist_base</span><span class="p">,</span> <span class="p">[</span><span class="n">x2_transform</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>You will notice that we pass the dimension of the context variable, <span class="math notranslate nohighlight">\(M=1\)</span>, to the conditional spline helper function.</p>
<p>Until we condition on a value of <span class="math notranslate nohighlight">\(x_1\)</span>, the <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#conditionaltransformeddistribution">ConditionalTransformedDistribution</a> object is merely a placeholder and cannot be used for sampling or scoring. By calling its <a class="reference external" href="http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.ConditionalDistribution.condition">.condition(context)</a> method, we obtain a
<a class="reference external" href="https://pytorch.org/docs/master/distributions.html#transformeddistribution">TransformedDistribution</a> for which all its conditional transforms have been conditioned on <code class="docutils literal notranslate"><span class="pre">context</span></code>.</p>
<p>For example, to draw a sample from <span class="math notranslate nohighlight">\(x_2\mid x_1=1\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dist_x2_given_x1</span><span class="o">.</span><span class="n">condition</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([-0.4529])
</pre></div></div>
</div>
<p>In general, the context variable may have batch dimensions and these dimensions must broadcast over the batch dimensions of the input variable.</p>
<p>Now, combining the two distributions and training it on the toy dataset:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">smoke_test</span> <span class="k">else</span> <span class="mi">5001</span>
<span class="n">modules</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">x1_transform</span><span class="p">,</span> <span class="n">x2_transform</span><span class="p">])</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][:,</span><span class="kc">None</span><span class="p">]</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][:,</span><span class="kc">None</span><span class="p">]</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">ln_p_x1</span> <span class="o">=</span> <span class="n">dist_x1</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">ln_p_x2_given_x1</span> <span class="o">=</span> <span class="n">dist_x2_given_x1</span><span class="o">.</span><span class="n">condition</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">ln_p_x1</span> <span class="o">+</span> <span class="n">ln_p_x2_given_x1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">dist_x1</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
    <span class="n">dist_x2_given_x1</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;step: </span><span class="si">{}</span><span class="s1">, loss: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
step: 0, loss: 5.663331031799316
step: 500, loss: 2.040316581726074
step: 1000, loss: 1.9603266716003418
step: 1500, loss: 1.8922736644744873
step: 2000, loss: 1.8509924411773682
step: 2500, loss: 1.8328033685684204
step: 3000, loss: 1.9166260957717896
step: 3500, loss: 1.877435326576233
step: 4000, loss: 1.8416743278503418
step: 4500, loss: 1.8379391431808472
step: 5000, loss: 1.824593424797058
CPU times: user 9min 24s, sys: 5.93 s, total: 9min 30s
Wall time: 1min 23s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x1_flow</span> <span class="o">=</span> <span class="n">dist_x1</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1000</span><span class="p">,]))</span>
<span class="n">x2_flow</span> <span class="o">=</span> <span class="n">dist_x2_given_x1</span><span class="o">.</span><span class="n">condition</span><span class="p">(</span><span class="n">x1_flow</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1000</span><span class="p">,]))</span>
<span class="n">X_flow</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x1_flow</span><span class="p">,</span> <span class="n">x2_flow</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Joint Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_flow</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_1)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">X_flow</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span>
             <span class="n">hist_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">},</span>
             <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;linewidth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">&#39;flow&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(x_2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_38_0.png" src="_images/normalizing_flows_i_38_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/normalizing_flows_i_38_1.png" src="_images/normalizing_flows_i_38_1.png" />
</div>
</div>
</div>
<div class="section" id="Conclusions">
<h3>Conclusions<a class="headerlink" href="#Conclusions" title="Permalink to this headline">¶</a></h3>
<p>In this tutorial, we have explained the basic idea behind normalizing flows and the Pyro interface to create flows to represent univariate, multivariate, and conditional distributions. It is useful to think of flows as a powerful general-purpose tool in your probabilistic modelling toolkit, and you can replace any existing distribution in your model with one to increase its flexibility and performance. We hope you have fun exploring the power of normalizing flows!</p>
</div>
<div class="section" id="References">
<h3>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>E.G. Tabak, Christina Turner. <a class="reference external" href="https://www.math.nyu.edu/faculty/tabak/publications/Tabak-Turner.pdf">A Family of Nonparametric Density Estimation Algorithms</a>. Communications on Pure and Applied Mathematics, 66(2):145–164, 2013.</p></li>
<li><p>Danilo Jimenez Rezende, Shakir Mohamed. <a class="reference external" href="http://proceedings.mlr.press/v37/rezende15.pdf">Variational Inference with Normalizing Flows</a>. ICML 2015.</p></li>
<li><p>Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. <a class="reference external" href="https://arxiv.org/abs/1908.09257">Normalizing Flows: An Introduction and Review of Current Methods</a>. [arXiv:1908.09257] 2019.</p></li>
<li><p>George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, Balaji Lakshminarayanan. <a class="reference external" href="https://arxiv.org/abs/1912.02762">Normalizing Flows for Probabilistic Modeling and Inference</a>. [arXiv:1912.02762] 2019.</p></li>
<li><p>Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. <a class="reference external" href="https://arxiv.org/abs/1906.04032">Neural Spline Flows</a>. NeurIPS 2019.</p></li>
<li><p>Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. <a class="reference external" href="https://arxiv.org/abs/2001.05168">Invertible Generative Modeling using Linear Rational Splines</a>. AISTATS 2020.</p></li>
<li><p>James Stewart. <a class="reference external" href="https://www.stewartcalculus.com/">Calculus</a>. Cengage Learning. 9th Edition 2020.</p></li>
<li><p>Laurent Dinh, David Krueger, Yoshua Bengio. <a class="reference external" href="https://arxiv.org/abs/1410.8516">NICE: Non-linear Independent Components Estimation</a>. Workshop contribution at ICLR 2015.</p></li>
<li><p>Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio. <a class="reference external" href="https://arxiv.org/abs/1605.08803">Density estimation using Real-NVP</a>. Conference paper at ICLR 2017.</p></li>
<li><p>David Ha, Andrew Dai, Quoc V. Le. <a class="reference external" href="https://arxiv.org/abs/1609.09106">HyperNetworks</a>. Workshop contribution at ICLR 2017.</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dmm.html" class="btn btn-neutral float-right" title="Deep Markov Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cvae.html" class="btn btn-neutral float-left" title="Conditional Variational Auto-encoder" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2017-2018, Uber Technologies, Inc

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>