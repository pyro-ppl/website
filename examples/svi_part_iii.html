<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Pyro Tutorials 1.8.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="svi_part_iv.html" />
    <link rel="prev" title="&lt;no title&gt;" href="svi_part_ii.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/svi_part_iii.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<dl>
<dt>{</dt><dd><dl>
<dt>“cells”: [</dt><dd><dl>
<dt>{</dt><dd><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“# SVI Part III: ELBO Gradient Estimatorsn”,
“n”,
“## Setupn”,
“n”,
“We’ve defined a Pyro model with observations ${\bf x}$ and latents ${\bf z}$ of the form $p_{\theta}({\bf x}, {\bf z}) = p_{\theta}({\bf x}|{\bf z}) p_{\theta}({\bf z})$. We’ve also defined a Pyro guide (i.e. a variational distribution) of the form $q_{\phi}({\bf z})$. Here ${\theta}$ and $\phi$ are variational parameters for the model and guide, respectively. (In particular these are _not_ random variables that call for a Bayesian treatment).n”,
“n”,
“We’d like to maximize the log evidence $\log p_{\theta}({\bf x})$ by maximizing the ELBO (the evidence lower bound) given by n”,
“n”,
“$${\rm ELBO} \equiv \mathbb{E}_{q_{\phi}({\bf z})} \left [ n”,
“\log p_{\theta}({\bf x}, {\bf z}) - \log q_{\phi}({\bf z})n”,
“\right]$$n”,
“n”,
“To do this we’re going to take (stochastic) gradient steps on the ELBO in the parameter space $\{ \theta, \phi \}$ (see references [1,2] for early work on this approach). So we need to be able to compute unbiased estimates of n”,
“n”,
“$$\nabla_{\theta,\phi} {\rm ELBO} = \nabla_{\theta,\phi}\mathbb{E}_{q_{\phi}({\bf z})} \left [ n”,
“\log p_{\theta}({\bf x}, {\bf z}) - \log q_{\phi}({\bf z})n”,
“\right]$$n”,
“n”,
“How do we do this for general stochastic functions <cite>model()</cite> and <cite>guide()</cite>? To simplify notation let’s generalize our discussion a bit and ask how we can compute gradients of expectations of an arbitrary cost function $f({\bf z})$. Let’s also drop any distinction between $\theta$ and $\phi$. So we want to computen”,
“n”,
“$$\nabla_{\phi}\mathbb{E}_{q_{\phi}({\bf z})} \left [n”,
“f_{\phi}({\bf z}) \right]$$n”,
“n”,
“Let’s start with the easiest case.n”,
“n”,
“## Easy Case: Reparameterizable Random Variablesn”,
“n”,
“Suppose that we can reparameterize things such that n”,
“n”,
“$$\mathbb{E}_{q_{\phi}({\bf z})} \left [f_{\phi}({\bf z}) \right]n”,
“=\mathbb{E}_{q({\bf \epsilon})} \left [f_{\phi}(g_{\phi}({\bf \epsilon})) \right]$$n”,
“n”,
“Crucially we’ve moved all the $\phi$ dependence inside of the expectation; $q({\bf \epsilon})$ is a fixed distribution with no dependence on $\phi$. This kind of reparameterization can be done for many distributions (e.g. the normal distribution); see reference [3] for a discussion. In this case we can pass the gradient straight through the expectation to getn”,
“n”,
“$$\nabla_{\phi}\mathbb{E}_{q({\bf \epsilon})} \left [f_{\phi}(g_{\phi}({\bf \epsilon})) \right]=n”,
“\mathbb{E}_{q({\bf \epsilon})} \left [\nabla_{\phi}f_{\phi}(g_{\phi}({\bf \epsilon})) \right]$$n”,
“n”,
“Assuming $f(\cdot)$ and $g(\cdot)$ are sufficiently smooth, we can now get unbiased estimates of the gradient of interest by taking a Monte Carlo estimate of this expectation.n”,
“n”,
“## Tricky Case: Non-reparameterizable Random Variablesn”,
“n”,
“What if we can’t do the above reparameterization? Unfortunately this is the case for many distributions of interest, for example all discrete distributions. In this case our estimator takes a bit more complicated form.n”,
“n”,
“We begin by expanding the gradient of interest asn”,
“n”,
“$$\nabla_{\phi}\mathbb{E}_{q_{\phi}({\bf z})} \left [n”,
“f_{\phi}({\bf z}) \right]= n”,
“\nabla_{\phi} \int d{\bf z} \; q_{\phi}({\bf z}) f_{\phi}({\bf z})$$n”,
“n”,
“and use the chain rule to write this as n”,
“n”,
“$$ \int d{\bf z} \; \left \{ (\nabla_{\phi}  q_{\phi}({\bf z})) f_{\phi}({\bf z}) + q_{\phi}({\bf z})(\nabla_{\phi} f_{\phi}({\bf z}))\right \} $$n”,
“n”,
“At this point we run into a problem. We know how to generate samples from $q(\cdot)$&amp;mdash;we just run the guide forward&amp;mdash;but $\nabla_{\phi}  q_{\phi}({\bf z})$ isn’t even a valid probability density. So we need to massage this formula so that it’s in the form of an expectation w.r.t. $q(\cdot)$. This is easily done using the identityn”,
“n”,
“$$ \nabla_{\phi}  q_{\phi}({\bf z}) = n”,
“q_{\phi}({\bf z})\nabla_{\phi} \log q_{\phi}({\bf z})$$n”,
“n”,
“which allows us to rewrite the gradient of interest as n”,
“n”,
“$$\mathbb{E}_{q_{\phi}({\bf z})} \left [n”,
“(\nabla_{\phi} \log q_{\phi}({\bf z})) f_{\phi}({\bf z}) + \nabla_{\phi} f_{\phi}({\bf z})\right]$$n”,
“n”,
“This form of the gradient estimator&amp;mdash;variously known as the REINFORCE estimator or the score function estimator or the likelihood ratio estimator&amp;mdash;is amenable to simple Monte Carlo estimation.n”,
“n”,
“Note that one way to package this result (which is convenient for implementation) is to introduce a surrogate objective functionn”,
“n”,
“$${\rm surrogate \;objective} \equivn”,
“\log q_{\phi}({\bf z}) \overline{f_{\phi}({\bf z})} + f_{\phi}({\bf z})$$  n”,
“n”,
“Here the bar indicates that the term is held constant (i.e. it is not to be differentiated w.r.t. $\phi$). To get a (single-sample) Monte Carlo gradient estimate, we sample the latent random variables, compute the surrogate objective, and differentiate. The result is an unbiased estimate of $\nabla_{\phi}\mathbb{E}_{q_{\phi}({\bf z})} \left [n”,
“f_{\phi}({\bf z}) \right]$. In equations:n”,
“n”,
“$$\nabla_{\phi} {\rm ELBO} = \mathbb{E}_{q_{\phi}({\bf z})} \left [ n”,
“\nabla_{\phi} ({\rm surrogate \; objective}) \right]$$n”,
“n”,
“## Variance or Why I Wish I Was Doing MLE Deep Learningn”,
“n”,
“We now have a general recipe for an unbiased gradient estimator of expectations of cost functions. Unfortunately, in the more general case where our $q(\cdot)$ includes non-reparameterizable random variables, this estimator tends to have high variance. Indeed in many cases of interest the variance is so high that the estimator is effectively unusable. So we need strategies to reduce variance (for a discussion see reference [4]). We’re going to pursue two strategies. The first strategy takes advantage of the particular structure of the cost function $f(\cdot)$. The second strategy effectively introduces a way to reduce variance by using information from previous estimates of n”,
“$\mathbb{E}_{q_{\phi}({\bf z})} [ f_{\phi}({\bf z})]$. As such it is somewhat analogous to using momentum in stochastic gradient descent. n”,
“n”,
“### Reducing Variance via Dependency Structuren”,
“n”,
“In the above discussion we stuck to a general cost function $f_{\phi}({\bf z})$. We could continue in this vein (the approach we’re about to discuss is applicable in the general case) but for concreteness let’s zoom back in. In the case of stochastic variational inference, we’re interested in a particular cost function of the form &lt;br/&gt;&lt;br/&gt;n”,
“n”,
“$$\log p_{\theta}({\bf x} | {\rm Pa}_p ({\bf x})) +n”,
“\sum_i \log p_{\theta}({\bf z}_i | {\rm Pa}_p ({\bf z}_i)) n”,
“- \sum_i \log q_{\phi}({\bf z}_i | {\rm Pa}_q ({\bf z}_i))$$n”,
“n”,
“where we’ve broken the log ratio $\log p_{\theta}({\bf x}, {\bf z})/q_{\phi}({\bf z})$ into an observation log likelihood piece and a sum over the different latent random variables $\{{\bf z}_i \}$. We’ve also introduced the notation n”,
“${\rm Pa}_p (\cdot)$ and ${\rm Pa}_q (\cdot)$ to denote the parents of a given random variable in the model and in the guide, respectively. (The reader might worry what the appropriate notion of dependency would be in the case of general stochastic functions; here we simply mean regular ol’ dependency within a single execution trace). The point is that different terms in the cost function have different dependencies on the random variables $\{ {\bf z}_i \}$ and this is something we can leverage.n”,
“n”,
“To make a long story short, for any non-reparameterizable latent random variable ${\bf z}_i$ the surrogate objective is going to have a term n”,
“n”,
“$$\log q_{\phi}({\bf z}_i) \overline{f_{\phi}({\bf z})} $$n”,
“n”,
“It turns out that we can remove some of the terms in $\overline{f_{\phi}({\bf z})}$ and still get an unbiased gradient estimator; furthermore, doing so will generally decrease the variance. In particular (see reference [4] for details) we can remove any terms in $\overline{f_{\phi}({\bf z})}$ that are not downstream of the latent variable ${\bf z}_i$ (downstream w.r.t. to the dependency structure of the guide). Note that this general trick&amp;mdash;where certain random variables are dealt with analytically to reduce variance&amp;mdash;often goes under the name of Rao-Blackwellization.n”,
“n”,
“In Pyro, all of this logic is taken care of automatically by the <cite>SVI</cite> class. In particular as long as we use a <cite>TraceGraph_ELBO</cite> loss, Pyro will keep track of the dependency structure within the execution traces of the model and guide and construct a surrogate objective that has all the unnecessary terms removed:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;svi</span> <span class="pre">=</span> <span class="pre">SVI(model,</span> <span class="pre">guide,</span> <span class="pre">optimizer,</span> <span class="pre">TraceGraph_ELBO())\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“Note that leveraging this dependency information takes extra computations, so <cite>TraceGraph_ELBO</cite> should only be used in the case where your model has non-reparameterizable random variables; in most applications <cite>Trace_ELBO</cite> suffices.”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### An Example with Rao-Blackwellization:n”,
“n”,
“Suppose we have a gaussian mixture model with $K$ components. For each data point we: (i) first sample the component distribution $k \in [1,…,K]$; and (ii) observe the data point using the $k^{\rm th}$ component distribution. The simplest way to write down a model of this sort is as follows:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;ks</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;k\&quot;,</span> <span class="pre">dist.Categorical(probs)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">.to_event(1))\n&quot;,</span>
<span class="pre">&quot;pyro.sample(\&quot;obs\&quot;,</span> <span class="pre">dist.Normal(locs[ks],</span> <span class="pre">scale)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">.to_event(1),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">obs=data)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“Since the user hasn’t taken care to mark any of the conditional independencies in the model, the gradient estimator constructed by Pyro’s <cite>SVI</cite> class is unable to take advantage of Rao-Blackwellization, with the result that the gradient estimator will tend to suffer from high variance. To address this problem the user needs to explicitly mark the conditional independence. Happily, this is not much work:n”,
“n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">mark</span> <span class="pre">conditional</span> <span class="pre">independence</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">(assumed</span> <span class="pre">to</span> <span class="pre">be</span> <span class="pre">along</span> <span class="pre">the</span> <span class="pre">rightmost</span> <span class="pre">tensor</span> <span class="pre">dimension)\n&quot;,</span>
<span class="pre">&quot;with</span> <span class="pre">pyro.plate(\&quot;foo\&quot;,</span> <span class="pre">data.size(-1)):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">ks</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;k\&quot;,</span> <span class="pre">dist.Categorical(probs))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;obs\&quot;,</span> <span class="pre">dist.Normal(locs[ks],</span> <span class="pre">scale),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">obs=data)\n&quot;,</span>
<span class="pre">&quot;`</span></code>      n”,
“n”,
“That’s all there is to it.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Aside: Dependency tracking in Pyron”,
“n”,
“Finally, a word about dependency tracking. Tracking dependency within a stochastic function that includes arbitrary Python code is a bit tricky. The approach currently implemented in Pyro is analogous to the one used in WebPPL (cf. reference [5]). Briefly, a conservative notion of dependency is used that relies on sequential ordering. If random variable ${\bf z}_2$ follows ${\bf z}_1$ in a given stochastic function then ${\bf z}_2$ _may <a href="#id1"><span class="problematic" id="id2">be_</span></a> dependent on ${\bf z}_1$ and therefore _is_ assumed to be dependent. To mitigate the overly coarse conclusions that can be drawn by this kind of dependency tracking, Pyro includes constructs for declaring things as independent, namely <cite>plate</cite> and <cite>markov</cite> ([see the previous tutorial](svi_part_ii.ipynb)). For use cases with non-reparameterizable variables, it is therefore important for the user to make use of these constructs (when applicable) to take full advantage of the variance reduction provided by <cite>SVI</cite>. In some cases it may also pay to consider reordering random variables within a stochastic function (if possible).n”,
“n”,
“### Reducing Variance with Data-Dependent Baselinesn”,
“n”,
“The second strategy for reducing variance in our ELBO gradient estimator goes under the name of baselines (see e.g. reference [6]). It actually makes use of the same bit of math that underlies the variance reduction strategy discussed above, except now instead of removing terms we’re going to add terms. Basically, instead of removing terms with zero expectation that tend to _contribute_ to the variance, we’re going to add specially chosen terms with zero expectation that work to _reduce_ the variance. As such, this is a control variate strategy.n”,
“n”,
“In more detail, the idea is to take advantage of the fact that for any constant $b$, the following identity holdsn”,
“n”,
“$$\mathbb{E}_{q_{\phi}({\bf z})} \left [\nabla_{\phi}n”,
“(\log q_{\phi}({\bf z}) \times b) \right]=0$$n”,
“n”,
“This follows since $q(\cdot)$ is normalized:n”,
“n”,
“$$\mathbb{E}_{q_{\phi}({\bf z})} \left [\nabla_{\phi}n”,
“\log q_{\phi}({\bf z}) \right]=n”,
” \int \!d{\bf z} \; q_{\phi}({\bf z}) \nabla_{\phi}n”,
“\log q_{\phi}({\bf z})=n”,
” \int \! d{\bf z} \; \nabla_{\phi} q_{\phi}({\bf z})=n”,
“\nabla_{\phi} \int \! d{\bf z} \;  q_{\phi}({\bf z})=\nabla_{\phi} 1 = 0$$n”,
“n”,
“What this means is that we can replace any termn”,
“n”,
“$$\log q_{\phi}({\bf z}_i) \overline{f_{\phi}({\bf z})} $$n”,
“n”,
“in our surrogate objective withn”,
“n”,
“$$\log q_{\phi}({\bf z}_i) \left(\overline{f_{\phi}({\bf z})}-b\right) $$n”,
“n”,
“Doing so doesn’t affect the mean of our gradient estimator but it does affect the variance. If we choose $b$ wisely, we can hope to reduce the variance. In fact, $b$ need not be a constant: it can depend on any of the random choices upstream (or sidestream) of ${\bf z}_i$.n”,
“n”,
“#### Baselines in Pyron”,
“n”,
“There are several ways the user can instruct Pyro to use baselines in the context of stochastic variational inference. Since baselines can be attached to any non-reparameterizable random variable, the current baseline interface is at the level of the <cite>pyro.sample</cite> statement. In particular the baseline interface makes use of an argument <cite>baseline</cite>, which is a dictionary that specifies baseline options. Note that it only makes sense to specify baselines for sample statements within the guide (and not in the model).n”,
“n”,
“##### Decaying Average Baselinen”,
“n”,
“The simplest baseline is constructed from a running average of recent samples of $\overline{f_{\phi}({\bf z})}$. In Pyro this kind of baseline can be invoked as followsn”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;z</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z\&quot;,</span> <span class="pre">dist.Bernoulli(...),</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">infer=dict(baseline={'use_decaying_avg_baseline':</span> <span class="pre">True,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">'baseline_beta':</span> <span class="pre">0.95}))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“The optional argument <cite>baseline_beta</cite> specifies the decay rate of the decaying average (default value: <cite>0.90</cite>).n”,
“n”,
“#### Neural Baselinesn”,
“n”,
“In some cases a decaying average baseline works well. In others using a baseline that depends on upstream randomness is crucial for getting good variance reduction. A powerful approach for constructing such a baseline is to use a neural network that can be adapted during the course of learning. Pyro provides two ways to specify such a baseline (for an extended example see the [AIR tutorial](air.ipynb)).n”,
“n”,
“First the user needs to decide what inputs the baseline is going to consume (e.g. the current datapoint under consideration or the previously sampled random variable). Then the user needs to construct a <cite>nn.Module</cite> that encapsulates the baseline computation. This might look something liken”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;class</span> <span class="pre">BaselineNN(nn.Module):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">dim_input,</span> <span class="pre">dim_hidden):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">super().__init__()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.linear</span> <span class="pre">=</span> <span class="pre">nn.Linear(dim_input,</span> <span class="pre">dim_hidden)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">...</span> <span class="pre">finish</span> <span class="pre">initialization</span> <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">forward(self,</span> <span class="pre">x):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">hidden</span> <span class="pre">=</span> <span class="pre">self.linear(x)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">...</span> <span class="pre">do</span> <span class="pre">more</span> <span class="pre">computations</span> <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">baseline\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“Then, assuming the BaselineNN object <cite>baseline_module</cite> has been initialized somewhere else, in the guide we’ll have something liken”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">guide(x):</span>&#160; <span class="pre">#</span> <span class="pre">here</span> <span class="pre">x</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">current</span> <span class="pre">mini-batch</span> <span class="pre">of</span> <span class="pre">data\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.module(\&quot;my_baseline\&quot;,</span> <span class="pre">baseline_module)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">...</span> <span class="pre">other</span> <span class="pre">computations</span> <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">z</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z\&quot;,</span> <span class="pre">dist.Bernoulli(...),</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">infer=dict(baseline={'nn_baseline':</span> <span class="pre">baseline_module,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">'nn_baseline_input':</span> <span class="pre">x}))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“Here the argument <cite>nn_baseline</cite> tells Pyro which <cite>nn.Module</cite> to use to construct the baseline. On the backend the argument <cite>nn_baseline_input</cite> is fed into the forward method of the module to compute the baseline $b$. Note that the baseline module needs to be registered with Pyro with a <cite>pyro.module</cite> call so that Pyro is aware of the trainable parameters within the module.n”,
“n”,
“Under the hood Pyro constructs a loss of the form n”,
“n”,
“$${\rm baseline\; loss} \equiv\left(\overline{f_{\phi}({\bf z})} - b  \right)^2$$n”,
“n”,
“which is used to adapt the parameters of the neural network. There’s no theorem that suggests this is the optimal loss function to use in this context (it’s not), but in practice it can work pretty well. Just as for the decaying average baseline, the idea is that a baseline that can track the mean $\overline{f_{\phi}({\bf z})}$ will help reduce the variance. Under the hood <cite>SVI</cite> takes one step on the baseline loss in conjunction with a step on the ELBO. n”,
“n”,
“Note that in practice it can be important to use a different set of learning hyperparameters (e.g. a higher learning rate) for baseline parameters. In Pyro this can be done as follows:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">per_param_args(param_name):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">if</span> <span class="pre">'baseline'</span> <span class="pre">in</span> <span class="pre">param_name:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">{\&quot;lr\&quot;:</span> <span class="pre">0.010}\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">else:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">{\&quot;lr\&quot;:</span> <span class="pre">0.001}\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;optimizer</span> <span class="pre">=</span> <span class="pre">optim.Adam(per_param_args)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“Note that in order for the overall procedure to be correct the baseline parameters should only be optimized through the baseline loss. Similarly the model and guide parameters should only be optimized through the ELBO. To ensure that this is the case under the hood <cite>SVI</cite> detaches the baseline $b$ that enters the ELBO from the autograd graph.  Also, since the inputs to the neural baseline may depend on the parameters of the model and guide, the inputs are also detached from the autograd graph before they are fed into the neural network. n”,
“n”,
“Finally, there is an alternate way for the user to specify a neural baseline. Simply use the argument <cite>baseline_value</cite>:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;b</span> <span class="pre">=</span> <span class="pre">#</span> <span class="pre">do</span> <span class="pre">baseline</span> <span class="pre">computation\n&quot;,</span>
<span class="pre">&quot;z</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z\&quot;,</span> <span class="pre">dist.Bernoulli(...),</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">infer=dict(baseline={'baseline_value':</span> <span class="pre">b}))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“This works as above, except in this case it’s the user’s responsibility to make sure that any autograd tape connecting $b$ to the parameters of the model and guide has been cut. Or to say the same thing in language more familiar to PyTorch users, any inputs to $b$ that depend on $\theta$ or $\phi$ need to be detached from the autograd graph with <cite>detach()</cite> statements.n”,
“n”,
“#### A complete example with baselinesn”,
“n”,
“Recall that in the [first SVI tutorial](svi_part_i.ipynb) we considered a bernoulli-beta model for coin flips. Because the beta random variable is non-reparameterizable (or rather not easily reparameterizable), the corresponding ELBO gradients can be quite noisy. In that context we dealt with this problem by using a Beta distribution that provides (approximate) reparameterized gradients. Here we showcase how a simple decaying average baseline can reduce the variance in the case where the Beta distribution is treated as non-reparameterized (so that the ELBO gradient estimator is of the score function type). While we’re at it, we also use <cite>plate</cite> to write our model in a fully vectorized manner.n”,
“n”,
“Instead of directly comparing gradient variances, we’re going to see how many steps it takes for SVI to converge. Recall that for this particular model (because of conjugacy) we can compute the exact posterior. So to assess the utility of baselines in this context, we setup the following simple experiment. We initialize the guide at a specified set of variational parameters. We then do SVI until the variational parameters have gotten to within a fixed tolerance of the parameters of the exact posterior. We do this both with and without the decaying average baseline. We then compare the number of gradient steps we needed in the two cases. Here’s the complete code:n”,
“n”,
“(_Since apart from the use <a href="#id3"><span class="problematic" id="id4">of_</span></a> <cite>plate</cite> _and_ <cite>use_decaying_avg_baseline</cite>, _this code is very similar to the code in parts I and II of the SVI tutorial, we’re not going to go through the code line by line._)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: null,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“import osn”,
“import torchn”,
“import torch.distributions.constraints as constraintsn”,
“import pyron”,
“import pyro.distributions as distn”,
“# Pyro also has a reparameterized Beta distribution so we importn”,
“# the non-reparameterized version to make our pointn”,
“from pyro.distributions.testing.fakes import NonreparameterizedBetan”,
“import pyro.optim as optimn”,
“from pyro.infer import SVI, TraceGraph_ELBOn”,
“import sysn”,
“n”,
“assert pyro.__version__.startswith(‘1.8.1’)n”,
“n”,
“# this is for running the notebook in our testing frameworkn”,
“smoke_test = (‘CI’ in os.environ)n”,
“max_steps = 2 if smoke_test else 10000n”,
“n”,
“n”,
“def param_abs_error(name, target):n”,
”    return torch.sum(torch.abs(target - pyro.param(name))).item()n”,
“n”,
“n”,
“class BernoulliBetaExample:n”,
”    def __init__(self, max_steps):n”,
”        # the maximum number of inference steps we don”,
”        self.max_steps = max_stepsn”,
”        # the two hyperparameters for the beta priorn”,
”        self.alpha0 = 10.0n”,
”        self.beta0 = 10.0n”,
”        # the dataset consists of six 1s and four 0sn”,
”        self.data = torch.zeros(10)n”,
”        self.data[0:6] = torch.ones(6)n”,
”        self.n_data = self.data.size(0)n”,
”        # compute the alpha parameter of the exact beta posteriorn”,
”        self.alpha_n = self.data.sum() + self.alpha0n”,
”        # compute the beta parameter of the exact beta posteriorn”,
”        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)n”,
”        # initial values of the two variational parametersn”,
”        self.alpha_q_0 = 15.0n”,
”        self.beta_q_0 = 15.0n”,
“n”,
”    def model(self, use_decaying_avg_baseline):n”,
”        # sample <cite>latent_fairness</cite> from the beta priorn”,
”        f = pyro.sample(&quot;latent_fairness&quot;, dist.Beta(self.alpha0, self.beta0))n”,
”        # use plate to indicate that the observations aren”,
”        # conditionally independent given f and get vectorizationn”,
”        with pyro.plate(&quot;data_plate&quot;):n”,
”            # observe all ten datapoints using the bernoulli likelihoodn”,
”            pyro.sample(&quot;obs&quot;, dist.Bernoulli(f), obs=self.data)n”,
“n”,
”    def guide(self, use_decaying_avg_baseline):n”,
”        # register the two variational parameters with pyron”,
”        alpha_q = pyro.param(&quot;alpha_q&quot;, torch.tensor(self.alpha_q_0),n”,
”                             constraint=constraints.positive)n”,
”        beta_q = pyro.param(&quot;beta_q&quot;, torch.tensor(self.beta_q_0),n”,
”                            constraint=constraints.positive)n”,
”        # sample f from the beta variational distributionn”,
”        baseline_dict = {‘use_decaying_avg_baseline’: use_decaying_avg_baseline,n”,
”                         ‘baseline_beta’: 0.90}n”,
”        # note that the baseline_dict specifies whether we’re usingn”,
”        # decaying average baselines or notn”,
”        pyro.sample(&quot;latent_fairness&quot;, NonreparameterizedBeta(alpha_q, beta_q),n”,
”                    infer=dict(baseline=baseline_dict))n”,
“n”,
”    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):n”,
”        # clear the param store in case we’re in a REPLn”,
”        pyro.clear_param_store()n”,
”        # setup the optimizer and the inference algorithmn”,
”        optimizer = optim.Adam({&quot;lr&quot;: .0005, &quot;betas&quot;: (0.93, 0.999)})n”,
”        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())n”,
”        print(&quot;Doing inference with use_decaying_avg_baseline=%s&quot; % use_decaying_avg_baseline)n”,
“n”,
”        # do up to this many steps of inferencen”,
”        for k in range(self.max_steps):n”,
”            svi.step(use_decaying_avg_baseline)n”,
”            if k % 100 == 0:n”,
”                print(‘.’, end=’’)n”,
”                sys.stdout.flush()n”,
“n”,
”            # compute the distance to the parameters of the true posteriorn”,
”            alpha_error = param_abs_error(&quot;alpha_q&quot;, self.alpha_n)n”,
”            beta_error = param_abs_error(&quot;beta_q&quot;, self.beta_n)n”,
“n”,
”            # stop inference early if we’re close to the true posteriorn”,
”            if alpha_error &lt; tolerance and beta_error &lt; tolerance:n”,
”                breakn”,
“n”,
”        print(&quot;\nDid %d steps of inference.&quot; % k)n”,
”        print((&quot;Final absolute errors for the two variational parameters &quot; +n”,
”               &quot;were %.4f &amp; %.4f&quot;) % (alpha_error, beta_error))n”,
“n”,
“# do the experimentn”,
“bbe = BernoulliBetaExample(max_steps=max_steps)n”,
“bbe.do_inference(use_decaying_avg_baseline=True)n”,
“bbe.do_inference(use_decaying_avg_baseline=False)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“<strong>Sample output:</strong>n”,
“<code class="docutils literal notranslate"><span class="pre">`\n&quot;,</span>
<span class="pre">&quot;Doing</span> <span class="pre">inference</span> <span class="pre">with</span> <span class="pre">use_decaying_avg_baseline=True\n&quot;,</span>
<span class="pre">&quot;....................\n&quot;,</span>
<span class="pre">&quot;Did</span> <span class="pre">1932</span> <span class="pre">steps</span> <span class="pre">of</span> <span class="pre">inference.\n&quot;,</span>
<span class="pre">&quot;Final</span> <span class="pre">absolute</span> <span class="pre">errors</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">two</span> <span class="pre">variational</span> <span class="pre">parameters</span> <span class="pre">were</span> <span class="pre">0.7997</span> <span class="pre">&amp;</span> <span class="pre">0.0800\n&quot;,</span>
<span class="pre">&quot;Doing</span> <span class="pre">inference</span> <span class="pre">with</span> <span class="pre">use_decaying_avg_baseline=False\n&quot;,</span>
<span class="pre">&quot;..................................................\n&quot;,</span>
<span class="pre">&quot;Did</span> <span class="pre">4908</span> <span class="pre">steps</span> <span class="pre">of</span> <span class="pre">inference.\n&quot;,</span>
<span class="pre">&quot;Final</span> <span class="pre">absolute</span> <span class="pre">errors</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">two</span> <span class="pre">variational</span> <span class="pre">parameters</span> <span class="pre">were</span> <span class="pre">0.7991</span> <span class="pre">&amp;</span> <span class="pre">0.2532\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“For this particular run we can see that baselines roughly halved the number of steps of SVI we needed to do. The results are stochastic and will vary from run to run, but this is an encouraging result. This is a pretty contrived example, but for certain model and guide pairs, baselines can provide a substantial win. “</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Referencesn”,
“n”,
“[1] <cite>Automated Variational Inference in Probabilistic Programming</cite>,n”,
“&lt;br/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“David Wingate, Theo Webern”,
“n”,
“[2] <cite>Black Box Variational Inference</cite>,&lt;br/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Rajesh Ranganath, Sean Gerrish, David M. Blein”,
“n”,
“[3] <cite>Auto-Encoding Variational Bayes</cite>,&lt;br/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Diederik P Kingma, Max Wellingn”,
“n”,
“[4] <cite>Gradient Estimation Using Stochastic Computation Graphs</cite>,n”,
“&lt;br/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
”    John Schulman, Nicolas Heess, Theophane Weber, Pieter Abbeeln”,
”    n”,
“[5] <cite>Deep Amortized Inference for Probabilistic Programs</cite>n”,
“&lt;br/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Daniel Ritchie, Paul Horsfall, Noah D. Goodmann”,
“n”,
“[6] <cite>Neural Variational Inference and Learning in Belief Networks</cite>n”,
“&lt;br/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Andriy Mnih, Karol Gregor”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>],
“metadata”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“kernelspec”: {</dt><dd><p>“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”</p>
</dd>
</dl>
<p>},
“language_info”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“codemirror_mode”: {</dt><dd><p>“name”: “ipython”,
“version”: 3</p>
</dd>
</dl>
<p>},
“file_extension”: “.py”,
“mimetype”: “text/x-python”,
“name”: “python”,
“nbconvert_exporter”: “python”,
“pygments_lexer”: “ipython3”,
“version”: “3.6.10”</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>},
“nbformat”: 4,
“nbformat_minor”: 2</p>
</dd>
</dl>
<p>}</p>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="svi_part_ii.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="svi_part_iv.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>