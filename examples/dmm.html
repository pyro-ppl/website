<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Pyro Tutorials 1.8.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="air.html" />
    <link rel="prev" title="&lt;no title&gt;" href="normalizing_flows_i.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/dmm.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<dl>
<dt>{</dt><dd><dl>
<dt>“cells”: [</dt><dd><dl>
<dt>{</dt><dd><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“# Deep Markov Model n”,
“n”,
“## Introductionn”,
“n”,
“We’re going to build a deep probabilistic model for sequential data: the deep markov model. The particular dataset we want to model is composed of snippets of polyphonic music. Each time slice in a sequence spans a quarter note and is represented by an 88-dimensional binary vector that encodes the notes at that time step. n”,
“n”,
“Since music is (obviously) temporally coherent, we need a model that can represent complex time dependencies in the observed data. It would not, for example, be appropriate to consider a model in which the notes at a particular time step are independent of the notes at previous time steps. One way to do this is to build a latent variable model in which the variability and temporal structure of the observations is controlled by the dynamics of the latent variables. n”,
“n”,
“One particular realization of this idea is a markov model, in which we have a chain of latent variables, with each latent variable in the chain conditioned on the previous latent variable. This is a powerful approach, but if we want to represent complex data with complex (and in this case unknown) dynamics, we would like our model to be sufficiently flexible to accommodate dynamics that are potentially highly non-linear. Thus a deep markov model: we allow for the transition probabilities governing the dynamics of the latent variables as well as the the emission probabilities that govern how the observations are generated by the latent dynamics to be parameterized by (non-linear) neural networks.n”,
“n”,
“The specific model we’re going to implement is based on the following reference:n”,
“n”,
“[1] <cite>Structured Inference Networks for Nonlinear State Space Models</cite>,&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
”    Rahul G. Krishnan, Uri Shalit, David Sontagn”,
”    n”,
“Please note that while we do not assume that the reader of this tutorial has read the reference, it’s definitely a good place to look for a more comprehensive discussion of the deep markov model in  the context of other time series models.n”,
“n”,
“We’ve described the model, but how do we go about training it? The inference strategy we’re going to use is variational inference, which requires specifying a parameterized family of distributions that can be used to approximate the posterior distribution over the latent random variables. Given the non-linearities and complex time-dependencies inherent in our model and data, we expect the exact posterior to be highly non-trivial. So we’re going to need a flexible family of variational distributions if we hope to learn a good model. Happily, together PyTorch and Pyro provide all the necessary ingredients. As we will see, assembling them will be straightforward. Let’s get to work.”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## The Modeln”,
”    n”,
“A convenient way to describe the high-level structure of the model is with a graphical model.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “raw”,
“metadata”: {</p>
<blockquote>
<div><p>“raw_mimetype”: “text/html”</p>
</div></blockquote>
<p>},
“source”: [</p>
<blockquote>
<div><p>“&lt;center&gt;&lt;figure&gt;&lt;img src=&quot;_static/img/model.png&quot; style=&quot;width: 500px;&quot;&gt;&lt;figcaption&gt; &lt;font size=&quot;+1&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: The model rolled out for T=3 time steps.&lt;/font&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/center&gt;”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Here, we’ve rolled out the model assuming that the sequence of observations is of length three: $\{{\bf x}_1, {\bf x}_2, {\bf x}_3\}$. Mirroring the sequence of observations we also have a sequence of latent random variables: $\{{\bf z}_1, {\bf z}_2, {\bf z}_3\}$. The figure encodes the structure of the model. The corresponding joint distribution isn”,
“n”,
“$$p({\bf x}_{123} , {\bf z}_{123})=p({\bf x}_1|{\bf z}_1)p({\bf x}_2|{\bf z}_2)p({\bf x}_3|{\bf z}_3)p({\bf z}_1)p({\bf z}_2|{\bf z}_1)p({\bf z}_3|{\bf z}_2)$$n”,
“n”,
“Conditioned on ${\bf z}_t$, each observation ${\bf x}_t$ is independent of the other observations. This can be read off from the fact that each ${\bf x}_t$ only depends on the corresponding latent ${\bf z}_t$, as indicated by the downward pointing arrows. We can also read off the markov property of the model: each latent ${\bf z}_t$, when conditioned on the previous latent ${\bf z}_{t-1}$, is independent of all previous latents $\{ {\bf z}_{t-2}, {\bf z}_{t-3}, …\}$. This effectively says that everything one needs to know about the state of the system at time $t$ is encapsulated by  the latent ${\bf z}_{t}$.n”,
“n”,
“We will assume that the observation likelihoods, i.e. the probability distributions $p({{\bf x}_t}|{{\bf z}_t})$ that control the observations, are given by the bernoulli distribution. This is an appropriate choice since our observations are all 0 or 1. For the probability distributions $p({\bf z}_t|{\bf z}_{t-1})$ that control the latent dynamics, we choose (conditional) gaussian distributions with diagonal covariances. This is reasonable since we assume that the latent space is continuous. n”,
”   n”,
“n”,
” n”,
“The solid black squares represent non-linear functions parameterized by neural networks. This is what makes this a _deep_ markov model. Note that the black squares appear in two different places: in between pairs of latents and in between latents and observations. The non-linear function that connects the latent variables (‘Trans’ in Fig. 1) controls the dynamics of the latent variables. Since we allow the conditional probability distribution of ${\bf z}_{t}$ to depend on ${\bf z}_{t-1}$ in a complex way, we will be able to capture complex dynamics in our model. Similarly, the non-linear function that connects the latent variables to the observations (‘Emit’ in Fig. 1) controls how the observations depend on the latent dynamics. n”,
“n”,
“Some additional notes:n”,
“- we can freely choose the dimension of the latent space to suit the problem at hand: small latent spaces for simple problems and larger latent spaces for problems with complex dynamicsn”,
“- note the parameter ${\bf z}_0$ in Fig. 1. as will become more apparent from the code, this is just a convenient way for us to parameterize the probability distribution $p({\bf z}_1)$ for the first time step, where there are no previous latents to condition on.n”,
“n”,
“### The Gated Transition and the Emittern”,
“n”,
“Without further ado, let’s start writing some code. We first define the two PyTorch Modules that correspond to the black squares in Fig. 1. First the emission function:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;class</span> <span class="pre">Emitter(nn.Module):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">Parameterizes</span> <span class="pre">the</span> <span class="pre">bernoulli</span> <span class="pre">observation</span> <span class="pre">likelihood</span> <span class="pre">p(x_t</span> <span class="pre">|</span> <span class="pre">z_t)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">input_dim,</span> <span class="pre">z_dim,</span> <span class="pre">emission_dim):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">super().__init__()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">initialize</span> <span class="pre">the</span> <span class="pre">three</span> <span class="pre">linear</span> <span class="pre">transformations</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">neural</span> <span class="pre">network\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_z_to_hidden</span> <span class="pre">=</span> <span class="pre">nn.Linear(z_dim,</span> <span class="pre">emission_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_hidden_to_hidden</span> <span class="pre">=</span> <span class="pre">nn.Linear(emission_dim,</span> <span class="pre">emission_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_hidden_to_input</span> <span class="pre">=</span> <span class="pre">nn.Linear(emission_dim,</span> <span class="pre">input_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">initialize</span> <span class="pre">the</span> <span class="pre">two</span> <span class="pre">non-linearities</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">neural</span> <span class="pre">network\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.relu</span> <span class="pre">=</span> <span class="pre">nn.ReLU()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.sigmoid</span> <span class="pre">=</span> <span class="pre">nn.Sigmoid()\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">forward(self,</span> <span class="pre">z_t):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">Given</span> <span class="pre">the</span> <span class="pre">latent</span> <span class="pre">z</span> <span class="pre">at</span> <span class="pre">a</span> <span class="pre">particular</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">t</span> <span class="pre">we</span> <span class="pre">return</span> <span class="pre">the</span> <span class="pre">vector</span> <span class="pre">of</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">probabilities</span> <span class="pre">`ps`</span> <span class="pre">that</span> <span class="pre">parameterizes</span> <span class="pre">the</span> <span class="pre">bernoulli</span> <span class="pre">distribution</span> <span class="pre">p(x_t|z_t)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h1</span> <span class="pre">=</span> <span class="pre">self.relu(self.lin_z_to_hidden(z_t))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h2</span> <span class="pre">=</span> <span class="pre">self.relu(self.lin_hidden_to_hidden(h1))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">ps</span> <span class="pre">=</span> <span class="pre">self.sigmoid(self.lin_hidden_to_input(h2))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">ps\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“In the constructor we define the linear transformations that will be used in our emission function. Note that <cite>emission_dim</cite> is the number of hidden units in the neural network. We also define the non-linearities that we will be using. The forward call defines the computational flow of the function. We take in the latent ${\bf z}_{t}$ as input and do a sequence of transformations until we obtain a vector of length 88 that defines the emission probabilities of our bernoulli likelihood. Because of the sigmoid, each element of <cite>ps</cite> will be between 0 and 1 and will define a valid probability. Taken together the elements of <cite>ps</cite> encode which notes we expect to observe at time $t$ given the state of the system (as encoded in ${\bf z}_{t}$).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Now we define the gated transition function:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;class</span> <span class="pre">GatedTransition(nn.Module):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">Parameterizes</span> <span class="pre">the</span> <span class="pre">gaussian</span> <span class="pre">latent</span> <span class="pre">transition</span> <span class="pre">probability</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1})\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">See</span> <span class="pre">section</span> <span class="pre">5</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">reference</span> <span class="pre">for</span> <span class="pre">comparison.\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">z_dim,</span> <span class="pre">transition_dim):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">super().__init__()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">initialize</span> <span class="pre">the</span> <span class="pre">six</span> <span class="pre">linear</span> <span class="pre">transformations</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">neural</span> <span class="pre">network\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_gate_z_to_hidden</span> <span class="pre">=</span> <span class="pre">nn.Linear(z_dim,</span> <span class="pre">transition_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_gate_hidden_to_z</span> <span class="pre">=</span> <span class="pre">nn.Linear(transition_dim,</span> <span class="pre">z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_proposed_mean_z_to_hidden</span> <span class="pre">=</span> <span class="pre">nn.Linear(z_dim,</span> <span class="pre">transition_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_proposed_mean_hidden_to_z</span> <span class="pre">=</span> <span class="pre">nn.Linear(transition_dim,</span> <span class="pre">z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_sig</span> <span class="pre">=</span> <span class="pre">nn.Linear(z_dim,</span> <span class="pre">z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_z_to_loc</span> <span class="pre">=</span> <span class="pre">nn.Linear(z_dim,</span> <span class="pre">z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">modify</span> <span class="pre">the</span> <span class="pre">default</span> <span class="pre">initialization</span> <span class="pre">of</span> <span class="pre">lin_z_to_loc\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">so</span> <span class="pre">that</span> <span class="pre">it's</span> <span class="pre">starts</span> <span class="pre">out</span> <span class="pre">as</span> <span class="pre">the</span> <span class="pre">identity</span> <span class="pre">function\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_z_to_loc.weight.data</span> <span class="pre">=</span> <span class="pre">torch.eye(z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_z_to_loc.bias.data</span> <span class="pre">=</span> <span class="pre">torch.zeros(z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">initialize</span> <span class="pre">the</span> <span class="pre">three</span> <span class="pre">non-linearities</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">neural</span> <span class="pre">network\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.relu</span> <span class="pre">=</span> <span class="pre">nn.ReLU()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.sigmoid</span> <span class="pre">=</span> <span class="pre">nn.Sigmoid()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.softplus</span> <span class="pre">=</span> <span class="pre">nn.Softplus()\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">forward(self,</span> <span class="pre">z_t_1):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">Given</span> <span class="pre">the</span> <span class="pre">latent</span> <span class="pre">z_{t-1}</span> <span class="pre">corresponding</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">t-1\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">we</span> <span class="pre">return</span> <span class="pre">the</span> <span class="pre">mean</span> <span class="pre">and</span> <span class="pre">scale</span> <span class="pre">vectors</span> <span class="pre">that</span> <span class="pre">parameterize</span> <span class="pre">the\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">(diagonal)</span> <span class="pre">gaussian</span> <span class="pre">distribution</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1})\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">gating</span> <span class="pre">function\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">_gate</span> <span class="pre">=</span> <span class="pre">self.relu(self.lin_gate_z_to_hidden(z_t_1))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">gate</span> <span class="pre">=</span> <span class="pre">self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">'proposed</span> <span class="pre">mean'\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">_proposed_mean</span> <span class="pre">=</span> <span class="pre">self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">proposed_mean</span> <span class="pre">=</span> <span class="pre">self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">assemble</span> <span class="pre">the</span> <span class="pre">actual</span> <span class="pre">mean</span> <span class="pre">used</span> <span class="pre">to</span> <span class="pre">sample</span> <span class="pre">z_t,</span> <span class="pre">which</span> <span class="pre">mixes</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">a</span> <span class="pre">linear</span> <span class="pre">transformation</span> <span class="pre">of</span> <span class="pre">z_{t-1}</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">proposed</span> <span class="pre">mean</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">modulated</span> <span class="pre">by</span> <span class="pre">the</span> <span class="pre">gating</span> <span class="pre">function\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">loc</span> <span class="pre">=</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">gate)</span> <span class="pre">*</span> <span class="pre">self.lin_z_to_loc(z_t_1)</span> <span class="pre">+</span> <span class="pre">gate</span> <span class="pre">*</span> <span class="pre">proposed_mean\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">scale</span> <span class="pre">used</span> <span class="pre">to</span> <span class="pre">sample</span> <span class="pre">z_t,</span> <span class="pre">using</span> <span class="pre">the</span> <span class="pre">proposed</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">mean</span> <span class="pre">from</span> <span class="pre">above</span> <span class="pre">as</span> <span class="pre">input.</span> <span class="pre">the</span> <span class="pre">softplus</span> <span class="pre">ensures</span> <span class="pre">that</span> <span class="pre">scale</span> <span class="pre">is</span> <span class="pre">positive\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">scale</span> <span class="pre">=</span> <span class="pre">self.softplus(self.lin_sig(self.relu(proposed_mean)))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">return</span> <span class="pre">loc,</span> <span class="pre">scale</span> <span class="pre">which</span> <span class="pre">can</span> <span class="pre">be</span> <span class="pre">fed</span> <span class="pre">into</span> <span class="pre">Normal\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">loc,</span> <span class="pre">scale\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“This mirrors the structure of <cite>Emitter</cite> above, with the difference that the computational flow is a bit more complicated. This is for two reasons. First, the output of <cite>GatedTransition</cite> needs to define a valid (diagonal) gaussian distribution. So we need to output two parameters: the mean <cite>loc</cite>, and the (square root) covariance <cite>scale</cite>. These both need to have the same dimension as the latent space. Second, we don’t want to _force_ the dynamics to be non-linear. Thus our mean <cite>loc</cite> is a sum of two terms, only one of which depends non-linearily on the input <cite>z_t_1</cite>. This way we can support both linear and non-linear dynamics (or indeed have the dynamics of part of the latent space be linear, while the remainder of the dynamics is non-linear). “</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Model - a Pyro Stochastic Functionn”,
“n”,
“So far everything we’ve done is pure PyTorch. To finish translating our model into code we need to bring Pyro into the picture. Basically we need to implement the stochastic nodes (i.e. the circles) in Fig. 1. To do this we introduce a callable <cite>model()</cite> that contains the Pyro primitive <cite>pyro.sample</cite>.  The <cite>sample</cite> statements will be used to specify the joint distribution over the latents ${\bf z}_{1:T}$. Additionally, the <cite>obs</cite> argument can be used with the <cite>sample</cite> statements to specify how the observations ${\bf x}_{1:T}$ depend on the latents. Before we look at the complete code for <cite>model()</cite>, let’s look at a stripped down version that contains the main logic:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">model(...):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">z_prev</span> <span class="pre">=</span> <span class="pre">self.z_0\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">sample</span> <span class="pre">the</span> <span class="pre">latents</span> <span class="pre">z</span> <span class="pre">and</span> <span class="pre">observed</span> <span class="pre">x's</span> <span class="pre">one</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">at</span> <span class="pre">a</span> <span class="pre">time\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">t</span> <span class="pre">in</span> <span class="pre">range(1,</span> <span class="pre">T_max</span> <span class="pre">+</span> <span class="pre">1):</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">two</span> <span class="pre">lines</span> <span class="pre">of</span> <span class="pre">code</span> <span class="pre">sample</span> <span class="pre">z_t</span> <span class="pre">~</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1}).\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">first</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">parameters</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">diagonal</span> <span class="pre">gaussian</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">distribution</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1})\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_loc,</span> <span class="pre">z_scale</span> <span class="pre">=</span> <span class="pre">self.trans(z_prev)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">then</span> <span class="pre">sample</span> <span class="pre">z_t</span> <span class="pre">according</span> <span class="pre">to</span> <span class="pre">dist.Normal(z_loc,</span> <span class="pre">z_scale)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_t</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z_%d\&quot;</span> <span class="pre">%</span> <span class="pre">t,</span> <span class="pre">dist.Normal(z_loc,</span> <span class="pre">z_scale))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">probabilities</span> <span class="pre">that</span> <span class="pre">parameterize</span> <span class="pre">the</span> <span class="pre">bernoulli</span> <span class="pre">likelihood\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">emission_probs_t</span> <span class="pre">=</span> <span class="pre">self.emitter(z_t)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">statement</span> <span class="pre">instructs</span> <span class="pre">pyro</span> <span class="pre">to</span> <span class="pre">observe</span> <span class="pre">x_t</span> <span class="pre">according</span> <span class="pre">to</span> <span class="pre">the\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">bernoulli</span> <span class="pre">distribution</span> <span class="pre">p(x_t|z_t)</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;obs_x_%d\&quot;</span> <span class="pre">%</span> <span class="pre">t,</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">dist.Bernoulli(emission_probs_t),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">obs=mini_batch[:,</span> <span class="pre">t</span> <span class="pre">-</span> <span class="pre">1,</span> <span class="pre">:])\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">latent</span> <span class="pre">sampled</span> <span class="pre">at</span> <span class="pre">this</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">conditioned</span> <span class="pre">upon</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">so</span> <span class="pre">keep</span> <span class="pre">track</span> <span class="pre">of</span> <span class="pre">it\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_prev</span> <span class="pre">=</span> <span class="pre">z_t</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“The first thing we need to do is sample ${\bf z}_1$. Once we’ve sampled ${\bf z}_1$, we can sample ${\bf z}_2 \sim p({\bf z}_2|{\bf z}_1)$ and so on. This is the logic implemented in the <cite>for</cite> loop. The parameters <cite>z_loc</cite> and <cite>z_scale</cite> that define the probability distributions $p({\bf z}_t|{\bf z}_{t-1})$ are computed using <cite>self.trans</cite>, which is just an instance of the <cite>GatedTransition</cite> module defined above. For the first time step at $t=1$ we condition on <cite>self.z_0</cite>, which is a (trainable) <cite>Parameter</cite>, while for subsequent time steps we condition on the previously drawn latent. Note that each random variable <cite>z_t</cite> is assigned a unique name by the user.n”,
“n”,
“Once we’ve sampled ${\bf z}_t$ at a given time step, we need to observe the datapoint ${\bf x}_t$. So we pass <cite>z_t</cite> through <cite>self.emitter</cite>, an instance of the <cite>Emitter</cite> module defined above to obtain <cite>emission_probs_t</cite>. Together with the argument <cite>dist.Bernoulli()</cite> in the <cite>sample</cite> statement, these probabilities fully specify the observation likelihood. Finally, we also specify the slice of observed data ${\bf x}_t$:  <cite>mini_batch[:, t - 1, :]</cite> using the <cite>obs</cite> argument to <cite>sample</cite>. n”,
“n”,
“This fully specifies our model and encapsulates it in a callable that can be passed to Pyro. Before we move on let’s look at the full version of <cite>model()</cite> and go through some of the details we glossed over in our first pass.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">model(self,</span> <span class="pre">mini_batch,</span> <span class="pre">mini_batch_reversed,</span> <span class="pre">mini_batch_mask,\n&quot;,</span>
<span class="pre">&quot;\t\t</span>&#160; <span class="pre">mini_batch_seq_lengths,</span> <span class="pre">annealing_factor=1.0):\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t#</span> <span class="pre">this</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">time</span> <span class="pre">steps</span> <span class="pre">we</span> <span class="pre">need</span> <span class="pre">to</span> <span class="pre">process</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">mini-batch\n&quot;,</span>
<span class="pre">&quot;\tT_max</span> <span class="pre">=</span> <span class="pre">mini_batch.size(1)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t#</span> <span class="pre">register</span> <span class="pre">all</span> <span class="pre">PyTorch</span> <span class="pre">(sub)modules</span> <span class="pre">with</span> <span class="pre">pyro\n&quot;,</span>
<span class="pre">&quot;\t#</span> <span class="pre">this</span> <span class="pre">needs</span> <span class="pre">to</span> <span class="pre">happen</span> <span class="pre">in</span> <span class="pre">both</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">and</span> <span class="pre">guide\n&quot;,</span>
<span class="pre">&quot;\tpyro.module(\&quot;dmm\&quot;,</span> <span class="pre">self)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t#</span> <span class="pre">set</span> <span class="pre">z_prev</span> <span class="pre">=</span> <span class="pre">z_0</span> <span class="pre">to</span> <span class="pre">setup</span> <span class="pre">the</span> <span class="pre">recursive</span> <span class="pre">conditioning</span> <span class="pre">in</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1})\n&quot;,</span>
<span class="pre">&quot;\tz_prev</span> <span class="pre">=</span> <span class="pre">self.z_0.expand(mini_batch.size(0),</span> <span class="pre">self.z_0.size(0))\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t#</span> <span class="pre">we</span> <span class="pre">enclose</span> <span class="pre">all</span> <span class="pre">the</span> <span class="pre">sample</span> <span class="pre">statements</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">in</span> <span class="pre">a</span> <span class="pre">plate.\n&quot;,</span>
<span class="pre">&quot;\t#</span> <span class="pre">this</span> <span class="pre">marks</span> <span class="pre">that</span> <span class="pre">each</span> <span class="pre">datapoint</span> <span class="pre">is</span> <span class="pre">conditionally</span> <span class="pre">independent</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">others\n&quot;,</span>
<span class="pre">&quot;\twith</span> <span class="pre">pyro.plate(\&quot;z_minibatch\&quot;,</span> <span class="pre">len(mini_batch)):\n&quot;,</span>
<span class="pre">&quot;\t\t#</span> <span class="pre">sample</span> <span class="pre">the</span> <span class="pre">latents</span> <span class="pre">z</span> <span class="pre">and</span> <span class="pre">observed</span> <span class="pre">x's</span> <span class="pre">one</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">at</span> <span class="pre">a</span> <span class="pre">time\n&quot;,</span>
<span class="pre">&quot;\t\tfor</span> <span class="pre">t</span> <span class="pre">in</span> <span class="pre">range(1,</span> <span class="pre">T_max</span> <span class="pre">+</span> <span class="pre">1):\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">chunk</span> <span class="pre">of</span> <span class="pre">code</span> <span class="pre">samples</span> <span class="pre">z_t</span> <span class="pre">~</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1})\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">note</span> <span class="pre">that</span> <span class="pre">(both</span> <span class="pre">here</span> <span class="pre">and</span> <span class="pre">elsewhere)</span> <span class="pre">we</span> <span class="pre">use</span> <span class="pre">poutine.scale</span> <span class="pre">to</span> <span class="pre">take</span> <span class="pre">care\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">of</span> <span class="pre">KL</span> <span class="pre">annealing.</span> <span class="pre">we</span> <span class="pre">use</span> <span class="pre">the</span> <span class="pre">mask()</span> <span class="pre">method</span> <span class="pre">to</span> <span class="pre">deal</span> <span class="pre">with</span> <span class="pre">raggedness\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">observed</span> <span class="pre">data</span> <span class="pre">(i.e.</span> <span class="pre">different</span> <span class="pre">sequences</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">mini-batch\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">have</span> <span class="pre">different</span> <span class="pre">lengths)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">first</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">parameters</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">diagonal</span> <span class="pre">gaussian</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">distribution</span> <span class="pre">p(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1})\n&quot;,</span>
<span class="pre">&quot;\t\t\tz_loc,</span> <span class="pre">z_scale</span> <span class="pre">=</span> <span class="pre">self.trans(z_prev)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">then</span> <span class="pre">sample</span> <span class="pre">z_t</span> <span class="pre">according</span> <span class="pre">to</span> <span class="pre">dist.Normal(z_loc,</span> <span class="pre">z_scale).\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">note</span> <span class="pre">that</span> <span class="pre">we</span> <span class="pre">use</span> <span class="pre">the</span> <span class="pre">reshape</span> <span class="pre">method</span> <span class="pre">so</span> <span class="pre">that</span> <span class="pre">the</span> <span class="pre">univariate</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">Normal</span> <span class="pre">distribution</span> <span class="pre">is</span> <span class="pre">treated</span> <span class="pre">as</span> <span class="pre">a</span> <span class="pre">multivariate</span> <span class="pre">Normal</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">distribution</span> <span class="pre">with</span> <span class="pre">a</span> <span class="pre">diagonal</span> <span class="pre">covariance.\n&quot;,</span>
<span class="pre">&quot;\t\t\twith</span> <span class="pre">poutine.scale(None,</span> <span class="pre">annealing_factor):\n&quot;,</span>
<span class="pre">&quot;\t\t\t\tz_t</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z_%d\&quot;</span> <span class="pre">%</span> <span class="pre">t,\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\t\t\t</span>&#160; <span class="pre">dist.Normal(z_loc,</span> <span class="pre">z_scale)\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\t\t\t\t</span>&#160; <span class="pre">.mask(mini_batch_mask[:,</span> <span class="pre">t</span> <span class="pre">-</span> <span class="pre">1:t])\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\t\t\t\t</span>&#160; <span class="pre">.to_event(1))\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">probabilities</span> <span class="pre">that</span> <span class="pre">parameterize</span> <span class="pre">the</span> <span class="pre">bernoulli</span> <span class="pre">likelihood\n&quot;,</span>
<span class="pre">&quot;\t\t\temission_probs_t</span> <span class="pre">=</span> <span class="pre">self.emitter(z_t)\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">statement</span> <span class="pre">instructs</span> <span class="pre">pyro</span> <span class="pre">to</span> <span class="pre">observe</span> <span class="pre">x_t</span> <span class="pre">according</span> <span class="pre">to</span> <span class="pre">the\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">bernoulli</span> <span class="pre">distribution</span> <span class="pre">p(x_t|z_t)\n&quot;,</span>
<span class="pre">&quot;\t\t\tpyro.sample(\&quot;obs_x_%d\&quot;</span> <span class="pre">%</span> <span class="pre">t,\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\tdist.Bernoulli(emission_probs_t)\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\t\t.mask(mini_batch_mask[:,</span> <span class="pre">t</span> <span class="pre">-</span> <span class="pre">1:t])\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\t\t.to_event(1),\n&quot;,</span>
<span class="pre">&quot;\t\t\t\t\t\tobs=mini_batch[:,</span> <span class="pre">t</span> <span class="pre">-</span> <span class="pre">1,</span> <span class="pre">:])\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">the</span> <span class="pre">latent</span> <span class="pre">sampled</span> <span class="pre">at</span> <span class="pre">this</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">conditioned</span> <span class="pre">upon\n&quot;,</span>
<span class="pre">&quot;\t\t\t#</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">so</span> <span class="pre">keep</span> <span class="pre">track</span> <span class="pre">of</span> <span class="pre">it\n&quot;,</span>
<span class="pre">&quot;\t\t\tz_prev</span> <span class="pre">=</span> <span class="pre">z_t\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“The first thing to note is that <cite>model()</cite> takes a number of arguments. For now let’s just take a look at <cite>mini_batch</cite> and <cite>mini_batch_mask</cite>. <cite>mini_batch</cite> is a three dimensional tensor, with the first dimension being the batch dimension, the second dimension being the temporal dimension, and the final dimension being the features (88-dimensional in our case). To speed up the code, whenever we run <cite>model</cite> we’re going to process an entire mini-batch of sequences (i.e. we’re going to take advantage of vectorization). n”,
“n”,
“This is sensible because our model is implicitly defined over a single observed sequence. The probability of a set of sequences is just given by the products of the individual sequence probabilities. In other words, given the parameters of the model the sequences are conditionally independent.n”,
“n”,
“This vectorization introduces some complications because sequences can be of different lengths. This is where <cite>mini_batch_mask</cite> comes in. <cite>mini_batch_mask</cite> is a two dimensional 0/1 mask of dimensions <cite>mini_batch_size</cite> x <cite>T_max</cite>, where <cite>T_max</cite> is the maximum length of any sequence in the mini-batch. This encodes which parts of <cite>mini_batch</cite> are valid observations. n”,
“n”,
“So the first thing we do is grab <cite>T_max</cite>: we have to unroll our model for at least this many time steps. Note that this will result in a lot of ‘wasted’ computation, since some of the sequences will be shorter than <cite>T_max</cite>, but this is a small price to pay for the big speed-ups that come with vectorization. We just need to make sure that none of the ‘wasted’ computations ‘pollute’ our model computation. We accomplish this by passing the mask appropriate to time step $t$ to the <cite>mask</cite> method (which acts on the distribution that needs masking).n”,
“n”,
“Finally, the line <cite>pyro.module(&quot;dmm&quot;, self)</cite> is equivalent to a bunch of <cite>pyro.param</cite> statements for each parameter in the model. This lets Pyro know which parameters are part of the model. Just like for the <cite>sample</cite> statement, we give the module a unique name. This name will be incorporated into the name of the <cite>Parameters</cite> in the model. We leave a discussion of the KL annealing factor for later.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Inferencen”,
“n”,
“At this point we’ve fully specified our model. The next step is to set ourselves up for inference. As mentioned in the introduction, our inference strategy is going to be variational inference (see [SVI Part I](svi_part_i.ipynb) for an introduction). So our next task is to build a family of variational distributions appropriate to doing inference in a deep markov model. However, at this point it’s worth emphasizing that nothing about the way we’ve implemented <cite>model()</cite> ties us to variational inference. In principle we could use _any_ inference strategy available in Pyro. For example, in this particular context one could imagine using some variant of Sequential Monte Carlo (although this is not currently supported in Pyro).n”,
“n”,
“### Guiden”,
“n”,
“The purpose of the guide (i.e. the variational distribution) is to provide a (parameterized) approximation to the exact posterior $p({\bf z}_{1:T}|{\bf x}_{1:T})$. Actually, there’s an implicit assumption here which we should make explicit, so let’s take a step back. n”,
“Suppose our dataset $\mathcal{D}$ consists of $N$ sequences n”,
“$\{ {\bf x}_{1:T_1}^1, {\bf x}_{1:T_2}^2, …, {\bf x}_{1:T_N}^N \}$. Then the posterior we’re actually interested in is given by n”,
“$p({\bf z}_{1:T_1}^1, {\bf z}_{1:T_2}^2, …, {\bf z}_{1:T_N}^N | \mathcal{D})$, i.e. we want to infer the latents for _all_ $N$ sequences. Even for small $N$ this is a very high-dimensional distribution that will require a very large number of parameters to specify. In particular if we were to directly parameterize the posterior in this form, the number of parameters required would grow (at least) linearly with $N$. One way to avoid this nasty growth with the size of the dataset is <em>amortization</em> (see the analogous discussion in [SVI Part II](svi_part_ii.ipynb)).n”,
“n”,
“#### Aside: Amortizationn”,
“n”,
“This works as follows. Instead of introducing variational parameters for each sequence in our dataset, we’re going to learn a single parametric function $f({\bf x}_{1:T})$ and work with a variational distribution that has the form $\prod_{n=1}^N q({\bf z}_{1:T_n}^n | f({\bf x}_{1:T_n}^n))$. The function $f(\cdot)$&amp;mdash;which basically maps a given observed sequence to a set of variational parameters tailored to that sequence&amp;mdash;will need to be sufficiently rich to capture the posterior accurately, but now we can handle large datasets without having to introduce an obscene number of variational parameters.n”,
“n”,
“So our task is to construct the function $f(\cdot)$. Since in our case we need to support variable-length sequences, it’s only natural that $f(\cdot)$ have a RNN in the loop. Before we look at the various component parts that make up our $f(\cdot)$ in detail, let’s look at a computational graph that encodes the basic structure: &lt;p&gt;”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “raw”,
“metadata”: {</p>
<blockquote>
<div><p>“raw_mimetype”: “text/html”</p>
</div></blockquote>
<p>},
“source”: [</p>
<blockquote>
<div><p>“&lt;center&gt;&lt;figure&gt;&lt;img src=&quot;_static/img/guide.png&quot; style=&quot;width: 400px;&quot;&gt;&lt;figcaption&gt; &lt;font size=&quot;+1&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: The guide rolled out for T=3 time steps. &lt;/font&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/center&gt;”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“At the bottom of the figure we have our sequence of three observations. These observations will be consumed by a RNN that reads the observations from right to left and outputs three hidden states $\{ {\bf h}_1, {\bf h}_2,{\bf h}_3\}$. Note that this computation is done _before_ we sample any latent variables. Next, each of the hidden states will be fed into a <cite>Combiner</cite> module whose job is to output the mean and covariance of the the conditional distribution $q({\bf z}_t | {\bf z}_{t-1}, {\bf x}_{t:T})$, which we take to be given by a diagonal gaussian distribution. (Just like in the model, the conditional structure of ${\bf z}_{1:T}$ in the guide is such that we sample ${\bf z}_t$ forward in time.) In addition to the RNN hidden state, the <cite>Combiner</cite> also takes the latent random variable from the previous time step as input, except for $t=1$, where it instead takes the trainable (variational) parameter ${\bf z}_0^{\rm{q}}$. n”,
“n”,
“#### Aside: Guide Structuren”,
“Why do we setup the RNN to consume the observations from right to left? Why not left to right? With this choice our conditional distribution $q({\bf z}_t <a href="#id1"><span class="problematic" id="id2">|</span></a>…)$ depends on two things:n”,
“n”,
“- the latent ${\bf z}_{t-1}$ from the previous time step; and n”,
“- the observations ${\bf x}_{t:T}$, i.e. the current observation together with all future observationsn”,
“n”,
“We are free to make other choices; all that is required is that that the guide is a properly normalized distribution that plays nice with autograd. This particular choice is motivated by the dependency structure of the true posterior: see reference [1] for a detailed discussion. In brief, while we could, for example, condition on the entire sequence of observations, because of the markov structure of the model everything that we need to know about the previous observations ${\bf x}_{1:t-1}$ is encapsulated by ${\bf z}_{t-1}$. We could condition on more things, but there’s no need; and doing so will probably tend to dilute the learning signal. So running the RNN from right to left is the most natural choice for this particular model.n”,
“n”,
“Let’s look at the component parts in detail. First, the <cite>Combiner</cite> module:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;class</span> <span class="pre">Combiner(nn.Module):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">Parameterizes</span> <span class="pre">q(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1},</span> <span class="pre">x_{t:T}),</span> <span class="pre">which</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">basic</span> <span class="pre">building</span> <span class="pre">block\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">of</span> <span class="pre">the</span> <span class="pre">guide</span> <span class="pre">(i.e.</span> <span class="pre">the</span> <span class="pre">variational</span> <span class="pre">distribution).</span> <span class="pre">The</span> <span class="pre">dependence</span> <span class="pre">on</span> <span class="pre">x_{t:T}</span> <span class="pre">is\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">through</span> <span class="pre">the</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">RNN</span> <span class="pre">(see</span> <span class="pre">the</span> <span class="pre">pytorch</span> <span class="pre">module</span> <span class="pre">`rnn`</span> <span class="pre">below)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">z_dim,</span> <span class="pre">rnn_dim):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">super().__init__()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">initialize</span> <span class="pre">the</span> <span class="pre">three</span> <span class="pre">linear</span> <span class="pre">transformations</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">neural</span> <span class="pre">network\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_z_to_hidden</span> <span class="pre">=</span> <span class="pre">nn.Linear(z_dim,</span> <span class="pre">rnn_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_hidden_to_loc</span> <span class="pre">=</span> <span class="pre">nn.Linear(rnn_dim,</span> <span class="pre">z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.lin_hidden_to_scale</span> <span class="pre">=</span> <span class="pre">nn.Linear(rnn_dim,</span> <span class="pre">z_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">initialize</span> <span class="pre">the</span> <span class="pre">two</span> <span class="pre">non-linearities</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">neural</span> <span class="pre">network\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.tanh</span> <span class="pre">=</span> <span class="pre">nn.Tanh()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.softplus</span> <span class="pre">=</span> <span class="pre">nn.Softplus()\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">forward(self,</span> <span class="pre">z_t_1,</span> <span class="pre">h_rnn):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">Given</span> <span class="pre">the</span> <span class="pre">latent</span> <span class="pre">z</span> <span class="pre">at</span> <span class="pre">at</span> <span class="pre">a</span> <span class="pre">particular</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">t-1</span> <span class="pre">as</span> <span class="pre">well</span> <span class="pre">as</span> <span class="pre">the</span> <span class="pre">hidden\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">state</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">RNN</span> <span class="pre">h(x_{t:T})</span> <span class="pre">we</span> <span class="pre">return</span> <span class="pre">the</span> <span class="pre">mean</span> <span class="pre">and</span> <span class="pre">scale</span> <span class="pre">vectors</span> <span class="pre">that\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">parameterize</span> <span class="pre">the</span> <span class="pre">(diagonal)</span> <span class="pre">gaussian</span> <span class="pre">distribution</span> <span class="pre">q(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1},</span> <span class="pre">x_{t:T})\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">combine</span> <span class="pre">the</span> <span class="pre">rnn</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">with</span> <span class="pre">a</span> <span class="pre">transformed</span> <span class="pre">version</span> <span class="pre">of</span> <span class="pre">z_t_1\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h_combined</span> <span class="pre">=</span> <span class="pre">0.5</span> <span class="pre">*</span> <span class="pre">(self.tanh(self.lin_z_to_hidden(z_t_1))</span> <span class="pre">+</span> <span class="pre">h_rnn)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">use</span> <span class="pre">the</span> <span class="pre">combined</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">to</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">mean</span> <span class="pre">used</span> <span class="pre">to</span> <span class="pre">sample</span> <span class="pre">z_t\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">loc</span> <span class="pre">=</span> <span class="pre">self.lin_hidden_to_loc(h_combined)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">use</span> <span class="pre">the</span> <span class="pre">combined</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">to</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">scale</span> <span class="pre">used</span> <span class="pre">to</span> <span class="pre">sample</span> <span class="pre">z_t\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">scale</span> <span class="pre">=</span> <span class="pre">self.softplus(self.lin_hidden_to_scale(h_combined))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">return</span> <span class="pre">loc,</span> <span class="pre">scale</span> <span class="pre">which</span> <span class="pre">can</span> <span class="pre">be</span> <span class="pre">fed</span> <span class="pre">into</span> <span class="pre">Normal\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">loc,</span> <span class="pre">scale\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“This module has the same general structure as <cite>Emitter</cite> and <cite>GatedTransition</cite> in the model. The only thing of note is that because the <cite>Combiner</cite> needs to consume two inputs at each time step, it transforms the inputs into a single combined hidden state <cite>h_combined</cite> before it computes the outputs. n”,
“n”,
“Apart from the RNN, we now have all the ingredients we need to construct our guide distribution.n”,
“Happily, PyTorch has great built-in RNN modules, so we don’t have much work to do here. We’ll see where we instantiate the RNN later. Let’s instead jump right into the definition of the stochastic function <cite>guide()</cite>.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">guide(self,</span> <span class="pre">mini_batch,</span> <span class="pre">mini_batch_reversed,</span> <span class="pre">mini_batch_mask,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">mini_batch_seq_lengths,</span> <span class="pre">annealing_factor=1.0):\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">this</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">time</span> <span class="pre">steps</span> <span class="pre">we</span> <span class="pre">need</span> <span class="pre">to</span> <span class="pre">process</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">mini-batch\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">T_max</span> <span class="pre">=</span> <span class="pre">mini_batch.size(1)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">register</span> <span class="pre">all</span> <span class="pre">PyTorch</span> <span class="pre">(sub)modules</span> <span class="pre">with</span> <span class="pre">pyro\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.module(\&quot;dmm\&quot;,</span> <span class="pre">self)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">if</span> <span class="pre">on</span> <span class="pre">gpu</span> <span class="pre">we</span> <span class="pre">need</span> <span class="pre">the</span> <span class="pre">fully</span> <span class="pre">broadcast</span> <span class="pre">view</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">rnn</span> <span class="pre">initial</span> <span class="pre">state\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">to</span> <span class="pre">be</span> <span class="pre">in</span> <span class="pre">contiguous</span> <span class="pre">gpu</span> <span class="pre">memory\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">h_0_contig</span> <span class="pre">=</span> <span class="pre">self.h_0.expand(1,</span> <span class="pre">mini_batch.size(0),</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.rnn.hidden_size).contiguous()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">push</span> <span class="pre">the</span> <span class="pre">observed</span> <span class="pre">x's</span> <span class="pre">through</span> <span class="pre">the</span> <span class="pre">rnn;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">rnn_output</span> <span class="pre">contains</span> <span class="pre">the</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">at</span> <span class="pre">each</span> <span class="pre">time</span> <span class="pre">step\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">rnn_output,</span> <span class="pre">_</span> <span class="pre">=</span> <span class="pre">self.rnn(mini_batch_reversed,</span> <span class="pre">h_0_contig)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">reverse</span> <span class="pre">the</span> <span class="pre">time-ordering</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">and</span> <span class="pre">un-pack</span> <span class="pre">it\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">rnn_output</span> <span class="pre">=</span> <span class="pre">poly.pad_and_reverse(rnn_output,</span> <span class="pre">mini_batch_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">set</span> <span class="pre">z_prev</span> <span class="pre">=</span> <span class="pre">z_q_0</span> <span class="pre">to</span> <span class="pre">setup</span> <span class="pre">the</span> <span class="pre">recursive</span> <span class="pre">conditioning</span> <span class="pre">in</span> <span class="pre">q(z_t</span> <span class="pre">|...)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">z_prev</span> <span class="pre">=</span> <span class="pre">self.z_q_0.expand(mini_batch.size(0),</span> <span class="pre">self.z_q_0.size(0))\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">we</span> <span class="pre">enclose</span> <span class="pre">all</span> <span class="pre">the</span> <span class="pre">sample</span> <span class="pre">statements</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">guide</span> <span class="pre">in</span> <span class="pre">a</span> <span class="pre">plate.\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">this</span> <span class="pre">marks</span> <span class="pre">that</span> <span class="pre">each</span> <span class="pre">datapoint</span> <span class="pre">is</span> <span class="pre">conditionally</span> <span class="pre">independent</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">others.\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">pyro.plate(\&quot;z_minibatch\&quot;,</span> <span class="pre">len(mini_batch)):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">sample</span> <span class="pre">the</span> <span class="pre">latents</span> <span class="pre">z</span> <span class="pre">one</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">at</span> <span class="pre">a</span> <span class="pre">time\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">t</span> <span class="pre">in</span> <span class="pre">range(1,</span> <span class="pre">T_max</span> <span class="pre">+</span> <span class="pre">1):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">two</span> <span class="pre">lines</span> <span class="pre">assemble</span> <span class="pre">the</span> <span class="pre">distribution</span> <span class="pre">q(z_t</span> <span class="pre">|</span> <span class="pre">z_{t-1},</span> <span class="pre">x_{t:T})\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_loc,</span> <span class="pre">z_scale</span> <span class="pre">=</span> <span class="pre">self.combiner(z_prev,</span> <span class="pre">rnn_output[:,</span> <span class="pre">t</span> <span class="pre">-</span> <span class="pre">1,</span> <span class="pre">:])\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_dist</span> <span class="pre">=</span> <span class="pre">dist.Normal(z_loc,</span> <span class="pre">z_scale)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">sample</span> <span class="pre">z_t</span> <span class="pre">from</span> <span class="pre">the</span> <span class="pre">distribution</span> <span class="pre">z_dist\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">pyro.poutine.scale(None,</span> <span class="pre">annealing_factor):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_t</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z_%d\&quot;</span> <span class="pre">%</span> <span class="pre">t,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_dist.mask(mini_batch_mask[:,</span> <span class="pre">t</span> <span class="pre">-</span> <span class="pre">1:t])\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">.to_event(1))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">latent</span> <span class="pre">sampled</span> <span class="pre">at</span> <span class="pre">this</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">conditioned</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">upon</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">next</span> <span class="pre">time</span> <span class="pre">step</span> <span class="pre">so</span> <span class="pre">keep</span> <span class="pre">track</span> <span class="pre">of</span> <span class="pre">it\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z_prev</span> <span class="pre">=</span> <span class="pre">z_t\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“The high-level structure of <cite>guide()</cite> is very similar to <cite>model()</cite>. First note that the model and guide take the same arguments: this is a general requirement for model/guide pairs in Pyro. As in the model, there’s a call to <cite>pyro.module</cite> that registers all the parameters with Pyro. Also, the <cite>for</cite> loop has the same structure as the one in <cite>model()</cite>, with the difference that the guide only needs to sample latents (there are no <cite>sample</cite> statements with the <cite>obs</cite> keyword). Finally, note that the names of the  latent variables in the guide exactly match those in the model. This is how Pyro knows to correctly align random variables. n”,
“n”,
“The RNN logic should be familar to PyTorch users, but let’s go through it quickly. First we prepare the initial state of the RNN, <cite>h_0</cite>. Then we invoke the RNN via its forward call; the resulting tensor <cite>rnn_output</cite> contains the hidden states for the entire mini-batch. Note that because we want the RNN to consume the observations from right to left, the input to the RNN is <cite>mini_batch_reversed</cite>, which is a copy of <cite>mini_batch</cite> with all the sequences running in _reverse_ temporal order. Furthermore, <cite>mini_batch_reversed</cite> has been wrapped in a PyTorch <cite>rnn.pack_padded_sequence</cite> so that the RNN can deal with variable-length sequences. Since we do our sampling in latent space in normal temporal order, we use the helper function <cite>pad_and_reverse</cite> to reverse the hidden state sequences in <cite>rnn_output</cite>, so that we can feed the <cite>Combiner</cite> RNN hidden states that are correctly aligned and ordered. This helper function also unpacks the <cite>rnn_output</cite> so that it is no longer in the form of a PyTorch <cite>rnn.pack_padded_sequence</cite>.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Packaging the Model and Guide as a PyTorch Modulen”,
“n”,
“At this juncture, we’re ready to proceed to inference. But before we do so let’s quickly go over how we packaged the model and guide as a single PyTorch Module. This is generally good practice, especially for larger models.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;class</span> <span class="pre">DMM(nn.Module):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">This</span> <span class="pre">PyTorch</span> <span class="pre">Module</span> <span class="pre">encapsulates</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">as</span> <span class="pre">well</span> <span class="pre">as</span> <span class="pre">the</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">variational</span> <span class="pre">distribution</span> <span class="pre">(the</span> <span class="pre">guide)</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">Deep</span> <span class="pre">Markov</span> <span class="pre">Model\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">\&quot;\&quot;\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">input_dim=88,</span> <span class="pre">z_dim=100,</span> <span class="pre">emission_dim=100,</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">transition_dim=200,</span> <span class="pre">rnn_dim=600,</span> <span class="pre">rnn_dropout_rate=0.0,</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">num_iafs=0,</span> <span class="pre">iaf_dim=50,</span> <span class="pre">use_cuda=False):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">super().__init__()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">instantiate</span> <span class="pre">pytorch</span> <span class="pre">modules</span> <span class="pre">used</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">and</span> <span class="pre">guide</span> <span class="pre">below\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.emitter</span> <span class="pre">=</span> <span class="pre">Emitter(input_dim,</span> <span class="pre">z_dim,</span> <span class="pre">emission_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.trans</span> <span class="pre">=</span> <span class="pre">GatedTransition(z_dim,</span> <span class="pre">transition_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.combiner</span> <span class="pre">=</span> <span class="pre">Combiner(z_dim,</span> <span class="pre">rnn_dim)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.rnn</span> <span class="pre">=</span> <span class="pre">nn.RNN(input_size=input_dim,</span> <span class="pre">hidden_size=rnn_dim,</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">nonlinearity='relu',</span> <span class="pre">batch_first=True,</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">bidirectional=False,</span> <span class="pre">num_layers=1,</span> <span class="pre">dropout=rnn_dropout_rate)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">define</span> <span class="pre">a</span> <span class="pre">(trainable)</span> <span class="pre">parameters</span> <span class="pre">z_0</span> <span class="pre">and</span> <span class="pre">z_q_0</span> <span class="pre">that</span> <span class="pre">help</span> <span class="pre">define</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">probability</span> <span class="pre">distributions</span> <span class="pre">p(z_1)</span> <span class="pre">and</span> <span class="pre">q(z_1)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">(since</span> <span class="pre">for</span> <span class="pre">t</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">there</span> <span class="pre">are</span> <span class="pre">no</span> <span class="pre">previous</span> <span class="pre">latents</span> <span class="pre">to</span> <span class="pre">condition</span> <span class="pre">on)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.z_0</span> <span class="pre">=</span> <span class="pre">nn.Parameter(torch.zeros(z_dim))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.z_q_0</span> <span class="pre">=</span> <span class="pre">nn.Parameter(torch.zeros(z_dim))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">define</span> <span class="pre">a</span> <span class="pre">(trainable)</span> <span class="pre">parameter</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">initial</span> <span class="pre">hidden</span> <span class="pre">state</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">rnn\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.h_0</span> <span class="pre">=</span> <span class="pre">nn.Parameter(torch.zeros(1,</span> <span class="pre">1,</span> <span class="pre">rnn_dim))\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.use_cuda</span> <span class="pre">=</span> <span class="pre">use_cuda\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">if</span> <span class="pre">on</span> <span class="pre">gpu</span> <span class="pre">cuda-ize</span> <span class="pre">all</span> <span class="pre">pytorch</span> <span class="pre">(sub)modules\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">if</span> <span class="pre">use_cuda:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">self.cuda()\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">p(x_{1:T}</span> <span class="pre">|</span> <span class="pre">z_{1:T})</span> <span class="pre">p(z_{1:T})\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">model(...):\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">...</span> <span class="pre">as</span> <span class="pre">above</span> <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">the</span> <span class="pre">guide</span> <span class="pre">q(z_{1:T}</span> <span class="pre">|</span> <span class="pre">x_{1:T})</span> <span class="pre">(i.e.</span> <span class="pre">the</span> <span class="pre">variational</span> <span class="pre">distribution)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">def</span> <span class="pre">guide(...):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">...</span> <span class="pre">as</span> <span class="pre">above</span> <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Since we’ve already gone over <cite>model</cite> and <cite>guide</cite>, our focus here is on the constructor. First we instantiate the four PyTorch modules that we use in our model and guide. On the model-side: <cite>Emitter</cite> and <cite>GatedTransition</cite>. On the guide-side: <cite>Combiner</cite> and the RNN. n”,
“n”,
“Next we define PyTorch <cite>Parameter`s for the initial state of the RNN as well as `z_0</cite> and <cite>z_q_0</cite>, which are fed into <cite>self.trans</cite> and <cite>self.combiner</cite>, respectively, in lieu of the non-existent random variable $\bf z_0$. n”,
“n”,
“The important point to make here is that all of these <cite>Module`s and `Parameter`s are attributes of `DMM</cite> (which itself inherits from <cite>nn.Module</cite>). This has the consequence they are all automatically registered as belonging to the module. So, for example, when we call <cite>parameters()</cite> on an instance of <cite>DMM</cite>, PyTorch will know to return all the relevant parameters. It also means that when we invoke <cite>pyro.module(&quot;dmm&quot;, self)</cite> in <cite>model()</cite> and <cite>guide()</cite>, all the parameters of both the model and guide will be registered with Pyro. Finally, it means that if we’re running on a GPU, the call to <cite>cuda()</cite> will move all the parameters into GPU memory.n”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Stochastic Variational Inferencen”,
“n”,
“With our model and guide at hand, we’re finally ready to do inference. Before we look at the full logic that is involved in a complete experimental script, let’s first see how to take a single gradient step. First we instantiate an instance of <cite>DMM</cite> and setup an optimizer.n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">instantiate</span> <span class="pre">the</span> <span class="pre">dmm\n&quot;,</span>
<span class="pre">&quot;dmm</span> <span class="pre">=</span> <span class="pre">DMM(input_dim,</span> <span class="pre">z_dim,</span> <span class="pre">emission_dim,</span> <span class="pre">transition_dim,</span> <span class="pre">rnn_dim,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">args.rnn_dropout_rate,</span> <span class="pre">args.num_iafs,</span> <span class="pre">args.iaf_dim,</span> <span class="pre">args.cuda)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">setup</span> <span class="pre">optimizer\n&quot;,</span>
<span class="pre">&quot;adam_params</span> <span class="pre">=</span> <span class="pre">{\&quot;lr\&quot;:</span> <span class="pre">args.learning_rate,</span> <span class="pre">\&quot;betas\&quot;:</span> <span class="pre">(args.beta1,</span> <span class="pre">args.beta2),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;clip_norm\&quot;:</span> <span class="pre">args.clip_norm,</span> <span class="pre">\&quot;lrd\&quot;:</span> <span class="pre">args.lr_decay,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;weight_decay\&quot;:</span> <span class="pre">args.weight_decay}\n&quot;,</span>
<span class="pre">&quot;optimizer</span> <span class="pre">=</span> <span class="pre">ClippedAdam(adam_params)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“Here we’re using an implementation of the Adam optimizer that includes gradient clipping. This mitigates some of the problems that can occur when training recurrent neural networks (e.g. vanishing/exploding gradients). Next we setup the inference algorithm. n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">setup</span> <span class="pre">inference</span> <span class="pre">algorithm\n&quot;,</span>
<span class="pre">&quot;svi</span> <span class="pre">=</span> <span class="pre">SVI(dmm.model,</span> <span class="pre">dmm.guide,</span> <span class="pre">optimizer,</span> <span class="pre">Trace_ELBO())\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“The inference algorithm <cite>SVI</cite> uses a stochastic gradient estimator to take gradient steps on an objective function, which in this case is given by the ELBO (the evidence lower bound). As the name indicates, the ELBO is a lower bound to the log evidence: $\log p(\mathcal{D})$. As we take gradient steps that maximize the ELBO, we move our guide $q(\cdot)$ closer to the exact posterior. n”,
“n”,
“The argument <cite>Trace_ELBO()</cite> constructs a version of the gradient estimator that doesn’t need access to the dependency structure of the model and guide. Since all the latent variables in our model are reparameterizable, this is the appropriate gradient estimator for our use case. (It’s also the default option.)n”,
“n”,
“Assuming we’ve prepared the various arguments of <cite>dmm.model</cite> and <cite>dmm.guide</cite>, taking a gradient step is accomplished by callingn”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;svi.step(mini_batch,</span> <span class="pre">...)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“That’s all there is to it!n”,
“n”,
“Well, not quite. This will be the main step in our inference algorithm, but we still need to implement a complete training loop with preparation of mini-batches, evaluation, and so on. This sort of logic will be familiar to any deep learner but let’s see how it looks in PyTorch/Pyro.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## The Black Magic of Optimizationn”,
“n”,
“Actually, before we get to the guts of training, let’s take a moment and think a bit about the optimization problem we’ve setup. We’ve traded Bayesian inference in a non-linear model with a high-dimensional latent space&amp;mdash;a hard problem&amp;mdash;for a particular optimization problem. Let’s not kid ourselves, this optimization problem is pretty hard too. Why? Let’s go through some of the reasons:n”,
“n”,
“- the space of parameters we’re optimizing over is very high-dimensional (it includes all the weights in all the neural networks we’ve defined).n”,
“- our objective function (the ELBO) cannot be computed analytically. so our parameter updates will be following noisy Monte Carlo gradient estimatesn”,
“- data-subsampling serves as an additional source of stochasticity: even if we wanted to, we couldn’t in general take gradient steps on the ELBO defined over the whole dataset (actually in our particular case the dataset isn’t so large, but let’s ignore that).n”,
“- given all the neural networks and non-linearities we have in the loop, our (stochastic) loss surface is highly non-trivialn”,
“n”,
“The upshot is that if we’re going to find reasonable (local) optima of the ELBO, we better take some care in deciding how to do optimization. This isn’t the time or place to discuss all the different strategies that one might adopt, but it’s important to emphasize how decisive a good or bad choice in learning hyperparameters (the learning rate, the mini-batch size, etc.) can be. n”,
“n”,
“Before we move on, let’s discuss one particular optimization strategy that we’re making use of in greater detail: KL annealing. In our case the ELBO is the sum of two terms: an expected log likelihood term (which measures model fit) and a sum of KL divergence terms (which serve to regularize the approximate posterior):n”,
“n”,
“$\rm{ELBO} = \mathbb{E}_{q({\bf z}_{1:T})}[\log p({\bf x}_{1:T}|{\bf z}_{1:T})] -  \mathbb{E}_{q({\bf z}_{1:T})}[ \log q({\bf z}_{1:T}) - \log p({\bf z}_{1:T})]$n”,
“n”,
“This latter term can be a quite strong regularizer, and in early stages of training it has a tendency to favor regions of the loss surface that contain lots of bad local optima. One strategy to avoid these bad local optima, which was also adopted in reference [1], is to anneal the KL divergence terms by multiplying them by a scalar <cite>annealing_factor</cite> that ranges between zero and one:n”,
“n”,
“$\mathbb{E}_{q({\bf z}_{1:T})}[\log p({\bf x}_{1:T}|{\bf z}_{1:T})] -  \rm{annealing\_factor} \times \mathbb{E}_{q({\bf z}_{1:T})}[ \log q({\bf z}_{1:T}) - \log p({\bf z}_{1:T})]$n”,
“n”,
“The idea is that during the course of training the  <cite>annealing_factor</cite> rises slowly from its initial value at/near zero to its final value at 1.0. The annealing schedule is arbitrary; below we will use a simple linear schedule. In terms of code, to scale the log likelihoods by the appropriate annealing factor we enclose each of the latent sample statements in the model and guide with a <cite>pyro.poutine.scale</cite> context.n”,
“n”,
“Finally, we should mention that the main difference between the DMM implementation described here and the one used in reference [1] is that they take advantage of the analytic formula for the KL divergence between two gaussian distributions (whereas we rely on Monte Carlo estimates). This leads to lower variance gradient estimates of the ELBO, which makes training a bit easier. We can still train the model without making this analytic substitution, but training probably takes somewhat longer because of the higher variance. To use analytic KL divergences use [TraceMeanField_ELBO](<a class="reference external" href="http://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.trace_mean_field_elbo.TraceMeanField_ELBO">http://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.trace_mean_field_elbo.TraceMeanField_ELBO</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Data Loading, Training, and Evaluationn”,
“n”,
“First we load the data. There are 229 sequences in the training dataset, each with an average length of ~60 time steps.n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;jsb_file_loc</span> <span class="pre">=</span> <span class="pre">\&quot;./data/jsb_processed.pkl\&quot;\n&quot;,</span>
<span class="pre">&quot;data</span> <span class="pre">=</span> <span class="pre">pickle.load(open(jsb_file_loc,</span> <span class="pre">\&quot;rb\&quot;))\n&quot;,</span>
<span class="pre">&quot;training_seq_lengths</span> <span class="pre">=</span> <span class="pre">data['train']['sequence_lengths']\n&quot;,</span>
<span class="pre">&quot;training_data_sequences</span> <span class="pre">=</span> <span class="pre">data['train']['sequences']\n&quot;,</span>
<span class="pre">&quot;test_seq_lengths</span> <span class="pre">=</span> <span class="pre">data['test']['sequence_lengths']\n&quot;,</span>
<span class="pre">&quot;test_data_sequences</span> <span class="pre">=</span> <span class="pre">data['test']['sequences']\n&quot;,</span>
<span class="pre">&quot;val_seq_lengths</span> <span class="pre">=</span> <span class="pre">data['valid']['sequence_lengths']\n&quot;,</span>
<span class="pre">&quot;val_data_sequences</span> <span class="pre">=</span> <span class="pre">data['valid']['sequences']\n&quot;,</span>
<span class="pre">&quot;N_train_data</span> <span class="pre">=</span> <span class="pre">len(training_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;N_train_time_slices</span> <span class="pre">=</span> <span class="pre">np.sum(training_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;N_mini_batches</span> <span class="pre">=</span> <span class="pre">int(N_train_data</span> <span class="pre">/</span> <span class="pre">args.mini_batch_size</span> <span class="pre">+\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">int(N_train_data</span> <span class="pre">%</span> <span class="pre">args.mini_batch_size</span> <span class="pre">&gt;</span> <span class="pre">0))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“For this dataset we will typically use a <cite>mini_batch_size</cite> of 20, so that there will be 12 mini-batches per epoch. Next we define the function <cite>process_minibatch</cite> which prepares a mini-batch for training and takes a gradient step:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">process_minibatch(epoch,</span> <span class="pre">which_mini_batch,</span> <span class="pre">shuffled_indices):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">if</span> <span class="pre">args.annealing_epochs</span> <span class="pre">&gt;</span> <span class="pre">0</span> <span class="pre">and</span> <span class="pre">epoch</span> <span class="pre">&lt;</span> <span class="pre">args.annealing_epochs:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">KL</span> <span class="pre">annealing</span> <span class="pre">factor</span> <span class="pre">appropriate</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">current</span> <span class="pre">mini-batch</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">current</span> <span class="pre">epoch\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">min_af</span> <span class="pre">=</span> <span class="pre">args.minimum_annealing_factor\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">annealing_factor</span> <span class="pre">=</span> <span class="pre">min_af</span> <span class="pre">+</span> <span class="pre">(1.0</span> <span class="pre">-</span> <span class="pre">min_af)</span> <span class="pre">*</span> <span class="pre">\\</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">(float(which_mini_batch</span> <span class="pre">+</span> <span class="pre">epoch</span> <span class="pre">*</span> <span class="pre">N_mini_batches</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">/\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">float(args.annealing_epochs</span> <span class="pre">*</span> <span class="pre">N_mini_batches))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">else:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">by</span> <span class="pre">default</span> <span class="pre">the</span> <span class="pre">KL</span> <span class="pre">annealing</span> <span class="pre">factor</span> <span class="pre">is</span> <span class="pre">unity\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">annealing_factor</span> <span class="pre">=</span> <span class="pre">1.0</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">which</span> <span class="pre">sequences</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">training</span> <span class="pre">set</span> <span class="pre">we</span> <span class="pre">should</span> <span class="pre">grab\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">mini_batch_start</span> <span class="pre">=</span> <span class="pre">(which_mini_batch</span> <span class="pre">*</span> <span class="pre">args.mini_batch_size)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">mini_batch_end</span> <span class="pre">=</span> <span class="pre">np.min([(which_mini_batch</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">*</span> <span class="pre">args.mini_batch_size,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">N_train_data])\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">mini_batch_indices</span> <span class="pre">=</span> <span class="pre">shuffled_indices[mini_batch_start:mini_batch_end]\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">grab</span> <span class="pre">the</span> <span class="pre">fully</span> <span class="pre">prepped</span> <span class="pre">mini-batch</span> <span class="pre">using</span> <span class="pre">the</span> <span class="pre">helper</span> <span class="pre">function</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">data</span> <span class="pre">loader\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">mini_batch,</span> <span class="pre">mini_batch_reversed,</span> <span class="pre">mini_batch_mask,</span> <span class="pre">mini_batch_seq_lengths</span> <span class="pre">\\\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">=</span> <span class="pre">poly.get_mini_batch(mini_batch_indices,</span> <span class="pre">training_data_sequences,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">training_seq_lengths,</span> <span class="pre">cuda=args.cuda)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">do</span> <span class="pre">an</span> <span class="pre">actual</span> <span class="pre">gradient</span> <span class="pre">step\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">loss</span> <span class="pre">=</span> <span class="pre">svi.step(mini_batch,</span> <span class="pre">mini_batch_reversed,</span> <span class="pre">mini_batch_mask,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">mini_batch_seq_lengths,</span> <span class="pre">annealing_factor)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">keep</span> <span class="pre">track</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">training</span> <span class="pre">loss\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">loss\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“We first compute the KL annealing factor appropriate to the mini-batch (according to a linear schedule as described earlier). We then compute the mini-batch indices, which we pass to the helper function <cite>get_mini_batch</cite>. This helper function takes care of a number of different things:n”,
“n”,
“- it sorts each mini-batch by sequence lengthn”,
“- it calls another helper function to get a copy of the mini-batch in reversed temporal ordern”,
“- it packs each reversed mini-batch in a <cite>rnn.pack_padded_sequence</cite>, which is then ready to be ingested by the RNNn”,
“- it cuda-izes all tensors if we’re on a GPUn”,
“- it calls another helper function to get an appropriate 0/1 mask for the mini-batchn”,
“n”,
“We then pipe all the return values of <cite>get_mini_batch()</cite> into <cite>elbo.step(…)</cite>. Recall that these arguments will be further piped to <cite>model(…)</cite> and <cite>guide(…)</cite> during construction of the gradient estimator in <cite>elbo</cite>. Finally, we return a float which is a noisy estimate of the loss for that mini-batch.n”,
“n”,
“We now have all the ingredients required for the main bit of our training loop:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;times</span> <span class="pre">=</span> <span class="pre">[time.time()]\n&quot;,</span>
<span class="pre">&quot;for</span> <span class="pre">epoch</span> <span class="pre">in</span> <span class="pre">range(args.num_epochs):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">accumulator</span> <span class="pre">for</span> <span class="pre">our</span> <span class="pre">estimate</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">negative</span> <span class="pre">log</span> <span class="pre">likelihood</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">(or</span> <span class="pre">rather</span> <span class="pre">-elbo)</span> <span class="pre">for</span> <span class="pre">this</span> <span class="pre">epoch\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">epoch_nll</span> <span class="pre">=</span> <span class="pre">0.0</span> <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">prepare</span> <span class="pre">mini-batch</span> <span class="pre">subsampling</span> <span class="pre">indices</span> <span class="pre">for</span> <span class="pre">this</span> <span class="pre">epoch\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">shuffled_indices</span> <span class="pre">=</span> <span class="pre">np.arange(N_train_data)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">np.random.shuffle(shuffled_indices)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">process</span> <span class="pre">each</span> <span class="pre">mini-batch;</span> <span class="pre">this</span> <span class="pre">is</span> <span class="pre">where</span> <span class="pre">we</span> <span class="pre">take</span> <span class="pre">gradient</span> <span class="pre">steps\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">which_mini_batch</span> <span class="pre">in</span> <span class="pre">range(N_mini_batches):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">epoch_nll</span> <span class="pre">+=</span> <span class="pre">process_minibatch(epoch,</span> <span class="pre">which_mini_batch,</span> <span class="pre">shuffled_indices)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">report</span> <span class="pre">training</span> <span class="pre">diagnostics\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">times.append(time.time())\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">epoch_time</span> <span class="pre">=</span> <span class="pre">times[-1]</span> <span class="pre">-</span> <span class="pre">times[-2]\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;[training</span> <span class="pre">epoch</span> <span class="pre">%04d]</span>&#160; <span class="pre">%.4f</span> <span class="pre">\\t\\t\\t\\t(dt</span> <span class="pre">=</span> <span class="pre">%.3f</span> <span class="pre">sec)\&quot;</span> <span class="pre">%\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">(epoch,</span> <span class="pre">epoch_nll</span> <span class="pre">/</span> <span class="pre">N_train_time_slices,</span> <span class="pre">epoch_time))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“At the beginning of each epoch we shuffle the indices pointing to the training data. We then process each mini-batch until we’ve gone through the entire training set, accumulating the training loss as we go. Finally we report some diagnostic info. Note that we normalize the loss by the total number of time slices in the training set (this allows us to compare to reference [1]). “</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Evaluationn”,
“This training loop is still missing any kind of evaluation diagnostics. Let’s fix that. First we need to prepare the validation and test data for evaluation. Since the validation and test datasets are small enough that we can easily fit them into memory, we’re going to process each dataset batchwise (i.e. we will not be breaking up the dataset into mini-batches). [_Aside: at this point the reader may ask why we don’t do the same thing for the training set. The reason is that additional stochasticity due to data-subsampling is often advantageous during optimization: in particular it can help us avoid local optima._]  And, in fact, in order to get a lessy noisy estimate of the ELBO, we’re going to compute a multi-sample estimate. The simplest way to do this would be as follows:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;val_loss</span> <span class="pre">=</span> <span class="pre">svi.evaluate_loss(val_batch,</span> <span class="pre">...,</span> <span class="pre">num_particles=5)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“This, however, would involve an explicit <cite>for</cite> loop with five iterations. For our particular model, we can do better and vectorize the whole computation. The only way to do this currently in Pyro is to explicitly replicate the data <cite>n_eval_samples</cite> many times. This is the strategy we follow:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">package</span> <span class="pre">repeated</span> <span class="pre">copies</span> <span class="pre">of</span> <span class="pre">val/test</span> <span class="pre">data</span> <span class="pre">for</span> <span class="pre">faster</span> <span class="pre">evaluation\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">(i.e.</span> <span class="pre">set</span> <span class="pre">us</span> <span class="pre">up</span> <span class="pre">for</span> <span class="pre">vectorization)\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">rep(x):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">np.repeat(x,</span> <span class="pre">n_eval_samples,</span> <span class="pre">axis=0)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">get</span> <span class="pre">the</span> <span class="pre">validation/test</span> <span class="pre">data</span> <span class="pre">ready</span> <span class="pre">for</span> <span class="pre">the</span> <span class="pre">dmm:</span> <span class="pre">pack</span> <span class="pre">into</span> <span class="pre">sequences,</span> <span class="pre">etc.\n&quot;,</span>
<span class="pre">&quot;val_seq_lengths</span> <span class="pre">=</span> <span class="pre">rep(val_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;test_seq_lengths</span> <span class="pre">=</span> <span class="pre">rep(test_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;val_batch,</span> <span class="pre">val_batch_reversed,</span> <span class="pre">val_batch_mask,</span> <span class="pre">val_seq_lengths</span> <span class="pre">=</span> <span class="pre">poly.get_mini_batch(\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">np.arange(n_eval_samples</span> <span class="pre">*</span> <span class="pre">val_data_sequences.shape[0]),</span> <span class="pre">rep(val_data_sequences),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">val_seq_lengths,</span> <span class="pre">cuda=args.cuda)\n&quot;,</span>
<span class="pre">&quot;test_batch,</span> <span class="pre">test_batch_reversed,</span> <span class="pre">test_batch_mask,</span> <span class="pre">test_seq_lengths</span> <span class="pre">=</span> <span class="pre">\\\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">poly.get_mini_batch(np.arange(n_eval_samples</span> <span class="pre">*</span> <span class="pre">test_data_sequences.shape[0]),</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">rep(test_data_sequences),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">test_seq_lengths,</span> <span class="pre">cuda=args.cuda)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“With the test and validation data now fully prepped, we define the helper function that does the evaluation: n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">do_evaluation():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">put</span> <span class="pre">the</span> <span class="pre">RNN</span> <span class="pre">into</span> <span class="pre">evaluation</span> <span class="pre">mode</span> <span class="pre">(i.e.</span> <span class="pre">turn</span> <span class="pre">off</span> <span class="pre">drop-out</span> <span class="pre">if</span> <span class="pre">applicable)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">dmm.rnn.eval()\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">compute</span> <span class="pre">the</span> <span class="pre">validation</span> <span class="pre">and</span> <span class="pre">test</span> <span class="pre">loss\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">val_nll</span> <span class="pre">=</span> <span class="pre">svi.evaluate_loss(val_batch,</span> <span class="pre">val_batch_reversed,</span> <span class="pre">val_batch_mask,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">val_seq_lengths)</span> <span class="pre">/</span> <span class="pre">np.sum(val_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">test_nll</span> <span class="pre">=</span> <span class="pre">svi.evaluate_loss(test_batch,</span> <span class="pre">test_batch_reversed,</span> <span class="pre">test_batch_mask,\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">test_seq_lengths)</span> <span class="pre">/</span> <span class="pre">np.sum(test_seq_lengths)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">put</span> <span class="pre">the</span> <span class="pre">RNN</span> <span class="pre">back</span> <span class="pre">into</span> <span class="pre">training</span> <span class="pre">mode</span> <span class="pre">(i.e.</span> <span class="pre">turn</span> <span class="pre">on</span> <span class="pre">drop-out</span> <span class="pre">if</span> <span class="pre">applicable)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">dmm.rnn.train()\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">return</span> <span class="pre">val_nll,</span> <span class="pre">test_nll\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“We simply call the <cite>evaluate_loss</cite> method of <cite>elbo</cite>, which takes the same arguments as <cite>step()</cite>, namely the arguments that are passed to the model and guide. Note that we have to put the RNN into and out of evaluation mode to account for dropout. We can now stick <cite>do_evaluation()</cite> into the training loop; see [the source code](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm.py">https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm.py</a>) for details.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Resultsn”,
“n”,
“Let’s make sure that our implementation gives reasonable results. We can use the numbers reported in reference [1] as a sanity check. For the same dataset and a similar model/guide setup (dimension of the latent space, number of hidden units in the RNN, etc.) they report a normalized negative log likelihood (NLL) of <cite>6.93</cite> on the testset (lower is better$)^{\S}$. This is to be compared to our result of <cite>6.87</cite>. These numbers are very much in the same ball park, which is reassuring. It seems that, at least for this dataset, not using analytic expressions for the KL divergences doesn’t degrade the quality of the learned model (although, as discussed above, the training probably takes somewhat longer).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “raw”,
“metadata”: {</p>
<blockquote>
<div><p>“raw_mimetype”: “text/html”</p>
</div></blockquote>
<p>},
“source”: [</p>
<blockquote>
<div><p>“&lt;figure&gt;&lt;img src=&quot;_static/img/test_nll.png&quot; style=&quot;width: 400px;&quot;&gt;&lt;center&gt;&lt;figcaption&gt; &lt;font size=&quot;-1&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: Progress on the test set NLL as training progresses for a sample training run. &lt;/font&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/center&gt;”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“In the figure we show how the test NLL progresses during training for a single sample run (one with a rather conservative learning rate). Most of the progress is during the first 3000 epochs or so, with some marginal gains if we let training go on for longer. On a GeForce GTX 1080, 5000 epochs takes about 20 hours.n”,
“n”,
“n”,
“| <cite>num_iafs</cite>  | test NLL   <a href="#id5"><span class="problematic" id="id6">|\n&quot;,
&quot;|</span></a>—<a href="#id7"><span class="problematic" id="id8">|---|</span></a>n”,
“| <cite>0</cite>  | <cite>6.87</cite>  | n”,
“| <cite>1</cite>  | <cite>6.82</cite>  <a href="#id9"><span class="problematic" id="id10">|\n&quot;,
&quot;|</span></a> <cite>2</cite>  | <cite>6.80</cite> <a href="#id3"><span class="problematic" id="id4">|</span></a>n”,
“n”,
“Finally, we also report results for guides with normalizing flows in the mix (details to be found in the next section). n”,
“n”,
“${ \S\;}$ Actually, they seem to report two numbers—6.93 and 7.03—for the same model/guide and it’s not entirely clear how the two reported numbers are different.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Bells, whistles, and other improvementsn”,
“n”,
“### Inverse Autoregressive Flowsn”,
“n”,
“One of the great things about a probabilistic programming language is that it encourages modularity. Let’s showcase an example in the context of the DMM. We’re going to make our variational distribution richer by adding normalizing flows to the mix (see reference [2] for a discussion). <strong>This will only cost us four additional lines of code!</strong>n”,
“n”,
“First, in the <cite>DMM</cite> constructor we addn”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;iafs</span> <span class="pre">=</span> <span class="pre">[AffineAutoregressive(AutoRegressiveNN(z_dim,</span> <span class="pre">[iaf_dim]))</span> <span class="pre">for</span> <span class="pre">_</span> <span class="pre">in</span> <span class="pre">range(num_iafs)]\n&quot;,</span>
<span class="pre">&quot;self.iafs</span> <span class="pre">=</span> <span class="pre">nn.ModuleList(iafs)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“This instantiates <cite>num_iafs</cite> many bijective transforms of the <cite>AffineAutoregressive</cite> type (see references [3,4]); each normalizing flow will have <cite>iaf_dim</cite> many hidden units. We then bundle the normalizing flows in a <cite>nn.ModuleList</cite>; this is just the PyTorchy way to package a list of <cite>nn.Module`s. Next, in the guide we add the linesn”,
“n”,
“```pythonn”,
“if self.iafs.__len__() &gt; 0:n”,
”    z_dist = TransformedDistribution(z_dist, self.iafs)n”,
“``</cite>n”,
“n”,
“Here we’re taking the base distribution <cite>z_dist</cite>, which in our case is a conditional gaussian distribution, and using the <cite>TransformedDistribution</cite> construct we transform it into a non-gaussian distribution that is, by construction, richer than the base distribution. Voila!”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Checkpointingn”,
“n”,
“If we want to recover from a catastrophic failure in our training loop, there are two kinds of state we need to keep track of. The first is the various parameters of the model and guide. The second is the state of the optimizers (e.g. in Adam this will include the running average of recent gradient estimates for each parameter).n”,
“n”,
“In Pyro, the parameters can all be found in the <cite>ParamStore</cite>. However, PyTorch also keeps track of them for us via the <cite>parameters()</cite> method of <cite>nn.Module</cite>. So one simple way we can save the parameters of the model and guide is to make use of the <cite>state_dict()</cite> method of <cite>dmm</cite> in conjunction with <cite>torch.save()</cite>; see below. In the case that we have <cite>AffineAutoregressive</cite>’s in the loop, this is in fact the only option at our disposal. This is because the <cite>AffineAutoregressive</cite> module contains what are called ‘persistent buffers’ in PyTorch parlance. These are things that carry state but are not <cite>Parameter`s. The `state_dict()</cite> and <cite>load_state_dict()</cite> methods of <cite>nn.Module</cite> know how to deal with buffers correctly.n”,
“n”,
“To save the state of the optimizers, we have to use functionality inside of <cite>pyro.optim.PyroOptim</cite>. Recall that the typical user never interacts directly with PyTorch <cite>Optimizers</cite> when using Pyro; since parameters can be created dynamically in an arbitrary probabilistic program, Pyro needs to manage <cite>Optimizers</cite> for us. In our case saving the optimizer state will be as easy as calling <cite>optimizer.save()</cite>. The loading logic is entirely analagous. So our entire logic for saving and loading checkpoints only takes a few lines:n”,
“n”,
“<code class="docutils literal notranslate"><span class="pre">`python\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">saves</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">and</span> <span class="pre">optimizer</span> <span class="pre">states</span> <span class="pre">to</span> <span class="pre">disk\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">save_checkpoint():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;saving</span> <span class="pre">model</span> <span class="pre">to</span> <span class="pre">%s...\&quot;</span> <span class="pre">%</span> <span class="pre">args.save_model)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">torch.save(dmm.state_dict(),</span> <span class="pre">args.save_model)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;saving</span> <span class="pre">optimizer</span> <span class="pre">states</span> <span class="pre">to</span> <span class="pre">%s...\&quot;</span> <span class="pre">%</span> <span class="pre">args.save_opt)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">optimizer.save(args.save_opt)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;done</span> <span class="pre">saving</span> <span class="pre">model</span> <span class="pre">and</span> <span class="pre">optimizer</span> <span class="pre">checkpoints</span> <span class="pre">to</span> <span class="pre">disk.\&quot;)\n&quot;,</span>
<span class="pre">&quot;\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">loads</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">and</span> <span class="pre">optimizer</span> <span class="pre">states</span> <span class="pre">from</span> <span class="pre">disk\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">load_checkpoint():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">assert</span> <span class="pre">exists(args.load_opt)</span> <span class="pre">and</span> <span class="pre">exists(args.load_model),</span> <span class="pre">\\\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">\&quot;--load-model</span> <span class="pre">and/or</span> <span class="pre">--load-opt</span> <span class="pre">misspecified\&quot;\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;loading</span> <span class="pre">model</span> <span class="pre">from</span> <span class="pre">%s...\&quot;</span> <span class="pre">%</span> <span class="pre">args.load_model)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">dmm.load_state_dict(torch.load(args.load_model))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;loading</span> <span class="pre">optimizer</span> <span class="pre">states</span> <span class="pre">from</span> <span class="pre">%s...\&quot;</span> <span class="pre">%</span> <span class="pre">args.load_opt)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">optimizer.load(args.load_opt)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">log(\&quot;done</span> <span class="pre">loading</span> <span class="pre">model</span> <span class="pre">and</span> <span class="pre">optimizer</span> <span class="pre">states.\&quot;)\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Some final commentsn”,
“n”,
“A deep markov model is a relatively complex model. Now that we’ve taken the effort to implement a version of the deep markov model tailored to the polyphonic music dataset, we should ask ourselves what else we can do. What if we’re handed a different sequential dataset? Do we have to start all over?n”,
“n”,
“Not at all! The beauty of probalistic programming is that it enables&amp;mdash;and encourages&amp;mdash;modular approaches to modeling and inference. Adapting our polyphonic music model to a dataset with continuous observations is as simple as changing the observation likelihood. The vast majority of the code could be taken over unchanged. This means that with a little bit of extra work, the code in this tutorial could be repurposed to enable a huge variety of different models. n”,
“n”,
“See the complete code on [Github](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm.py).n">https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm.py).n</a>”,
“n”,
“## Referencesn”,
“n”,
“[1] <cite>Structured Inference Networks for Nonlinear State Space Models</cite>,&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
”    Rahul G. Krishnan, Uri Shalit, David Sontagn”,
” n”,
“[2] <cite>Variational Inference with Normalizing Flows</cite>,n”,
“&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Danilo Jimenez Rezende, Shakir Mohamed n”,
” n”,
“[3] <cite>Improving Variational Inference with Inverse Autoregressive Flow</cite>,n”,
“&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling    n”,
“n”,
“[4] <cite>MADE: Masked Autoencoder for Distribution Estimation</cite>,n”,
“&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle n”,
“n”,
“[5] <cite>Modeling Temporal Dependencies in High-Dimensional Sequences:</cite>n”,
“&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“<cite>Application to Polyphonic Music Generation and Transcription</cite>,n”,
“&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;n”,
“Boulanger-Lewandowski, N., Bengio, Y. and Vincent, P.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>],
“metadata”: {</p>
<blockquote>
<div><p>“celltoolbar”: “Raw Cell Format”,
“kernelspec”: {</p>
<blockquote>
<div><p>“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”</p>
</div></blockquote>
<p>},
“language_info”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“codemirror_mode”: {</dt><dd><p>“name”: “ipython”,
“version”: 3</p>
</dd>
</dl>
<p>},
“file_extension”: “.py”,
“mimetype”: “text/x-python”,
“name”: “python”,
“nbconvert_exporter”: “python”,
“pygments_lexer”: “ipython3”,
“version”: “3.6.10”</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>},
“nbformat”: 4,
“nbformat_minor”: 2</p>
</dd>
</dl>
<p>}</p>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="normalizing_flows_i.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="air.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>