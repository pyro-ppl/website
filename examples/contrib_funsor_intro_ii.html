<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Pyro Tutorials 1.8.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Example: hidden Markov models with pyro.contrib.funsor and pyroapi" href="hmm_funsor.html" />
    <link rel="prev" title="&lt;no title&gt;" href="contrib_funsor_intro_i.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/contrib_funsor_intro_ii.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<dl>
<dt>{</dt><dd><dl>
<dt>“cells”: [</dt><dd><dl>
<dt>{</dt><dd><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“# <cite>pyro.contrib.funsor</cite>, a new backend for Pyro - Building inference algorithms (Part 2)”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 1,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“from collections import OrderedDictn”,
“import functoolsn”,
“n”,
“import torchn”,
“from torch.distributions import constraintsn”,
“n”,
“import funsorn”,
“n”,
“from pyro import set_rng_seed as pyro_set_rng_seedn”,
“from pyro.ops.indexing import Vindexn”,
“from pyro.poutine.messenger import Messengern”,
“n”,
“funsor.set_backend(&quot;torch&quot;)n”,
“torch.set_default_dtype(torch.float32)n”,
“pyro_set_rng_seed(101)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Introductionn”,
“n”,
“In part 1 of this tutorial, we were introduced to the new <cite>pyro.contrib.funsor</cite> backend for Pyro.n”,
“n”,
“Here we’ll look at how to use the components in <cite>pyro.contrib.funsor</cite> to implement a variable elimination inference algorithm from scratch. This tutorial assumes readers are familiar with enumeration-based inference algorithms in Pyro. For background and motivation, readers should consult the [enumeration tutorial](<a class="reference external" href="http://pyro.ai/examples/enumeration.html).n">http://pyro.ai/examples/enumeration.html).n</a>”,
“n”,
“As before, we’ll use <cite>pyroapi</cite> so that we can write our model with standard Pyro syntax.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 2,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“import pyro.contrib.funsorn”,
“import pyroapin”,
“from pyroapi import infer, handlers, ops, optim, pyron”,
“from pyroapi import distributions as dist”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We will be working with the following model throughout. It is a discrete-state continuous-observation hidden Markov model with learnable transition and emission distributions that depend on a global random variable.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 3,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“data = [torch.tensor(1.)] * 10n”,
“n”,
“def model(data, verbose):n”,
“n”,
”    p = pyro.param(&quot;probs&quot;, lambda: torch.rand((3, 3)), constraint=constraints.simplex)n”,
”    locs_mean = pyro.param(&quot;locs_mean&quot;, lambda: torch.ones((3,)))n”,
”    locs = pyro.sample(&quot;locs&quot;, dist.Normal(locs_mean, 1.).to_event(1))n”,
”    if verbose:n”,
”        print(&quot;locs.shape = {}&quot;.format(locs.shape))n”,
“n”,
”    x = 0n”,
”    for i in pyro.markov(range(len(data))):n”,
”        x = pyro.sample(&quot;x{}&quot;.format(i), dist.Categorical(p[x]), infer={&quot;enumerate&quot;: &quot;parallel&quot;})n”,
”        if verbose:n”,
”            print(&quot;x{}.shape = &quot;.format(i), x.shape)n”,
”        pyro.sample(&quot;y{}&quot;.format(i), dist.Normal(Vindex(locs)[…, x], 1.), obs=data[i])”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We can run <cite>model</cite> under the default Pyro backend and the new <cite>contrib.funsor</cite> backend with <cite>pyroapi</cite>:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 4,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“locs.shape = torch.Size([3])n”,
“x0.shape =  torch.Size([])n”,
“x1.shape =  torch.Size([])n”,
“x2.shape =  torch.Size([])n”,
“x3.shape =  torch.Size([])n”,
“x4.shape =  torch.Size([])n”,
“x5.shape =  torch.Size([])n”,
“x6.shape =  torch.Size([])n”,
“x7.shape =  torch.Size([])n”,
“x8.shape =  torch.Size([])n”,
“x9.shape =  torch.Size([])n”,
“locs.shape = torch.Size([3])n”,
“x0.shape =  torch.Size([])n”,
“x1.shape =  torch.Size([])n”,
“x2.shape =  torch.Size([])n”,
“x3.shape =  torch.Size([])n”,
“x4.shape =  torch.Size([])n”,
“x5.shape =  torch.Size([])n”,
“x6.shape =  torch.Size([])n”,
“x7.shape =  torch.Size([])n”,
“x8.shape =  torch.Size([])n”,
“x9.shape =  torch.Size([])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“# default backend: &quot;pyro&quot;n”,
“with pyroapi.pyro_backend(&quot;pyro&quot;):n”,
”    model(data, verbose=True)n”,
”    n”,
“# new backend: &quot;contrib.funsor&quot;n”,
“with pyroapi.pyro_backend(&quot;contrib.funsor&quot;):n”,
”    model(data, verbose=True)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Enumerating discrete variablesn”,
“n”,
“Our first step is to implement an effect handler that performs parallel enumeration of discrete latent variables. Here we will implement a stripped-down version of <cite>pyro.poutine.enum</cite>, the effect handler behind Pyro’s most powerful general-purpose inference algorithms <cite>pyro.infer.TraceEnum_ELBO</cite> and <cite>pyro.infer.mcmc.HMC</cite>.n”,
“n”,
“We’ll do that by constructing a <cite>funsor.Tensor</cite> representing the support of each discrete latent variable and using the new <cite>pyro.to_data</cite> primitive from part 1 to convert it to a <cite>torch.Tensor</cite> with the appropriate shape.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 5,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“from pyro.contrib.funsor.handlers.named_messenger import NamedMessengern”,
“n”,
“class EnumMessenger(NamedMessenger):n”,
”    n”,
”    &#64;pyroapi.pyro_backend(&quot;contrib.funsor&quot;)  # necessary since we invoke pyro.to_data and pyro.to_funsorn”,
”    def _pyro_sample(self, msg):n”,
”        if msg[&quot;done&quot;] or msg[&quot;is_observed&quot;] or msg[&quot;infer&quot;].get(&quot;enumerate&quot;) != &quot;parallel&quot;:n”,
”            returnn”,
“n”,
”        # We first compute a raw value using the standard enumerate_support method.n”,
”        # enumerate_support returns a value of shape:n”,
”        #     (support_size,) + (1,) * len(msg[&quot;fn&quot;].batch_shape).n”,
”        raw_value = msg[&quot;fn&quot;].enumerate_support(expand=False)n”,
”        n”,
”        # Next we’ll use pyro.to_funsor to indicate that this dimension is fresh.n”,
”        # This is guaranteed because we use msg[‘name’], the name of this pyro.sample site,n”,
”        # as the name for this positional dimension, and sample site names must be unique.n”,
”        funsor_value = pyro.to_funsor(n”,
”            raw_value,n”,
”            output=funsor.Bint[raw_value.shape[0]],n”,
”            dim_to_name={-raw_value.dim(): msg[&quot;name&quot;]},n”,
”        )n”,
“n”,
”        # Finally, we convert the value back to a PyTorch tensor with to_data,n”,
”        # which has the effect of reshaping and possibly permuting dimensions of raw_value.n”,
”        # Applying to_funsor and to_data in this way guarantees thatn”,
”        # each enumerated random variable gets a unique fresh positional dimensionn”,
”        # and that we can convert the model’s log-probability tensors to funsor.Tensorsn”,
”        # in a globally consistent manner.n”,
”        msg[&quot;value&quot;] = pyro.to_data(funsor_value)n”,
”        msg[&quot;done&quot;] = True”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Because this is an introductory tutorial, this implementation of <cite>EnumMessenger</cite> works directly with the site’s PyTorch distribution since users familiar with PyTorch and Pyro may find it easier to understand. However, when using <cite>contrib.funsor</cite> to implement an inference algorithm in a more realistic setting, it is usually preferable to do as much computation as possible on funsors, as this tends to simplify complex indexing, broadcasting or shape manipulation logic.n”,
“n”,
“For example, in <cite>EnumMessenger</cite>, we might instead call <cite>pyro.to_funsor</cite> on <cite>msg[&quot;fn&quot;]</cite>:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;funsor_dist</span> <span class="pre">=</span> <span class="pre">pyro.to_funsor(msg[\&quot;fn\&quot;],</span> <span class="pre">output=funsor.Real)(value=msg[\&quot;name\&quot;])\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">enumerate_support</span> <span class="pre">defined</span> <span class="pre">whenever</span> <span class="pre">isinstance(funsor_dist,</span> <span class="pre">funsor.distribution.Distribution)\n&quot;,</span>
<span class="pre">&quot;funsor_value</span> <span class="pre">=</span> <span class="pre">funsor_dist.enumerate_support(expand=False)\n&quot;,</span>
<span class="pre">&quot;raw_value</span> <span class="pre">=</span> <span class="pre">pyro.to_data(funsor_value)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“Most of the more complete inference algorithms implemented in <cite>pyro.contrib.funsor</cite> follow this pattern, and we will see an example later in this tutorial. Before we continue, let’s see what effect <cite>EnumMessenger</cite> has on the shapes of random variables in our model:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 6,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“locs.shape = torch.Size([3])n”,
“x0.shape =  torch.Size([3, 1, 1, 1, 1])n”,
“x1.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x2.shape =  torch.Size([3, 1, 1, 1, 1])n”,
“x3.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x4.shape =  torch.Size([3, 1, 1, 1, 1])n”,
“x5.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x6.shape =  torch.Size([3, 1, 1, 1, 1])n”,
“x7.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x8.shape =  torch.Size([3, 1, 1, 1, 1])n”,
“x9.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“with pyroapi.pyro_backend(&quot;contrib.funsor&quot;), \n”,
”        EnumMessenger():n”,
”    model(data, True)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Vectorizing a model across multiple samplesn”,
“n”,
“Next, since our priors over global variables are continuous and cannot be enumerated exactly, we will implement an effect handler that uses a global dimension to draw multiple samples in parallel from the model. Our implementation will allocate a new particle dimension using <cite>pyro.to_data</cite> as in <cite>EnumMessenger</cite> above, but unlike the enumeration dimensions, we want the particle dimension to be shared across all sample sites, so we will mark it as a <cite>DimType.GLOBAL</cite> dimension when invoking <cite>pyro.to_funsor</cite>.n”,
“n”,
“Recall that in part 1 we saw that <cite>DimType.GLOBAL</cite> dimensions must be deallocated manually or they will persist until the final effect handler has exited. This low-level detail is taken care of automatically by the <cite>GlobalNameMessenger</cite> handler provided in <cite>pyro.contrib.funsor</cite> as a base class for any effect handlers that allocate global dimensions. Our vectorization effect handler will inherit from this class.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 7,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“from pyro.contrib.funsor.handlers.named_messenger import GlobalNamedMessengern”,
“from pyro.contrib.funsor.handlers.runtime import DimRequest, DimTypen”,
“n”,
“class VectorizeMessenger(GlobalNamedMessenger):n”,
”    n”,
”    def __init__(self, size, name=&quot;_PARTICLES&quot;):n”,
”        super().__init__()n”,
”        self.name = namen”,
”        self.size = sizen”,
“n”,
”    &#64;pyroapi.pyro_backend(&quot;contrib.funsor&quot;)n”,
”    def _pyro_sample(self, msg):n”,
”        if msg[&quot;is_observed&quot;] or msg[&quot;done&quot;] or msg[&quot;infer&quot;].get(&quot;enumerate&quot;) == &quot;parallel&quot;:n”,
”            returnn”,
”        n”,
”        # we’ll first draw a raw batch of samples similarly to EnumMessenger.n”,
”        # However, since we are drawing a single batch from the joint distribution,n”,
”        # we don’t need to take multiple samples if the site is already batched.n”,
”        if self.name in pyro.to_funsor(msg[&quot;fn&quot;], funsor.Real).inputs:n”,
”            raw_value = msg[&quot;fn&quot;].rsample()n”,
”        else:n”,
”            raw_value = msg[&quot;fn&quot;].rsample(sample_shape=(self.size,))n”,
”        n”,
”        # As before, we’ll use pyro.to_funsor to register the new dimension.n”,
”        # This time, we indicate that the particle dimension should be treated as a global dimension.n”,
”        fresh_dim = len(msg[&quot;fn&quot;].event_shape) - raw_value.dim()n”,
”        funsor_value = pyro.to_funsor(n”,
”            raw_value,n”,
”            output=funsor.Reals[tuple(msg[&quot;fn&quot;].event_shape)],n”,
”            dim_to_name={fresh_dim: DimRequest(value=self.name, dim_type=DimType.GLOBAL)},n”,
”        )n”,
”        n”,
”        # finally, convert the sample to a PyTorch tensor using to_data as beforen”,
”        msg[&quot;value&quot;] = pyro.to_data(funsor_value)n”,
”        msg[&quot;done&quot;] = True”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Let’s see what effect <cite>VectorizeMessenger</cite> has on the shapes of the values in <cite>model</cite>:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 8,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“locs.shape = torch.Size([10, 1, 1, 1, 1, 3])n”,
“x0.shape =  torch.Size([])n”,
“x1.shape =  torch.Size([])n”,
“x2.shape =  torch.Size([])n”,
“x3.shape =  torch.Size([])n”,
“x4.shape =  torch.Size([])n”,
“x5.shape =  torch.Size([])n”,
“x6.shape =  torch.Size([])n”,
“x7.shape =  torch.Size([])n”,
“x8.shape =  torch.Size([])n”,
“x9.shape =  torch.Size([])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“with pyroapi.pyro_backend(&quot;contrib.funsor&quot;), \n”,
”        VectorizeMessenger(size=10):n”,
”    model(data, verbose=True)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“And now in combination with <cite>EnumMessenger</cite>:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 9,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“locs.shape = torch.Size([10, 1, 1, 1, 1, 3])n”,
“x0.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x1.shape =  torch.Size([3, 1, 1, 1, 1, 1, 1])n”,
“x2.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x3.shape =  torch.Size([3, 1, 1, 1, 1, 1, 1])n”,
“x4.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x5.shape =  torch.Size([3, 1, 1, 1, 1, 1, 1])n”,
“x6.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x7.shape =  torch.Size([3, 1, 1, 1, 1, 1, 1])n”,
“x8.shape =  torch.Size([3, 1, 1, 1, 1, 1])n”,
“x9.shape =  torch.Size([3, 1, 1, 1, 1, 1, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“with pyroapi.pyro_backend(&quot;contrib.funsor&quot;), \n”,
”        VectorizeMessenger(size=10), EnumMessenger():n”,
”    model(data, verbose=True)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Computing an ELBO with variable eliminationn”,
“n”,
“Now that we have tools for enumerating discrete variables and drawing batches of samples, we can use those to compute quantities of interest for inference algorithms.n”,
“n”,
“Most inference algorithms in Pyro work with <cite>pyro.poutine.Trace`s, custom data structures that contain parameters and sample site distributions and values and all of the associated metadata needed for inference computations. Our third effect handler `LogJointMessenger</cite> departs from this design pattern, eliminating a tremendous amount of boilerplate in the process. It will automatically build up a lazy Funsor expression for the logarithm of the joint probability density of a model; when working with <cite>Trace`s, this process must be triggered manually by calling `Trace.compute_log_probs()</cite> and eagerly computing an objective from the resulting individual log-probability tensors in the trace.n”,
“n”,
“In our implementation of <cite>LogJointMessenger</cite>, unlike the previous two effect handlers, we will call <cite>pyro.to_funsor</cite> on both the sample value and the distribution to show how nearly all inference operations including log-probability density evaluation can be performed on <a href="#id1"><span class="problematic" id="id2">`</span></a>funsor.Funsor`s directly.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 10,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“class LogJointMessenger(Messenger):n”,
“n”,
”    def __enter__(self):n”,
”        self.log_joint = funsor.Number(0.)n”,
”        return super().__enter__()n”,
“n”,
”    &#64;pyroapi.pyro_backend(&quot;contrib.funsor&quot;)n”,
”    def _pyro_post_sample(self, msg):n”,
”        n”,
”        # for Monte Carlo-sampled variables, we don’t include a log-density term:n”,
”        if not msg[&quot;is_observed&quot;] and not msg[&quot;infer&quot;].get(&quot;enumerate&quot;):n”,
”            returnn”,
”        n”,
”        with funsor.interpreter.interpretation(funsor.terms.lazy):n”,
”            funsor_dist = pyro.to_funsor(msg[&quot;fn&quot;], output=funsor.Real)n”,
”            funsor_value = pyro.to_funsor(msg[&quot;value&quot;], output=funsor_dist.inputs[&quot;value&quot;])n”,
”            self.log_joint += funsor_dist(value=funsor_value)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“And finally the actual loss function, which applies our three effect handlers to compute an expression for the log-density, marginalizes over discrete variables with <cite>funsor.ops.logaddexp</cite>, averages over Monte Carlo samples with <cite>funsor.ops.add</cite>, and evaluates the final lazy expression using Funsor’s <cite>optimize</cite> interpretation for variable elimination.n”,
“n”,
“Note that <cite>log_z</cite> exactly collapses the model’s local discrete latent variables but is an ELBO wrt any continuous latent variables, and is thus equivalent to a simple version of <cite>TraceEnum_ELBO</cite> with an empty guide.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 11,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“&#64;pyroapi.pyro_backend(&quot;contrib.funsor&quot;)n”,
“def log_z(model, model_args, size=10):n”,
”    with LogJointMessenger() as tr, \n”,
”            VectorizeMessenger(size=size) as v, \n”,
”            EnumMessenger():n”,
”        model(<a href="#id3"><span class="problematic" id="id4">*</span></a>model_args)n”,
“n”,
”    with funsor.interpreter.interpretation(funsor.terms.lazy):n”,
”        prod_vars = frozenset({v.name})n”,
”        sum_vars = frozenset(tr.log_joint.inputs) - prod_varsn”,
”        n”,
”        # sum over the discrete random variables we enumeratedn”,
”        expr = tr.log_joint.reduce(funsor.ops.logaddexp, sum_vars)n”,
”        n”,
”        # average over the sample dimensionn”,
”        expr = expr.reduce(funsor.ops.add, prod_vars) - funsor.Number(float(size))n”,
“n”,
”    return pyro.to_data(funsor.optimizer.apply_optimizer(expr))”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Putting it all togethern”,
“n”,
“Finally, with all this machinery implemented, we can compute stochastic gradients wrt the ELBO.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 12,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“tensor(-133.6274, grad_fn=&lt;AddBackward0&gt;)n”,
“tensor(-129.2379, grad_fn=&lt;AddBackward0&gt;)n”,
“tensor(-125.9609, grad_fn=&lt;AddBackward0&gt;)n”,
“tensor(-123.7484, grad_fn=&lt;AddBackward0&gt;)n”,
“tensor(-122.3034, grad_fn=&lt;AddBackward0&gt;)n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“with pyroapi.pyro_backend(&quot;contrib.funsor&quot;):n”,
”    model(data, verbose=False)  # initialize parametersn”,
”    params = [pyro.param(&quot;probs&quot;).unconstrained(), pyro.param(&quot;locs_mean&quot;).unconstrained()]n”,
“n”,
“optimizer = torch.optim.Adam(params, lr=0.1)n”,
“for step in range(5):n”,
”    optimizer.zero_grad()n”,
”    log_marginal = log_z(model, (data, False))n”,
”    (-log_marginal).backward()n”,
”    optimizer.step()n”,
”    print(log_marginal)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: null,
“metadata”: {},
“outputs”: [],
“source”: []</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>],
“metadata”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“kernelspec”: {</dt><dd><p>“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”</p>
</dd>
</dl>
<p>},
“language_info”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“codemirror_mode”: {</dt><dd><p>“name”: “ipython”,
“version”: 3</p>
</dd>
</dl>
<p>},
“file_extension”: “.py”,
“mimetype”: “text/x-python”,
“name”: “python”,
“nbconvert_exporter”: “python”,
“pygments_lexer”: “ipython3”,
“version”: “3.7.3”</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>},
“nbformat”: 4,
“nbformat_minor”: 2</p>
</dd>
</dl>
<p>}</p>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="contrib_funsor_intro_i.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hmm_funsor.html" class="btn btn-neutral float-right" title="Example: hidden Markov models with pyro.contrib.funsor and pyroapi" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>