<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Pyro Tutorials 1.8.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="gmm.html" />
    <link rel="prev" title="&lt;no title&gt;" href="scanvi.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.8.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Practical Pyro and PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="svi_horovod.html">Example: distributed training via Horovod</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Generative Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Example: Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Example: Sparse Gamma Deep Exponential Family</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Discrete Latent Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="toy_mixture_model_discrete_enumeration.html">Example: Toy Mixture Model With Discrete Enumeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Example: Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Example: Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_hmm.html">Example: hierarchical mixed-effect hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Example: Discrete Factor Graph Inference with Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Example: Amortized Latent Dirichlet Allocation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Customizing Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">Example: Neural MCMC with NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_regression.html">Example: Sparse Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoname_examples.html">Example: reducing boilerplate with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.autoname</span></code></a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Time Series</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Example: Gaussian Process Time Series Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Gaussian Processes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Example: Deep Kernel Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Epidemiology</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="epi_sir.html">Example: Univariate epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="epi_regional.html">Example: Regional epidemiological models</a></li>
<li class="toctree-l1"><a class="reference internal" href="sir_hmc.html">Example: Epidemiological inference via HMC</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Application: Biological sequences</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mue_profile.html">Example: Constant + MuE (Profile HMM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="mue_factor.html">Example: Probabilistic PCA + MuE (FactorMuE)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Inference Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="baseball.html">Example: analyzing baseball stats with MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Example: Inference with Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lkj.html">Example: MCMC with an LKJ prior over covariances</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Example: Sequential Monte Carlo Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="inclined_plane.html">Example: importance sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Understanding Pyro's Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm_funsor.html">Example: hidden Markov models with <code class="docutils literal notranslate"><span class="pre">pyro.contrib.funsor</span></code> and <code class="docutils literal notranslate"><span class="pre">pyroapi</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/enumeration.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<dl>
<dt>{</dt><dd><dl>
<dt>“cells”: [</dt><dd><dl>
<dt>{</dt><dd><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“# Inference with Discrete Latent Variablesn”,
“n”,
“This tutorial describes Pyro’s enumeration strategy for discrete latent variable models.n”,
“This tutorial assumes the reader is already familiar with the [Tensor Shapes Tutorial](<a class="reference external" href="http://pyro.ai/examples/tensor_shapes.html).n">http://pyro.ai/examples/tensor_shapes.html).n</a>”,
“n”,
“#### Summary n”,
“n”,
“- Pyro implements automatic enumeration over discrete latent variables.n”,
“- This strategy can be used alone or inside SVI (via [TraceEnum_ELBO](<a class="reference external" href="http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.traceenum_elbo.TraceEnum_ELBO">http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.traceenum_elbo.TraceEnum_ELBO</a>)), HMC, or NUTS.n”,
“- The standalone [infer_discrete](<a class="reference external" href="http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.discrete.infer_discrete">http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.discrete.infer_discrete</a>) can generate samples or MAP estimates.n”,
“- Annotate a sample site <cite>infer={&quot;enumerate&quot;: &quot;parallel&quot;}</cite> to trigger enumeration.n”,
“- If a sample site determines downstream structure, instead use <cite>{&quot;enumerate&quot;: &quot;sequential&quot;}</cite>.n”,
“- Write your models to allow arbitrarily deep batching on the left, e.g. use broadcasting.n”,
“- Inference cost is exponential in treewidth, so try to write models with narrow treewidth.n”,
“- If you have trouble, ask for help on [forum.pyro.ai](<a class="reference external" href="https://forum.pyro.ai)!n">https://forum.pyro.ai)!n</a>”,
“n”,
“#### Table of contentsn”,
“n”,
“- [Overview](#Overview)n”,
“- [Mechanics of enumeration](#Mechanics-of-enumeration)n”,
”  - [Multiple latent variables](#Multiple-latent-variables)n”,
”  - [Examining discrete latent states](#Examining-discrete-latent-states)n”,
”  - [Indexing with enumerated variables](#Indexing-with-enumerated-variables)n”,
“- [Plates and enumeration](#Plates-and-enumeration)n”,
”  - [Dependencies among plates](#Dependencies-among-plates)n”,
“- [Time series example](#Time-series-example)n”,
”  - [How to enumerate more than 25 variables](#How-to-enumerate-more-than-25-variables)”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 1,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“import osn”,
“import torchn”,
“import pyron”,
“import pyro.distributions as distn”,
“from torch.distributions import constraintsn”,
“from pyro import poutinen”,
“from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate, infer_discreten”,
“from pyro.infer.autoguide import AutoNormaln”,
“from pyro.ops.indexing import Vindexn”,
“n”,
“smoke_test = (‘CI’ in os.environ)n”,
“assert pyro.__version__.startswith(‘1.8.1’)n”,
“pyro.set_rng_seed(0)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Overview &lt;a class=&quot;anchor&quot; id=&quot;Overview&quot;&gt;&lt;/a&gt;n”,
“n”,
“Pyro’s enumeration strategy ([Obermeyer et al. 2019](<a class="reference external" href="https://arxiv.org/abs/1902.03210">https://arxiv.org/abs/1902.03210</a>)) encompasses popular algorithms including variable elimination, exact message passing, forward-filter-backward-sample, inside-out, Baum-Welch, and many other special-case algorithms. Aside from enumeration, Pyro implements a number of inference strategies including variational inference ([SVI](<a class="reference external" href="http://docs.pyro.ai/en/dev/inference_algos.html">http://docs.pyro.ai/en/dev/inference_algos.html</a>)) and monte carlo ([HMC](<a class="reference external" href="http://docs.pyro.ai/en/dev/mcmc.html#pyro.infer.mcmc.HMC">http://docs.pyro.ai/en/dev/mcmc.html#pyro.infer.mcmc.HMC</a>) and [NUTS](<a class="reference external" href="http://docs.pyro.ai/en/dev/mcmc.html#pyro.infer.mcmc.NUTS">http://docs.pyro.ai/en/dev/mcmc.html#pyro.infer.mcmc.NUTS</a>)). Enumeration can be used either as a stand-alone strategy via [infer_discrete](<a class="reference external" href="http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.discrete.infer_discrete">http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.discrete.infer_discrete</a>), or as a component of other strategies. Thus enumeration allows Pyro to marginalize out discrete latent variables in HMC and SVI models, and to use variational enumeration of discrete variables in SVI guides.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Mechanics of enumeration  &lt;a class=&quot;anchor&quot; id=&quot;Mechanics-of-enumeration&quot;&gt;&lt;/a&gt;n”,
“n”,
“The core idea of enumeration is to interpret discrete [pyro.sample](<a class="reference external" href="http://docs.pyro.ai/en/dev/primitives.html#pyro.sample">http://docs.pyro.ai/en/dev/primitives.html#pyro.sample</a>) statements as full enumeration rather than random sampling. Other inference algorithms can then sum out the enumerated values. For example a sample statement might return a tensor of scalar shape under the standard &quot;sample&quot; interpretation (we’ll illustrate with trivial model and guide):”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 2,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“guide z = 4n”,
“model z = 4n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“def model():n”,
”    z = pyro.sample(&quot;z&quot;, dist.Categorical(torch.ones(5)))n”,
”    print(f&quot;model z = {z}&quot;)n”,
“n”,
“def guide():n”,
”    z = pyro.sample(&quot;z&quot;, dist.Categorical(torch.ones(5)))n”,
”    print(f&quot;guide z = {z}&quot;)n”,
“n”,
“elbo = Trace_ELBO()n”,
“elbo.loss(model, guide);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“However under the enumeration interpretation, the same sample site will return a fully enumerated set of values, based on its distribution’s [.enumerate_support()](<a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.enumerate_support">https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.enumerate_support</a>) method.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 3,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“guide z = tensor([0, 1, 2, 3, 4])n”,
“model z = tensor([0, 1, 2, 3, 4])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“elbo = TraceEnum_ELBO(max_plate_nesting=0)n”,
“elbo.loss(model, config_enumerate(guide, &quot;parallel&quot;));”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Note that we’ve used &quot;parallel&quot; enumeration to enumerate along a new tensor dimension. This is cheap and allows Pyro to parallelize computation, but requires downstream program structure to avoid branching on the value of <cite>z</cite>. To support dynamic program structure, you can instead use &quot;sequential&quot; enumeration, which runs the entire model,guide pair once per sample value, but requires running the model multiple times.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 4,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“guide z = 4n”,
“model z = 4n”,
“guide z = 3n”,
“model z = 3n”,
“guide z = 2n”,
“model z = 2n”,
“guide z = 1n”,
“model z = 1n”,
“guide z = 0n”,
“model z = 0n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“elbo = TraceEnum_ELBO(max_plate_nesting=0)n”,
“elbo.loss(model, config_enumerate(guide, &quot;sequential&quot;));”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Parallel enumeration is cheaper but more complex than sequential enumeration, so we’ll focus the rest of this tutorial on the parallel variant. Note that both forms can be interleaved.n”,
“n”,
“### Multiple latent variables &lt;a class=&quot;anchor&quot; id=&quot;Multiple-latent-variables&quot;&gt;&lt;/a&gt;n”,
“n”,
“We just saw that a single discrete sample site can be enumerated via nonstandard interpretation. A model with a single discrete latent variable is a mixture model. Models with multiple discrete latent variables can be more complex, including HMMs, CRFs, DBNs, and other structured models. In models with multiple discrete latent variables, Pyro enumerates each variable in a different tensor dimension (counting from the right; see [Tensor Shapes Tutorial](<a class="reference external" href="http://pyro.ai/examples/tensor_shapes.html">http://pyro.ai/examples/tensor_shapes.html</a>)). This allows Pyro to determine the dependency graph among variables and then perform cheap exact inference using variable elimination algorithms.n”,
“n”,
“To understand enumeration dimension allocation, consider the following model, where here we collapse variables out of the model, rather than enumerate them in the guide.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 5,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Sampling:n”,
”  model x.shape = torch.Size([])n”,
”  model y.shape = torch.Size([])n”,
”  model z.shape = torch.Size([])n”,
“Enumerated Inference:n”,
”  model x.shape = torch.Size([3])n”,
”  model y.shape = torch.Size([3, 1])n”,
”  model z.shape = torch.Size([3, 1, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“&#64;config_enumeraten”,
“def model():n”,
”    p = pyro.param(&quot;p&quot;, torch.randn(3, 3).exp(), constraint=constraints.simplex)n”,
”    x = pyro.sample(&quot;x&quot;, dist.Categorical(p[0]))n”,
”    y = pyro.sample(&quot;y&quot;, dist.Categorical(p[x]))n”,
”    z = pyro.sample(&quot;z&quot;, dist.Categorical(p[y]))n”,
”    print(f&quot;  model x.shape = {x.shape}&quot;)n”,
”    print(f&quot;  model y.shape = {y.shape}&quot;)n”,
”    print(f&quot;  model z.shape = {z.shape}&quot;)n”,
”    return x, y, zn”,
”    n”,
“def guide():n”,
”    passn”,
“n”,
“pyro.clear_param_store()n”,
“print(&quot;Sampling:&quot;)n”,
“model()n”,
“print(&quot;Enumerated Inference:&quot;)n”,
“elbo = TraceEnum_ELBO(max_plate_nesting=0)n”,
“elbo.loss(model, guide);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Examining discrete latent states &lt;a class=&quot;anchor&quot; id=&quot;Examining-discrete-latent-states&quot;&gt;&lt;/a&gt;n”,
“n”,
“While enumeration in SVI allows fast learning of parameters like <cite>p</cite> above, it does not give access to predicted values of the discrete latent variables like <cite>x,y,z</cite> above. We can access these using a standalone [infer_discrete](<a class="reference external" href="http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.discrete.infer_discrete">http://docs.pyro.ai/en/dev/inference_algos.html#pyro.infer.discrete.infer_discrete</a>) handler. In this case the guide was trivial, so we can simply wrap the model in <cite>infer_discrete</cite>. We need to pass a <cite>first_available_dim</cite> argument to tell <cite>infer_discrete</cite> which dimensions are available for enumeration; this is related to the <cite>max_plate_nesting</cite> arg of <cite>TraceEnum_ELBO</cite> vian”,
“<code class="docutils literal notranslate"><span class="pre">`\n&quot;,</span>
<span class="pre">&quot;first_available_dim</span> <span class="pre">=</span> <span class="pre">-1</span> <span class="pre">-</span> <span class="pre">max_plate_nesting\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 6,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>”  model x.shape = torch.Size([3])n”,
”  model y.shape = torch.Size([3, 1])n”,
”  model z.shape = torch.Size([3, 1, 1])n”,
”  model x.shape = torch.Size([])n”,
”  model y.shape = torch.Size([])n”,
”  model z.shape = torch.Size([])n”,
“x = 2n”,
“y = 1n”,
“z = 0n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“serving_model = infer_discrete(model, first_available_dim=-1)n”,
“x, y, z = serving_model()  # takes the same args as model(), here no argsn”,
“print(f&quot;x = {x}&quot;)n”,
“print(f&quot;y = {y}&quot;)n”,
“print(f&quot;z = {z}&quot;)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Notice that under the hood <cite>infer_discrete</cite> runs the model twice: first in forward-filter mode where sites are enumerated, then in replay-backward-sample model where sites are sampled. <cite>infer_discrete</cite> can also perform MAP inference by passing <cite>temperature=0</cite>. Note that while <cite>infer_discrete</cite> produces correct posterior samples, it does not currently produce correct logprobs, and should not be used in other gradient-based inference algorthms.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### Indexing with enumerated variablesn”,
“n”,
“It can be tricky to use [advanced indexing](<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html">https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html</a>) to select an element of a tensor using one or more enumerated variables. This is especially true in Pyro models where your model’s indexing operations need to work in multiple interpretations: both sampling from the model (to generate data) and during enumerated inference. For example, suppose a plated random variable <cite>z</cite> depends on two different random variables:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;p</span> <span class="pre">=</span> <span class="pre">pyro.param(\&quot;p\&quot;,</span> <span class="pre">torch.randn(5,</span> <span class="pre">4,</span> <span class="pre">3,</span> <span class="pre">2).exp(),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">constraint=constraints.simplex)\n&quot;,</span>
<span class="pre">&quot;x</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Categorical(torch.ones(4)))\n&quot;,</span>
<span class="pre">&quot;y</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;y\&quot;,</span> <span class="pre">dist.Categorical(torch.ones(3)))\n&quot;,</span>
<span class="pre">&quot;with</span> <span class="pre">pyro.plate(\&quot;z_plate\&quot;,</span> <span class="pre">5):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">p_xy</span> <span class="pre">=</span> <span class="pre">p[...,</span> <span class="pre">x,</span> <span class="pre">y,</span> <span class="pre">:]</span>&#160; <span class="pre">#</span> <span class="pre">Not</span> <span class="pre">compatible</span> <span class="pre">with</span> <span class="pre">enumeration!\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">z</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z\&quot;,</span> <span class="pre">dist.Categorical(p_xy)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“Due to advanced indexing semantics, the expression <cite>p[…, x, y, :]</cite> will work correctly without enumeration, but is incorrect when <cite>x</cite> or <cite>y</cite> is enumerated. Pyro provides a simple way to index correctly, but first let’s see how to correctly index using PyTorch’s advanced indexing without Pyro:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">Compatible</span> <span class="pre">with</span> <span class="pre">enumeration,</span> <span class="pre">but</span> <span class="pre">not</span> <span class="pre">recommended:\n&quot;,</span>
<span class="pre">&quot;p_xy</span> <span class="pre">=</span> <span class="pre">p[torch.arange(5,</span> <span class="pre">device=p.device).reshape(5,</span> <span class="pre">1),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">x.unsqueeze(-1),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">y.unsqueeze(-1),\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">torch.arange(2,</span> <span class="pre">device=p.device)]\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“Pyro provides a helper [Vindex()[]](<a class="reference external" href="http://docs.pyro.ai/en/dev/ops.html#pyro.ops.indexing.Vindex">http://docs.pyro.ai/en/dev/ops.html#pyro.ops.indexing.Vindex</a>) to use enumeration-compatible advanced indexing semantics rather than standard PyTorch/NumPy semantics. (Note the <cite>Vindex</cite> name and semantics follow the Numpy Enhancement Proposal [NEP 21](<a class="reference external" href="https://numpy.org/neps/nep-0021-advanced-indexing.html">https://numpy.org/neps/nep-0021-advanced-indexing.html</a>)). <cite>Vindex()[]</cite> makes the <cite>.__getitem__()</cite> operator broadcast like other familiar operators <cite>+</cite>, <cite>*</cite> etc. Using <cite>Vindex()[]</cite> we can write the same expression as if <cite>x</cite> and <cite>y</cite> were numbers (i.e. not enumerated):n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;#</span> <span class="pre">Recommended</span> <span class="pre">syntax</span> <span class="pre">compatible</span> <span class="pre">with</span> <span class="pre">enumeration:\n&quot;,</span>
<span class="pre">&quot;p_xy</span> <span class="pre">=</span> <span class="pre">Vindex(p)[...,</span> <span class="pre">x,</span> <span class="pre">y,</span> <span class="pre">:]\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“Here is a complete example:”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 7,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Sampling:n”,
”     p.shape = torch.Size([5, 4, 3, 2])n”,
”     x.shape = torch.Size([])n”,
”     y.shape = torch.Size([])n”,
”  p_xy.shape = torch.Size([5, 2])n”,
”     z.shape = torch.Size([5])n”,
“Enumerated Inference:n”,
”     p.shape = torch.Size([5, 4, 3, 2])n”,
”     x.shape = torch.Size([4, 1])n”,
”     y.shape = torch.Size([3, 1, 1])n”,
”  p_xy.shape = torch.Size([3, 4, 5, 2])n”,
”     z.shape = torch.Size([2, 1, 1, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“&#64;config_enumeraten”,
“def model():n”,
”    p = pyro.param(&quot;p&quot;, torch.randn(5, 4, 3, 2).exp(), constraint=constraints.simplex)n”,
”    x = pyro.sample(&quot;x&quot;, dist.Categorical(torch.ones(4)))n”,
”    y = pyro.sample(&quot;y&quot;, dist.Categorical(torch.ones(3)))n”,
”    with pyro.plate(&quot;z_plate&quot;, 5):n”,
”        p_xy = Vindex(p)[…, x, y, :]n”,
”        z = pyro.sample(&quot;z&quot;, dist.Categorical(p_xy))n”,
”    print(f&quot;     p.shape = {p.shape}&quot;)n”,
”    print(f&quot;     x.shape = {x.shape}&quot;)n”,
”    print(f&quot;     y.shape = {y.shape}&quot;)n”,
”    print(f&quot;  p_xy.shape = {p_xy.shape}&quot;)n”,
”    print(f&quot;     z.shape = {z.shape}&quot;)n”,
”    return x, y, zn”,
”    n”,
“def guide():n”,
”    passn”,
“n”,
“pyro.clear_param_store()n”,
“print(&quot;Sampling:&quot;)n”,
“model()n”,
“print(&quot;Enumerated Inference:&quot;)n”,
“elbo = TraceEnum_ELBO(max_plate_nesting=1)n”,
“elbo.loss(model, guide);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“When enumering within a plate (as described in the next section) <code class="docutils literal notranslate"><span class="pre">Vindex</span></code> can also be used together with capturing the plate index via <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">pyro.plate(...)</span> <span class="pre">as</span> <span class="pre">i</span></code> to index into batch dimensions.  Here’s an example with nontrivial event dimensions due to the <code class="docutils literal notranslate"><span class="pre">Dirichlet</span></code> distribution.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 8,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Sampling:n”,
”    p.shape = torch.Size([5, 4, 3])n”,
”    c.shape = torch.Size([6])n”,
”  vdx.shape = torch.Size([5])n”,
”    pc.shape = torch.Size([5, 6, 3])n”,
”    x.shape = torch.Size([5, 6])n”,
“Enumerated Inference:n”,
”    p.shape = torch.Size([5, 4, 3])n”,
”    c.shape = torch.Size([4, 1, 1])n”,
”  vdx.shape = torch.Size([5])n”,
”    pc.shape = torch.Size([4, 5, 1, 3])n”,
”    x.shape = torch.Size([5, 6])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“&#64;config_enumeraten”,
“def model():n”,
”    data_plate = pyro.plate(&quot;data_plate&quot;, 6, dim=-1)n”,
”    feature_plate = pyro.plate(&quot;feature_plate&quot;, 5, dim=-2)n”,
”    component_plate = pyro.plate(&quot;component_plate&quot;, 4, dim=-1)n”,
”    with feature_plate: n”,
”        with component_plate:n”,
”            p = pyro.sample(&quot;p&quot;, dist.Dirichlet(torch.ones(3)))n”,
”    with data_plate:n”,
”        c = pyro.sample(&quot;c&quot;, dist.Categorical(torch.ones(4)))n”,
”        with feature_plate as vdx:                # Capture plate index.n”,
”            pc = Vindex(p)[vdx[…, None], c, :]  # Reshape it and use in Vindex.n”,
”            x = pyro.sample(&quot;x&quot;, dist.Categorical(pc),n”,
”                            obs=torch.zeros(5, 6, dtype=torch.long))n”,
”    print(f&quot;    p.shape = {p.shape}&quot;)n”,
”    print(f&quot;    c.shape = {c.shape}&quot;)n”,
”    print(f&quot;  vdx.shape = {vdx.shape}&quot;)n”,
”    print(f&quot;    pc.shape = {pc.shape}&quot;)n”,
”    print(f&quot;    x.shape = {x.shape}&quot;)n”,
“n”,
“def guide():n”,
”    feature_plate = pyro.plate(&quot;feature_plate&quot;, 5, dim=-2)n”,
”    component_plate = pyro.plate(&quot;component_plate&quot;, 4, dim=-1)n”,
”    with feature_plate, component_plate:n”,
”        pyro.sample(&quot;p&quot;, dist.Dirichlet(torch.ones(3)))n”,
”    n”,
“pyro.clear_param_store()n”,
“print(&quot;Sampling:&quot;)n”,
“model()n”,
“print(&quot;Enumerated Inference:&quot;)n”,
“elbo = TraceEnum_ELBO(max_plate_nesting=2)n”,
“elbo.loss(model, guide);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Plates and enumeration &lt;a class=&quot;anchor&quot; id=&quot;Plates-and-enumeration&quot;&gt;&lt;/a&gt;n”,
“n”,
“Pyro [plates](<a class="reference external" href="http://docs.pyro.ai/en/dev/primitives.html#pyro.plate">http://docs.pyro.ai/en/dev/primitives.html#pyro.plate</a>) express conditional independence among random variables. Pyro’s enumeration strategy can take advantage of plates to reduce the high cost (exponential in the size of the plate) of enumerating a cartesian product down to a low cost (linear in the size of the plate) of enumerating conditionally independent random variables in lock-step. This is especially important for e.g. minibatched data.n”,
“n”,
“To illustrate, consider a gaussian mixture model with shared variance and different mean.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 9,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Sampling:n”,
”  Running model with 10 data pointsn”,
”    x.shape = torch.Size([10])n”,
”    dist.Normal(loc[x], scale).batch_shape = torch.Size([10])n”,
“Enumerated Inference:n”,
”  Running model with 10 data pointsn”,
”    x.shape = torch.Size([10])n”,
”    dist.Normal(loc[x], scale).batch_shape = torch.Size([10])n”,
”  Running model with 10 data pointsn”,
”    x.shape = torch.Size([3, 1])n”,
”    dist.Normal(loc[x], scale).batch_shape = torch.Size([3, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“&#64;config_enumeraten”,
“def model(data, num_components=3):n”,
”    print(f&quot;  Running model with {len(data)} data points&quot;)n”,
”    p = pyro.sample(&quot;p&quot;, dist.Dirichlet(0.5 * torch.ones(num_components)))n”,
”    scale = pyro.sample(&quot;scale&quot;, dist.LogNormal(0, num_components))n”,
”    with pyro.plate(&quot;components&quot;, num_components):n”,
”        loc = pyro.sample(&quot;loc&quot;, dist.Normal(0, 10))n”,
”    with pyro.plate(&quot;data&quot;, len(data)):n”,
”        x = pyro.sample(&quot;x&quot;, dist.Categorical(p))n”,
”        print(&quot;    x.shape = {}&quot;.format(x.shape))n”,
”        pyro.sample(&quot;obs&quot;, dist.Normal(loc[x], scale), obs=data)n”,
”        print(&quot;    dist.Normal(loc[x], scale).batch_shape = {}&quot;.format(n”,
”            dist.Normal(loc[x], scale).batch_shape))n”,
”        n”,
“guide = AutoNormal(poutine.block(model, hide=[&quot;x&quot;, &quot;data&quot;]))n”,
“n”,
“data = torch.randn(10)n”,
”        n”,
“pyro.clear_param_store()n”,
“print(&quot;Sampling:&quot;)n”,
“model(data)n”,
“print(&quot;Enumerated Inference:&quot;)n”,
“elbo = TraceEnum_ELBO(max_plate_nesting=1)n”,
“elbo.loss(model, guide, data);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Observe that during inference the model is run twice, first by the <cite>AutoNormal</cite> to trace sample sites, and second by <cite>elbo</cite> to compute loss. In the first run, <cite>x</cite> has the standard interpretation of one sample per datum, hence shape <cite>(10,)</cite>. In the second run enumeration can use the same three values <cite>(3,1)</cite> for all data points, and relies on broadcasting for any dependent sample or observe sites that depend on data. For example, in the <cite>pyro.sample(&quot;obs&quot;,…)</cite> statement, the distribution has shape <cite>(3,1)</cite>, the data has shape`(10,)`, and the broadcasted log probability tensor has shape <cite>(3,10)</cite>.n”,
“n”,
“For a more in-depth treatment of enumeration in mixture models, see the [Gaussian Mixture Model Tutorial](<a class="reference external" href="http://pyro.ai/examples/gmm.html">http://pyro.ai/examples/gmm.html</a>) and the [HMM Example](<a class="reference external" href="http://pyro.ai/examples/hmm.html).n">http://pyro.ai/examples/hmm.html).n</a>”,
“n”,
“### Dependencies among plates &lt;a class=&quot;anchor&quot; id=&quot;Dependencies-among-plates&quot;&gt;&lt;/a&gt;n”,
“n”,
“The computational savings of enumerating in vectorized plates comes with restrictions on the dependency structure of models (as described in ([Obermeyer et al. 2019](<a class="reference external" href="https://arxiv.org/abs/1902.03210">https://arxiv.org/abs/1902.03210</a>))). These restrictions are in addition to the usual restrictions of conditional independence. The enumeration restrictions are checked by <cite>TraceEnum_ELBO</cite> and will result in an error if violated (however the usual conditional independence restriction cannot be generally verified by Pyro). For completeness we list all three restrictions:n”,
“n”,
“#### Restriction 1: conditional independencen”,
“Variables within a plate may not depend on each other (along the plate dimension). This applies to any variable, whether or not it is enumerated. This applies to both sequential plates and vectorized plates. For example the following model is invalid:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">invalid_model():\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">x</span> <span class="pre">=</span> <span class="pre">0\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">pyro.plate(\&quot;invalid\&quot;,</span> <span class="pre">10):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">x</span> <span class="pre">=</span> <span class="pre">pyro.sample(f\&quot;x_{i}\&quot;,</span> <span class="pre">dist.Normal(x,</span> <span class="pre">1.))\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“#### Restriction 2: no downstream couplingn”,
“No variable outside of a vectorized plate can depend on an enumerated variable inside of that plate. This would violate Pyro’s exponential speedup assumption. For example the following model is invalid:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;&#64;config_enumerate\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">invalid_model(data):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">pyro.plate(\&quot;plate\&quot;,</span> <span class="pre">10):</span>&#160; <span class="pre">#</span> <span class="pre">&lt;---</span> <span class="pre">invalid</span> <span class="pre">vectorized</span> <span class="pre">plate\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">x</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Bernoulli(0.5))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">assert</span> <span class="pre">x.shape</span> <span class="pre">==</span> <span class="pre">(10,)\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;obs\&quot;,</span> <span class="pre">dist.Normal(x.sum(),</span> <span class="pre">1.),</span> <span class="pre">data)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
” To work around this restriction, you can convert the vectorized plate to a sequential plate:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;&#64;config_enumerate\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">valid_model(data):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">x</span> <span class="pre">=</span> <span class="pre">[]\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">pyro.plate(\&quot;plate\&quot;,</span> <span class="pre">10):</span>&#160; <span class="pre">#</span> <span class="pre">&lt;---</span> <span class="pre">valid</span> <span class="pre">sequential</span> <span class="pre">plate\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">x.append(pyro.sample(f\&quot;x_{i}\&quot;,</span> <span class="pre">dist.Bernoulli(0.5)))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">assert</span> <span class="pre">len(x)</span> <span class="pre">==</span> <span class="pre">10\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">pyro.sample(\&quot;obs\&quot;,</span> <span class="pre">dist.Normal(sum(x),</span> <span class="pre">1.),</span> <span class="pre">data)\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“n”,
“#### Restriction 3: single path leaving each platen”,
“The final restriction is subtle, but is required to enable Pyro’s exponential speedupn”,
“n”,
“&gt; For any enumerated variable <cite>x</cite>, the set of all enumerated variables on which <cite>x</cite> depends must be linearly orderable in their vectorized plate nesting.n”,
“n”,
“This requirement only applies when there are at least two plates and at least three variables in different plate contexts. The simplest counterexample is a Boltzmann machinen”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;&#64;config_enumerate\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">invalid_model(data):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">plate_1</span> <span class="pre">=</span> <span class="pre">pyro.plate(\&quot;plate_1\&quot;,</span> <span class="pre">10,</span> <span class="pre">dim=-1)</span>&#160; <span class="pre">#</span> <span class="pre">vectorized\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">plate_2</span> <span class="pre">=</span> <span class="pre">pyro.plate(\&quot;plate_2\&quot;,</span> <span class="pre">10,</span> <span class="pre">dim=-2)</span>&#160; <span class="pre">#</span> <span class="pre">vectorized\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">plate_1:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">x</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;y\&quot;,</span> <span class="pre">dist.Bernoulli(0.5))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">plate_2:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">y</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;x\&quot;,</span> <span class="pre">dist.Bernoulli(0.5))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">plate_1,</span> <span class="pre">plate2:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;z\&quot;,</span> <span class="pre">dist.Bernoulli((1.</span> <span class="pre">+</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y)</span> <span class="pre">/</span> <span class="pre">4.))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“Here we see that the variable <cite>z</cite> depends on variable <cite>x</cite> (which is in <cite>plate_1</cite> but not <cite>plate_2</cite>) and depends on variable <cite>y</cite> (which is in <cite>plate_2</cite> but not <cite>plate_1</cite>). This model is invalid because there is no way to linearly order <cite>x</cite> and <cite>y</cite> such that one’s plate nesting is less than the other.n”,
“n”,
“To work around this restriction, you can convert one of the plates to a sequential plate:n”,
“<code class="docutils literal notranslate"><span class="pre">`py\n&quot;,</span>
<span class="pre">&quot;&#64;config_enumerate\n&quot;,</span>
<span class="pre">&quot;def</span> <span class="pre">valid_model(data):\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">plate_1</span> <span class="pre">=</span> <span class="pre">pyro.plate(\&quot;plate_1\&quot;,</span> <span class="pre">10,</span> <span class="pre">dim=-1)</span>&#160; <span class="pre">#</span> <span class="pre">vectorized\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">plate_2</span> <span class="pre">=</span> <span class="pre">pyro.plate(\&quot;plate_2\&quot;,</span> <span class="pre">10)</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">#</span> <span class="pre">sequential\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">plate_1:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">x</span> <span class="pre">=</span> <span class="pre">pyro.sample(\&quot;y\&quot;,</span> <span class="pre">dist.Bernoulli(0.5))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">plate_2:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">y</span> <span class="pre">=</span> <span class="pre">pyro.sample(f\&quot;x_{i}\&quot;,</span> <span class="pre">dist.Bernoulli(0.5))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">with</span> <span class="pre">plate_1:\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">z</span> <span class="pre">=</span> <span class="pre">pyro.sample(f\&quot;z_{i}\&quot;,</span> <span class="pre">dist.Bernoulli((1.</span> <span class="pre">+</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y)</span> <span class="pre">/</span> <span class="pre">4.))\n&quot;,</span>
<span class="pre">&quot;</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">...\n&quot;,</span>
<span class="pre">&quot;`</span></code>n”,
“but beware that this increases the computational complexity, which may be exponential in the size of the sequential plate.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“## Time series example  &lt;a class=&quot;anchor&quot; id=&quot;Time-series-example&quot;&gt;&lt;/a&gt;n”,
“n”,
“Consider a discrete HMM with latent states $x_t$ and observations $y_t$. Suppose we want to learn the transition and emission probabilities.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 10,
“metadata”: {},
“outputs”: [],
“source”: [</p>
<blockquote>
<div><p>“data_dim = 4n”,
“num_steps = 10n”,
“data = dist.Categorical(torch.ones(num_steps, data_dim)).sample()n”,
“n”,
“def hmm_model(data, data_dim, hidden_dim=10):n”,
”    print(f&quot;Running for {len(data)} time steps&quot;)n”,
”    # Sample global matrices wrt a Jeffreys prior.n”,
”    with pyro.plate(&quot;hidden_state&quot;, hidden_dim):n”,
”        transition = pyro.sample(&quot;transition&quot;, dist.Dirichlet(0.5 * torch.ones(hidden_dim)))n”,
”        emission = pyro.sample(&quot;emission&quot;, dist.Dirichlet(0.5 * torch.ones(data_dim)))n”,
“n”,
”    x = 0  # initial staten”,
”    for t, y in enumerate(data):n”,
”        x = pyro.sample(f&quot;x_{t}&quot;, dist.Categorical(transition[x]),n”,
”                        infer={&quot;enumerate&quot;: &quot;parallel&quot;})n”,
”        pyro.sample(f&quot;  y_{t}&quot;, dist.Categorical(emission[x]), obs=y)n”,
”        print(f&quot;  x_{t}.shape = {x.shape}&quot;)”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“We can learn the global parameters using SVI with an autoguide.”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 11,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“Running for 10 time stepsn”,
”  x_0.shape = torch.Size([])n”,
”  x_1.shape = torch.Size([])n”,
”  x_2.shape = torch.Size([])n”,
”  x_3.shape = torch.Size([])n”,
”  x_4.shape = torch.Size([])n”,
”  x_5.shape = torch.Size([])n”,
”  x_6.shape = torch.Size([])n”,
”  x_7.shape = torch.Size([])n”,
”  x_8.shape = torch.Size([])n”,
”  x_9.shape = torch.Size([])n”,
“Running for 10 time stepsn”,
”  x_0.shape = torch.Size([10, 1])n”,
”  x_1.shape = torch.Size([10, 1, 1])n”,
”  x_2.shape = torch.Size([10, 1, 1, 1])n”,
”  x_3.shape = torch.Size([10, 1, 1, 1, 1])n”,
”  x_4.shape = torch.Size([10, 1, 1, 1, 1, 1])n”,
”  x_5.shape = torch.Size([10, 1, 1, 1, 1, 1, 1])n”,
”  x_6.shape = torch.Size([10, 1, 1, 1, 1, 1, 1, 1])n”,
”  x_7.shape = torch.Size([10, 1, 1, 1, 1, 1, 1, 1, 1])n”,
”  x_8.shape = torch.Size([10, 1, 1, 1, 1, 1, 1, 1, 1, 1])n”,
”  x_9.shape = torch.Size([10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“hmm_guide = AutoNormal(poutine.block(hmm_model, expose=[&quot;transition&quot;, &quot;emission&quot;]))n”,
“n”,
“pyro.clear_param_store()n”,
“elbo = TraceEnum_ELBO(max_plate_nesting=1)n”,
“elbo.loss(hmm_model, hmm_guide, data, data_dim=data_dim);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Notice that the model was run twice here: first it was run without enumeration by <cite>AutoNormal</cite>, so that the autoguide can record all sample sites; then second it is run by <cite>TraceEnum_ELBO</cite> with enumeration enabled. We see in the first run that samples have the standard interpretation, whereas in the second run samples have the enumeration interpretation.n”,
“n”,
“For more complex examples, including minibatching and multiple plates, see the [HMM tutorial](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/dev/examples/hmm.py">https://github.com/pyro-ppl/pyro/blob/dev/examples/hmm.py</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“### How to enumerate more than 25 variables &lt;a class=&quot;anchor&quot; id=&quot;How-to-enumerate-more-than-25-variables&quot;&gt;&lt;/a&gt;n”,
“n”,
“PyTorch tensors have a dimension limit of 25 in CUDA and 64 in CPU. By default Pyro enumerates each sample site in a new dimension. If you need more sample sites, you can annotate your model with  [pyro.markov](<a class="reference external" href="http://docs.pyro.ai/en/dev/poutine.html#pyro.poutine.markov">http://docs.pyro.ai/en/dev/poutine.html#pyro.poutine.markov</a>) to tell Pyro when it is safe to recycle tensor dimensions. Let’s see how that works with the HMM model from above. The only change we need is to annotate the for loop with <cite>pyro.markov</cite>, informing Pyro that the variables in each step of the loop depend only on variables outside of the loop and variables at this step and the previous step of the loop:n”,
“<code class="docutils literal notranslate"><span class="pre">`diff\n&quot;,</span>
<span class="pre">&quot;-</span> <span class="pre">for</span> <span class="pre">t,</span> <span class="pre">y</span> <span class="pre">in</span> <span class="pre">enumerate(data):\n&quot;,</span>
<span class="pre">&quot;+</span> <span class="pre">for</span> <span class="pre">t,</span> <span class="pre">y</span> <span class="pre">in</span> <span class="pre">pyro.markov(enumerate(data)):\n&quot;,</span>
<span class="pre">&quot;`</span></code>”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: 12,
“metadata”: {},
“outputs”: [</p>
<blockquote>
<div><dl>
<dt>{</dt><dd><p>“name”: “stdout”,
“output_type”: “stream”,
“text”: [</p>
<blockquote>
<div><p>“x_0.shape = torch.Size([10, 1])n”,
“x_1.shape = torch.Size([10, 1, 1])n”,
“x_2.shape = torch.Size([10, 1])n”,
“x_3.shape = torch.Size([10, 1, 1])n”,
“x_4.shape = torch.Size([10, 1])n”,
“x_5.shape = torch.Size([10, 1, 1])n”,
“x_6.shape = torch.Size([10, 1])n”,
“x_7.shape = torch.Size([10, 1, 1])n”,
“x_8.shape = torch.Size([10, 1])n”,
“x_9.shape = torch.Size([10, 1, 1])n”</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>],
“source”: [</p>
<blockquote>
<div><p>“def hmm_model(data, data_dim, hidden_dim=10):n”,
”    with pyro.plate(&quot;hidden_state&quot;, hidden_dim):n”,
”        transition = pyro.sample(&quot;transition&quot;, dist.Dirichlet(0.5 * torch.ones(hidden_dim)))n”,
”        emission = pyro.sample(&quot;emission&quot;, dist.Dirichlet(0.5 * torch.ones(data_dim)))n”,
“n”,
”    x = 0  # initial staten”,
”    for t, y in pyro.markov(enumerate(data)):n”,
”        x = pyro.sample(f&quot;x_{t}&quot;, dist.Categorical(transition[x]),n”,
”                        infer={&quot;enumerate&quot;: &quot;parallel&quot;})n”,
”        pyro.sample(f&quot;y_{t}&quot;, dist.Categorical(emission[x]), obs=y)n”,
”        print(f&quot;x_{t}.shape = {x.shape}&quot;)n”,
“n”,
“# We’ll reuse the same guide and elbo.n”,
“elbo.loss(hmm_model, hmm_guide, data, data_dim=data_dim);”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “markdown”,
“metadata”: {},
“source”: [</p>
<blockquote>
<div><p>“Notice that this model now only needs three tensor dimensions: one for the plate, one for even states, and one for odd states. For more complex examples, see the Dynamic Bayes Net model in the [HMM example](<a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/dev/examples/hmm.py">https://github.com/pyro-ppl/pyro/blob/dev/examples/hmm.py</a>).”</p>
</div></blockquote>
<p>]</p>
</div></blockquote>
<p>},
{</p>
<blockquote>
<div><p>“cell_type”: “code”,
“execution_count”: null,
“metadata”: {},
“outputs”: [],
“source”: []</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>],
“metadata”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“kernelspec”: {</dt><dd><p>“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”</p>
</dd>
</dl>
<p>},
“language_info”: {</p>
<blockquote>
<div><dl class="simple">
<dt>“codemirror_mode”: {</dt><dd><p>“name”: “ipython”,
“version”: 3</p>
</dd>
</dl>
<p>},
“file_extension”: “.py”,
“mimetype”: “text/x-python”,
“name”: “python”,
“nbconvert_exporter”: “python”,
“pygments_lexer”: “ipython3”,
“version”: “3.7.0”</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>},
“nbformat”: 4,
“nbformat_minor”: 2</p>
</dd>
</dl>
<p>}</p>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="scanvi.html" class="btn btn-neutral float-left" title="&lt;no title&gt;" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gmm.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pyro Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>